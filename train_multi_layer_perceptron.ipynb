{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc2fc48f-dbcc-4e6c-bc0e-8d1e72e670a4",
   "metadata": {},
   "source": [
    "# Train Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3375fb27-0be7-4da8-9378-9137587226e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nginyc/repos/job_posting_salary_prediction/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import salary\n",
    "import numpy as np\n",
    "from multi_layer_perceptron import Model, TensorTransformer, CustomNeuralNetRegressor\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "import cloudpickle\n",
    "from skopt import BayesSearchCV\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import random\n",
    "from skorch import dataset\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, EpochScoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eab292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f993bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7be00b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train) = salary.get_train_dataset(include_extracted_salaries=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fba5d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32103, 3670)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = salary.get_preprocessor(DEVICE)\n",
    "(train_size, num_features) = clone(preprocessor).fit_transform(X_train, y_train).shape\n",
    "(train_size, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc83ff-8948-46b2-94bd-096093c31122",
   "metadata": {},
   "source": [
    "## Train & Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9844899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-2.4578\u001b[0m  \u001b[32m12863843545.6000\u001b[0m  \u001b[35m12859947844.6222\u001b[0m  0.0045  3.9418\n",
      "      2  -2.3892  \u001b[32m12686213203.2000\u001b[0m  \u001b[35m12605016614.7713\u001b[0m  0.0045  3.9084\n",
      "      3  -2.2711  \u001b[32m12311866124.8000\u001b[0m  \u001b[35m12165454866.5384\u001b[0m  0.0045  3.6980\n",
      "      4  -2.1129  \u001b[32m11772134051.2000\u001b[0m  \u001b[35m11577183091.1676\u001b[0m  0.0045  3.4839\n",
      "      5  -1.9216  \u001b[32m11093737558.4000\u001b[0m  \u001b[35m10865966323.3919\u001b[0m  0.0045  3.7609\n",
      "      6  -1.7518  \u001b[32m10332289120.0000\u001b[0m  \u001b[35m10234334876.3808\u001b[0m  0.0045  4.0624\n",
      "      7  -1.4998  \u001b[32m9504962732.8000\u001b[0m  \u001b[35m9297213277.6391\u001b[0m  0.0045  3.6652\n",
      "      8  -1.3160  \u001b[32m8629168220.8000\u001b[0m  \u001b[35m8613590567.7680\u001b[0m  0.0045  3.7678\n",
      "      9  -1.1393  \u001b[32m7745812745.6000\u001b[0m  \u001b[35m7956450287.4549\u001b[0m  0.0045  5.3044\n",
      "     10  -0.8829  \u001b[32m6880543472.0000\u001b[0m  \u001b[35m7002736281.7894\u001b[0m  0.0045  4.4038\n",
      "     11  -0.6837  \u001b[32m6034134241.6000\u001b[0m  \u001b[35m6261743308.1721\u001b[0m  0.0045  5.6572\n",
      "     12  -0.5384  \u001b[32m5246639801.6000\u001b[0m  \u001b[35m5721603176.6027\u001b[0m  0.0045  5.0897\n",
      "     13  -0.3523  \u001b[32m4526543000.0000\u001b[0m  \u001b[35m5029554811.2408\u001b[0m  0.0045  4.1752\n",
      "     14  -0.1386  \u001b[32m3874745932.0000\u001b[0m  \u001b[35m4234482137.9264\u001b[0m  0.0045  4.6511\n",
      "     15  -0.0608  \u001b[32m3288191520.8000\u001b[0m  \u001b[35m3945172349.1096\u001b[0m  0.0045  4.5843\n",
      "     16  0.0675  \u001b[32m2761448409.6000\u001b[0m  \u001b[35m3468244270.6202\u001b[0m  0.0045  4.3989\n",
      "     17  0.1340  \u001b[32m2334797696.4000\u001b[0m  \u001b[35m3220763161.2163\u001b[0m  0.0045  4.1861\n",
      "     18  0.2273  \u001b[32m1965440302.4000\u001b[0m  \u001b[35m2873607494.8149\u001b[0m  0.0045  3.7161\n",
      "     19  0.3385  \u001b[32m1661840315.2000\u001b[0m  \u001b[35m2460290446.9130\u001b[0m  0.0045  3.9091\n",
      "     20  0.3833  \u001b[32m1414490268.4000\u001b[0m  \u001b[35m2293413154.7347\u001b[0m  0.0045  3.8859\n",
      "     21  0.4387  \u001b[32m1176301042.8000\u001b[0m  \u001b[35m2087375496.4594\u001b[0m  0.0045  3.8838\n",
      "     22  0.4673  \u001b[32m1051092569.6000\u001b[0m  \u001b[35m1981266691.5881\u001b[0m  0.0045  3.5019\n",
      "     23  0.4864  \u001b[32m915065742.2000\u001b[0m  \u001b[35m1910237997.2560\u001b[0m  0.0045  4.0341\n",
      "     24  0.5230  \u001b[32m814067574.2000\u001b[0m  \u001b[35m1774036393.5620\u001b[0m  0.0045  3.6054\n",
      "     25  0.5561  \u001b[32m722161137.6000\u001b[0m  \u001b[35m1651018596.3294\u001b[0m  0.0045  3.9872\n",
      "     26  0.5805  \u001b[32m658561438.8000\u001b[0m  \u001b[35m1560306233.0045\u001b[0m  0.0045  3.6165\n",
      "     27  0.5882  \u001b[32m612644766.6000\u001b[0m  \u001b[35m1531529137.8190\u001b[0m  0.0045  3.8376\n",
      "     28  0.5751  \u001b[32m577909967.7000\u001b[0m  1580114153.0045  0.0045  3.5830\n",
      "     29  0.5905  \u001b[32m561821929.6000\u001b[0m  \u001b[35m1523052158.9846\u001b[0m  0.0045  3.4389\n",
      "     30  0.6036  \u001b[32m505039431.2000\u001b[0m  \u001b[35m1474247565.1625\u001b[0m  0.0045  3.5623\n",
      "     31  0.6100  \u001b[32m492471269.9000\u001b[0m  \u001b[35m1450594297.3938\u001b[0m  0.0045  3.6644\n",
      "     32  0.6317  \u001b[32m440755873.9000\u001b[0m  \u001b[35m1369718053.3946\u001b[0m  0.0045  3.4010\n",
      "     33  0.6137  \u001b[32m434834104.8000\u001b[0m  1436640160.0934  0.0045  4.9745\n",
      "     34  0.6166  \u001b[32m434209628.5000\u001b[0m  1425927767.5281  0.0045  4.9442\n",
      "     35  0.5827  \u001b[32m415320147.0000\u001b[0m  1552033487.1497  0.0045  5.0887\n",
      "     36  0.6213  415421616.1000  1408590063.3864  0.0045  4.6123\n",
      "     37  0.6336  \u001b[32m356552907.9000\u001b[0m  \u001b[35m1362831761.9965\u001b[0m  0.0045  4.5340\n",
      "     38  0.6357  375912785.4000  \u001b[35m1354890963.9899\u001b[0m  0.0045  3.8200\n",
      "     39  0.6284  \u001b[32m348089717.1000\u001b[0m  1382059839.7135  0.0045  4.0950\n",
      "     40  0.6380  \u001b[32m347999415.5000\u001b[0m  \u001b[35m1346289003.2875\u001b[0m  0.0045  3.6538\n",
      "     41  0.6398  \u001b[32m331097476.4000\u001b[0m  \u001b[35m1339675866.0510\u001b[0m  0.0045  3.7051\n",
      "     42  0.6255  \u001b[32m318625112.7500\u001b[0m  1392630299.0726  0.0045  3.5981\n",
      "     43  0.6226  320762325.9500  1403597186.2830  0.0045  3.4951\n",
      "     44  0.6387  323871633.0500  1343789962.0697  0.0045  3.5176\n",
      "     45  0.6388  320195211.4000  1343418997.6157  0.0045  3.8981\n",
      "     46  0.6407  332075003.7000  \u001b[35m1336165233.8781\u001b[0m  0.0045  3.4310\n",
      "     47  0.6354  \u001b[32m302901958.6000\u001b[0m  1356126178.5447  0.0045  3.4463\n",
      "     48  0.6325  311072309.1000  1366595162.8857  0.0045  3.5117\n",
      "     49  0.6421  \u001b[32m291156955.8000\u001b[0m  \u001b[35m1331254419.3202\u001b[0m  0.0045  3.5041\n",
      "     50  0.6401  312126280.6000  1338508054.9675  0.0045  4.0470\n",
      "     51  0.6474  312511210.0000  \u001b[35m1311543269.2451\u001b[0m  0.0045  4.2387\n",
      "     52  0.6465  299150494.2000  1314849006.4240  0.0045  3.8082\n",
      "     53  0.6485  300741393.9000  \u001b[35m1307327827.9774\u001b[0m  0.0045  4.3318\n",
      "     54  0.6467  \u001b[32m278831109.1000\u001b[0m  1313884494.8507  0.0045  4.1172\n",
      "     55  0.6444  296037662.9000  1322345447.6807  0.0045  3.8192\n",
      "     56  0.6463  302151642.7000  1315629893.4444  0.0045  5.9095\n",
      "     57  0.6435  306358010.1000  1325746583.6839  0.0045  3.9277\n",
      "     58  0.6279  299247163.4000  1383757487.3148  0.0045  4.3691\n",
      "     59  0.6362  295361648.9000  1353097212.6611  0.0045  4.1269\n",
      "     60  0.6446  \u001b[32m275082324.3000\u001b[0m  1321780930.4668  0.0022  4.0224\n",
      "     61  0.6414  282704497.6000  1333535371.2315  0.0022  3.4922\n",
      "     62  0.6365  \u001b[32m266989142.8500\u001b[0m  1351837752.7927  0.0022  3.2620\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 53.\n",
      "[CV 1/5] END batch_size=128, lambda1=0.0001, lr=0.00447, module__dropout_rate=0.5, module__n_units_last=256, module__num_hidden_layers=3;, score=0.689 total time= 4.2min\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-2.3721\u001b[0m  \u001b[32m12710738009.6000\u001b[0m  \u001b[35m13020655428.0241\u001b[0m  0.0045  3.4437\n",
      "      2  -2.3024  \u001b[32m12529170086.4000\u001b[0m  \u001b[35m12751719851.7796\u001b[0m  0.0045  3.8536\n",
      "      3  -2.1870  \u001b[32m12165592246.4000\u001b[0m  \u001b[35m12305802630.9021\u001b[0m  0.0045  3.9170\n",
      "      4  -2.0370  \u001b[32m11623585536.0000\u001b[0m  \u001b[35m11726755915.3498\u001b[0m  0.0045  5.4577\n",
      "      5  -1.8685  \u001b[32m10953156377.6000\u001b[0m  \u001b[35m11076062958.7074\u001b[0m  0.0045  5.5266\n",
      "      6  -1.6656  \u001b[32m10189272508.8000\u001b[0m  \u001b[35m10292658948.0366\u001b[0m  0.0045  4.1077\n",
      "      7  -1.4678  \u001b[32m9337096489.6000\u001b[0m  \u001b[35m9528720226.7222\u001b[0m  0.0045  3.6853\n",
      "      8  -1.2959  \u001b[32m8466307529.6000\u001b[0m  \u001b[35m8865266237.6952\u001b[0m  0.0045  3.3236\n",
      "      9  -1.0488  \u001b[32m7594174614.4000\u001b[0m  \u001b[35m7911050931.9027\u001b[0m  0.0045  3.5426\n",
      "     10  -0.9047  \u001b[32m6718252736.0000\u001b[0m  \u001b[35m7354545503.4331\u001b[0m  0.0045  3.2433\n",
      "     11  -0.6631  \u001b[32m5880116422.4000\u001b[0m  \u001b[35m6421696876.5894\u001b[0m  0.0045  3.6040\n",
      "     12  -0.5128  \u001b[32m5120276856.0000\u001b[0m  \u001b[35m5841406866.4637\u001b[0m  0.0045  3.3013\n",
      "     13  -0.2649  \u001b[32m4411603632.0000\u001b[0m  \u001b[35m4884157855.4207\u001b[0m  0.0045  3.8602\n",
      "     14  -0.1536  \u001b[32m3753174236.0000\u001b[0m  \u001b[35m4454221350.3726\u001b[0m  0.0045  3.3051\n",
      "     15  -0.0598  \u001b[32m3178493705.6000\u001b[0m  \u001b[35m4092112566.4941\u001b[0m  0.0045  3.4969\n",
      "     16  0.0958  \u001b[32m2666992408.0000\u001b[0m  \u001b[35m3491210148.5038\u001b[0m  0.0045  3.1864\n",
      "     17  0.1799  \u001b[32m2235640925.6000\u001b[0m  \u001b[35m3166476311.1731\u001b[0m  0.0045  3.7734\n",
      "     18  0.2431  \u001b[32m1877089958.4000\u001b[0m  \u001b[35m2922470437.1766\u001b[0m  0.0045  4.1196\n",
      "     19  0.2998  \u001b[32m1556562843.6000\u001b[0m  \u001b[35m2703607514.4746\u001b[0m  0.0045  4.5744\n",
      "     20  0.3349  \u001b[32m1328275737.0000\u001b[0m  \u001b[35m2568146478.8943\u001b[0m  0.0045  3.2126\n",
      "     21  0.4300  \u001b[32m1129510638.2000\u001b[0m  \u001b[35m2200902395.0664\u001b[0m  0.0045  3.8789\n",
      "     22  0.4380  \u001b[32m971214506.6000\u001b[0m  \u001b[35m2169893093.1392\u001b[0m  0.0045  3.1911\n",
      "     23  0.4928  \u001b[32m821218568.4000\u001b[0m  \u001b[35m1958481142.0082\u001b[0m  0.0045  3.5616\n",
      "     24  0.4950  \u001b[32m759995106.2000\u001b[0m  \u001b[35m1949950856.1978\u001b[0m  0.0045  3.4708\n",
      "     25  0.5225  \u001b[32m658742787.2000\u001b[0m  \u001b[35m1843588151.0422\u001b[0m  0.0045  3.7508\n",
      "     26  0.5724  \u001b[32m584928771.6000\u001b[0m  \u001b[35m1651018857.7987\u001b[0m  0.0045  3.1331\n",
      "     27  0.5591  \u001b[32m542725151.0000\u001b[0m  1702473937.1805  0.0045  3.4197\n",
      "     28  0.5410  \u001b[32m506006198.2000\u001b[0m  1772489059.7687  0.0045  3.2802\n",
      "     29  0.5761  \u001b[32m467095951.1000\u001b[0m  \u001b[35m1636668068.8028\u001b[0m  0.0045  3.2338\n",
      "     30  0.5745  \u001b[32m446652103.5000\u001b[0m  1642979633.0621  0.0045  3.3701\n",
      "     31  0.5879  \u001b[32m427724489.7000\u001b[0m  \u001b[35m1591342218.5151\u001b[0m  0.0045  3.2950\n",
      "     32  0.6076  \u001b[32m410468447.9000\u001b[0m  \u001b[35m1515329188.5287\u001b[0m  0.0045  3.4459\n",
      "     33  0.5916  423079168.0000  1576904494.6950  0.0045  3.2226\n",
      "     34  0.5934  \u001b[32m378843790.2000\u001b[0m  1569831042.9901  0.0045  3.3340\n",
      "     35  0.5961  \u001b[32m367669885.4000\u001b[0m  1559634585.3159  0.0045  3.7853\n",
      "     36  0.5956  388719264.1000  1561545195.6924  0.0045  3.8621\n",
      "     37  0.6066  \u001b[32m346679534.5000\u001b[0m  1519104300.3777  0.0045  4.2578\n",
      "     38  0.5981  \u001b[32m311851632.5000\u001b[0m  1551700202.9449  0.0045  3.8500\n",
      "     39  0.6089  \u001b[32m310603114.6000\u001b[0m  \u001b[35m1510308934.7152\u001b[0m  0.0022  4.3770\n",
      "     40  0.6033  \u001b[32m282332480.9000\u001b[0m  1531652495.5484  0.0022  3.8289\n",
      "     41  0.6122  \u001b[32m271742938.6500\u001b[0m  \u001b[35m1497499065.8081\u001b[0m  0.0022  4.4197\n",
      "     42  0.6154  273144145.1500  \u001b[35m1485095162.0697\u001b[0m  0.0022  4.2002\n",
      "     43  0.6079  \u001b[32m263687213.2500\u001b[0m  1514193173.6780  0.0022  4.0935\n",
      "     44  0.6194  \u001b[32m251668751.3500\u001b[0m  \u001b[35m1469530293.7216\u001b[0m  0.0022  3.9908\n",
      "     45  0.6125  254352702.5500  1496120755.7033  0.0022  3.6133\n",
      "     46  0.6178  261348539.4500  1475851574.5937  0.0022  3.7849\n",
      "     47  0.6154  \u001b[32m251050677.2000\u001b[0m  1485064717.6547  0.0022  4.2006\n",
      "     48  0.6183  \u001b[32m240563827.6500\u001b[0m  1473665597.9692  0.0022  3.9268\n",
      "     49  0.6227  249116396.5000  \u001b[35m1456870261.1112\u001b[0m  0.0022  4.0332\n",
      "     50  0.6141  253985019.9500  1490234058.4030  0.0022  4.0462\n",
      "     51  0.6158  247250863.7500  1483435968.9095  0.0022  4.0239\n",
      "     52  0.6128  \u001b[32m235671695.8000\u001b[0m  1495046099.0742  0.0022  3.5933\n",
      "     53  0.6208  \u001b[32m225761328.5500\u001b[0m  1464148847.9034  0.0022  3.9653\n",
      "     54  0.6180  241169402.6000  1475123539.2984  0.0022  3.6456\n",
      "     55  0.6261  241679660.5500  \u001b[35m1443662261.7715\u001b[0m  0.0022  4.1726\n",
      "     56  0.6196  241964821.4000  1468944842.2535  0.0022  4.6121\n",
      "     57  0.6267  247296381.6500  \u001b[35m1441591454.7977\u001b[0m  0.0022  3.8268\n",
      "     58  0.6210  \u001b[32m222811407.7500\u001b[0m  1463595166.2496  0.0022  3.5874\n",
      "     59  0.6205  229572872.0000  1465498086.9581  0.0022  3.6352\n",
      "     60  0.6259  227800783.4000  1444530679.3039  0.0022  3.9364\n",
      "     61  0.6234  236273527.1000  1454229758.2807  0.0022  3.5793\n",
      "     62  0.6211  230416469.8500  1463181344.9905  0.0022  3.9830\n",
      "     63  0.6202  233522673.7000  1466478891.9042  0.0022  3.7173\n",
      "     64  0.6214  \u001b[32m206787765.3000\u001b[0m  1461950536.7584  0.0011  3.5934\n",
      "     65  0.6229  227294556.4500  1456242212.7779  0.0011  3.9377\n",
      "     66  0.6189  224060268.7500  1471570749.2716  0.0011  3.7873\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 57.\n",
      "[CV 2/5] END batch_size=128, lambda1=0.0001, lr=0.00447, module__dropout_rate=0.5, module__n_units_last=256, module__num_hidden_layers=3;, score=0.643 total time= 4.2min\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-2.3548\u001b[0m  \u001b[32m12717529683.2000\u001b[0m  \u001b[35m13030820073.0263\u001b[0m  0.0045  3.9696\n",
      "      2  -2.2794  \u001b[32m12531893696.0000\u001b[0m  \u001b[35m12738040833.7940\u001b[0m  0.0045  3.7896\n",
      "      3  -2.1705  \u001b[32m12171924281.6000\u001b[0m  \u001b[35m12314875304.1916\u001b[0m  0.0045  4.0030\n",
      "      4  -2.0246  \u001b[32m11633178099.2000\u001b[0m  \u001b[35m11748231611.5274\u001b[0m  0.0045  3.8981\n",
      "      5  -1.8442  \u001b[32m10966985209.6000\u001b[0m  \u001b[35m11047546888.7709\u001b[0m  0.0045  5.6515\n",
      "      6  -1.6352  \u001b[32m10198794860.8000\u001b[0m  \u001b[35m10235878670.9005\u001b[0m  0.0045  4.3760\n",
      "      7  -1.4718  \u001b[32m9369330166.4000\u001b[0m  \u001b[35m9601120074.6023\u001b[0m  0.0045  4.2456\n",
      "      8  -1.2433  \u001b[32m8498420083.2000\u001b[0m  \u001b[35m8713553223.5126\u001b[0m  0.0045  3.8718\n",
      "      9  -1.0455  \u001b[32m7612629865.6000\u001b[0m  \u001b[35m7945359532.8262\u001b[0m  0.0045  3.7050\n",
      "     10  -0.8500  \u001b[32m6756425240.0000\u001b[0m  \u001b[35m7185625866.4154\u001b[0m  0.0045  3.4950\n",
      "     11  -0.6592  \u001b[32m5917244416.0000\u001b[0m  \u001b[35m6444833918.5797\u001b[0m  0.0045  3.6435\n",
      "     12  -0.5108  \u001b[32m5162668992.0000\u001b[0m  \u001b[35m5868429331.9338\u001b[0m  0.0045  3.5611\n",
      "     13  -0.3079  \u001b[32m4455032379.2000\u001b[0m  \u001b[35m5080275189.7839\u001b[0m  0.0045  3.7624\n",
      "     14  -0.1276  \u001b[32m3781377257.6000\u001b[0m  \u001b[35m4379818424.4376\u001b[0m  0.0045  3.4784\n",
      "     15  -0.0158  \u001b[32m3208542268.8000\u001b[0m  \u001b[35m3945624581.2825\u001b[0m  0.0045  3.7934\n",
      "     16  0.0755  \u001b[32m2721100024.0000\u001b[0m  \u001b[35m3591037997.7481\u001b[0m  0.0045  3.4299\n",
      "     17  0.1374  \u001b[32m2277097272.0000\u001b[0m  \u001b[35m3350342599.8365\u001b[0m  0.0045  3.8430\n",
      "     18  0.2939  \u001b[32m1907183349.6000\u001b[0m  \u001b[35m2742600611.3078\u001b[0m  0.0045  3.3986\n",
      "     19  0.3702  \u001b[32m1606430883.6000\u001b[0m  \u001b[35m2446118750.3866\u001b[0m  0.0045  3.6778\n",
      "     20  0.3816  \u001b[32m1364557366.4000\u001b[0m  \u001b[35m2402054193.7847\u001b[0m  0.0045  3.2915\n",
      "     21  0.4381  \u001b[32m1183601397.8000\u001b[0m  \u001b[35m2182427882.0230\u001b[0m  0.0045  3.2924\n",
      "     22  0.4711  \u001b[32m1001702319.6000\u001b[0m  \u001b[35m2054467773.8197\u001b[0m  0.0045  3.7420\n",
      "     23  0.4753  \u001b[32m877013463.0000\u001b[0m  \u001b[35m2037978217.5495\u001b[0m  0.0045  3.2056\n",
      "     24  0.5461  \u001b[32m765503412.6000\u001b[0m  \u001b[35m1763088151.1232\u001b[0m  0.0045  3.6520\n",
      "     25  0.5390  \u001b[32m701493881.2000\u001b[0m  1790485636.5599  0.0045  3.3035\n",
      "     26  0.5470  \u001b[32m645767850.2000\u001b[0m  \u001b[35m1759446708.6004\u001b[0m  0.0045  3.9604\n",
      "     27  0.5687  \u001b[32m600987060.4000\u001b[0m  \u001b[35m1675127415.4285\u001b[0m  0.0045  3.4242\n",
      "     28  0.5863  \u001b[32m575472302.4000\u001b[0m  \u001b[35m1606751935.5390\u001b[0m  0.0045  3.7778\n",
      "     29  0.5923  \u001b[32m529170193.5000\u001b[0m  \u001b[35m1583698046.3305\u001b[0m  0.0045  3.8447\n",
      "     30  0.6024  \u001b[32m499997761.7000\u001b[0m  \u001b[35m1544480538.4123\u001b[0m  0.0045  3.5996\n",
      "     31  0.5945  \u001b[32m478350680.9000\u001b[0m  1575214033.1555  0.0045  3.4987\n",
      "     32  0.6059  \u001b[32m460701775.2000\u001b[0m  \u001b[35m1530887639.6589\u001b[0m  0.0045  3.6774\n",
      "     33  0.6124  \u001b[32m451197046.4000\u001b[0m  \u001b[35m1505572755.4355\u001b[0m  0.0045  3.4903\n",
      "     34  0.6083  \u001b[32m444777464.5000\u001b[0m  1521344775.8489  0.0045  3.7365\n",
      "     35  0.6108  \u001b[32m422949055.7000\u001b[0m  1511764100.8090  0.0045  3.4893\n",
      "     36  0.6099  \u001b[32m389978433.2000\u001b[0m  1515125068.0724  0.0045  3.9505\n",
      "     37  0.6287  \u001b[32m373803570.7000\u001b[0m  \u001b[35m1442041313.3767\u001b[0m  0.0045  3.4525\n",
      "     38  0.6169  \u001b[32m341837792.3000\u001b[0m  1488069953.4078  0.0045  3.7482\n",
      "     39  0.6093  \u001b[32m326809770.0000\u001b[0m  1517408185.8828  0.0045  3.5163\n",
      "     40  0.6304  345577019.5000  \u001b[35m1435793536.5731\u001b[0m  0.0045  4.6722\n",
      "     41  0.6306  335546173.2000  \u001b[35m1434982169.3907\u001b[0m  0.0045  4.0354\n",
      "     42  0.6229  342183248.0000  1464649294.5392  0.0045  4.9819\n",
      "     43  0.6258  \u001b[32m319653058.8000\u001b[0m  1453644098.2301  0.0045  4.9301\n",
      "     44  0.6235  328715795.9000  1462506359.1544  0.0045  4.2610\n",
      "     45  0.6293  \u001b[32m298092702.5500\u001b[0m  1440074663.0952  0.0045  4.0342\n",
      "     46  0.6297  304207131.3000  1438454776.7491  0.0045  4.3836\n",
      "     47  0.6277  317946752.5000  1446156705.0154  0.0045  3.3106\n",
      "     48  0.6327  \u001b[32m295866227.5500\u001b[0m  \u001b[35m1426563772.7732\u001b[0m  0.0022  4.2504\n",
      "     49  0.6208  \u001b[32m266273021.0000\u001b[0m  1472727054.5517  0.0022  3.6406\n",
      "     50  0.6402  272195437.4000  \u001b[35m1397666603.4557\u001b[0m  0.0022  3.9664\n",
      "     51  0.6293  269219083.2000  1439981610.3594  0.0022  3.3199\n",
      "     52  0.6284  271200383.0000  1443388139.0446  0.0022  3.9599\n",
      "     53  0.6391  \u001b[32m250601458.2500\u001b[0m  1401912424.7522  0.0022  3.4840\n",
      "     54  0.6343  251885027.5500  1420276115.2610  0.0022  3.6920\n",
      "     55  0.6321  265291203.7000  1429050018.1865  0.0022  3.5087\n",
      "     56  0.6330  255303891.6500  1425571830.6809  0.0022  3.7833\n",
      "     57  0.6387  254223377.9000  1403327252.6564  0.0011  3.4837\n",
      "     58  0.6367  \u001b[32m242509457.1500\u001b[0m  1411200674.6101  0.0011  3.7670\n",
      "     59  0.6264  \u001b[32m242384699.9000\u001b[0m  1451258632.7210  0.0011  3.5252\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 50.\n",
      "[CV 3/5] END batch_size=128, lambda1=0.0001, lr=0.00447, module__dropout_rate=0.5, module__n_units_last=256, module__num_hidden_layers=3;, score=0.684 total time= 3.8min\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-2.2873\u001b[0m  \u001b[32m12677687667.2000\u001b[0m  \u001b[35m13011282374.1919\u001b[0m  0.0045  3.2864\n",
      "      2  -2.2175  \u001b[32m12503664403.2000\u001b[0m  \u001b[35m12734893325.7045\u001b[0m  0.0045  3.7176\n",
      "      3  -2.1052  \u001b[32m12135273171.2000\u001b[0m  \u001b[35m12290448723.0742\u001b[0m  0.0045  3.5513\n",
      "      4  -1.9543  \u001b[32m11587363219.2000\u001b[0m  \u001b[35m11693198693.6126\u001b[0m  0.0045  3.9642\n",
      "      5  -1.7912  \u001b[32m10925558422.4000\u001b[0m  \u001b[35m11047645322.7393\u001b[0m  0.0045  3.6593\n",
      "      6  -1.5782  \u001b[32m10173590358.4000\u001b[0m  \u001b[35m10204618367.5764\u001b[0m  0.0045  4.8210\n",
      "      7  -1.3915  \u001b[32m9327828665.6000\u001b[0m  \u001b[35m9465641706.8203\u001b[0m  0.0045  4.7791\n",
      "      8  -1.2068  \u001b[32m8463156768.0000\u001b[0m  \u001b[35m8734413730.3111\u001b[0m  0.0045  4.1900\n",
      "      9  -0.9932  \u001b[32m7591978163.2000\u001b[0m  \u001b[35m7889014153.0450\u001b[0m  0.0045  3.7814\n",
      "     10  -0.8112  \u001b[32m6733371888.0000\u001b[0m  \u001b[35m7168706550.9301\u001b[0m  0.0045  3.6852\n",
      "     11  -0.6393  \u001b[32m5883818185.6000\u001b[0m  \u001b[35m6488510155.2252\u001b[0m  0.0045  3.8984\n",
      "     12  -0.4485  \u001b[32m5111704892.8000\u001b[0m  \u001b[35m5733059477.6033\u001b[0m  0.0045  3.4470\n",
      "     13  -0.2827  \u001b[32m4414612643.2000\u001b[0m  \u001b[35m5076829137.8034\u001b[0m  0.0045  3.8332\n",
      "     14  -0.0992  \u001b[32m3759734882.4000\u001b[0m  \u001b[35m4350808841.9420\u001b[0m  0.0045  4.4291\n",
      "     15  0.0035  \u001b[32m3186661176.8000\u001b[0m  \u001b[35m3944358863.1622\u001b[0m  0.0045  4.1941\n",
      "     16  0.0825  \u001b[32m2671323488.8000\u001b[0m  \u001b[35m3631487732.4135\u001b[0m  0.0045  3.5619\n",
      "     17  0.1912  \u001b[32m2250677406.0000\u001b[0m  \u001b[35m3201373888.6105\u001b[0m  0.0045  3.7888\n",
      "     18  0.3265  \u001b[32m1908784276.4000\u001b[0m  \u001b[35m2665857345.5573\u001b[0m  0.0045  5.2112\n",
      "     19  0.3674  \u001b[32m1597690829.2000\u001b[0m  \u001b[35m2503872483.6691\u001b[0m  0.0045  4.1253\n",
      "     20  0.4310  \u001b[32m1326755221.0000\u001b[0m  \u001b[35m2252123088.2834\u001b[0m  0.0045  4.0106\n",
      "     21  0.4559  \u001b[32m1165863972.6000\u001b[0m  \u001b[35m2153760594.7627\u001b[0m  0.0045  3.5711\n",
      "     22  0.4888  \u001b[32m1014270666.0000\u001b[0m  \u001b[35m2023327084.2655\u001b[0m  0.0045  4.4917\n",
      "     23  0.4932  \u001b[32m864716370.8000\u001b[0m  \u001b[35m2005787924.0210\u001b[0m  0.0045  3.8444\n",
      "     24  0.5247  \u001b[32m757857461.6000\u001b[0m  \u001b[35m1881109484.8012\u001b[0m  0.0045  4.4001\n",
      "     25  0.5669  \u001b[32m682611257.8000\u001b[0m  \u001b[35m1714197533.1781\u001b[0m  0.0045  3.8886\n",
      "     26  0.5781  \u001b[32m624424942.0000\u001b[0m  \u001b[35m1669795344.7444\u001b[0m  0.0045  4.0806\n",
      "     27  0.5985  \u001b[32m571878899.8000\u001b[0m  \u001b[35m1589256255.0843\u001b[0m  0.0045  4.2030\n",
      "     28  0.5977  \u001b[32m538749896.3000\u001b[0m  1592511004.5054  0.0045  4.0646\n",
      "     29  0.6162  \u001b[32m508125559.6000\u001b[0m  \u001b[35m1519011452.4057\u001b[0m  0.0045  3.9464\n",
      "     30  0.6065  \u001b[32m492139383.8000\u001b[0m  1557532489.3564  0.0045  4.0497\n",
      "     31  0.5991  \u001b[32m469691739.6000\u001b[0m  1586609814.8865  0.0045  4.2308\n",
      "     32  0.6042  \u001b[32m434286527.7000\u001b[0m  1566668124.9476  0.0045  4.1504\n",
      "     33  0.5981  \u001b[32m417682140.1000\u001b[0m  1590604090.3002  0.0045  4.0274\n",
      "     34  0.6205  \u001b[32m412390007.1000\u001b[0m  \u001b[35m1502083406.9815\u001b[0m  0.0045  4.0826\n",
      "     35  0.6076  413138553.6000  1553062515.2299  0.0045  3.3455\n",
      "     36  0.6029  \u001b[32m395574867.3000\u001b[0m  1571631472.4890  0.0045  4.0343\n",
      "     37  0.6385  \u001b[32m390314151.9000\u001b[0m  \u001b[35m1430898010.9356\u001b[0m  0.0045  3.3390\n",
      "     38  0.6167  \u001b[32m351382384.0000\u001b[0m  1517022098.4762  0.0045  3.7491\n",
      "     39  0.6147  \u001b[32m337529511.6000\u001b[0m  1524863010.4045  0.0045  3.2440\n",
      "     40  0.6240  \u001b[32m316314343.8000\u001b[0m  1488138547.5102  0.0045  3.6515\n",
      "     41  0.6268  324866696.5000  1477281323.9914  0.0045  3.4614\n",
      "     42  0.6258  316721951.2000  1481182423.0547  0.0045  3.0311\n",
      "     43  0.6311  \u001b[32m293287334.2000\u001b[0m  1460146348.3154  0.0045  3.6988\n",
      "     44  0.6394  \u001b[32m287754766.4500\u001b[0m  \u001b[35m1427305233.1182\u001b[0m  0.0022  3.0882\n",
      "     45  0.6370  \u001b[32m274939814.3500\u001b[0m  1436766091.2003  0.0022  3.6121\n",
      "     46  0.6344  276549389.3000  1446979627.6987  0.0022  3.0775\n",
      "     47  0.6337  \u001b[32m264758678.3500\u001b[0m  1449995529.8797  0.0022  3.2049\n",
      "     48  0.6407  265284654.0500  \u001b[35m1422259372.5770\u001b[0m  0.0022  3.3256\n",
      "     49  0.6421  \u001b[32m256559350.0500\u001b[0m  \u001b[35m1416431652.7094\u001b[0m  0.0022  3.4899\n",
      "     50  0.6304  260475722.5500  1462795965.0535  0.0022  3.0041\n",
      "     51  0.6386  \u001b[32m254069734.4500\u001b[0m  1430406048.0561  0.0022  3.5168\n",
      "     52  0.6372  254314614.2500  1436112301.1376  0.0022  3.2915\n",
      "     53  0.6342  \u001b[32m249230080.4000\u001b[0m  1447987140.9710  0.0022  3.3506\n",
      "     54  0.6314  252658581.2000  1458983871.1653  0.0022  3.0619\n",
      "     55  0.6418  \u001b[32m239923455.9000\u001b[0m  1417664118.5128  0.0022  4.1630\n",
      "     56  0.6376  \u001b[32m239492053.5500\u001b[0m  1434351764.4633  0.0011  4.3115\n",
      "     57  0.6337  \u001b[32m219626011.9000\u001b[0m  1449990582.9114  0.0011  3.8897\n",
      "     58  0.6374  \u001b[32m216699643.6000\u001b[0m  1435059381.2108  0.0011  3.4795\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 49.\n",
      "[CV 4/5] END batch_size=128, lambda1=0.0001, lr=0.00447, module__dropout_rate=0.5, module__n_units_last=256, module__num_hidden_layers=3;, score=0.644 total time= 3.7min\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-2.2656\u001b[0m  \u001b[32m12611499046.4000\u001b[0m  \u001b[35m13286110672.4579\u001b[0m  0.0045  3.7132\n",
      "      2  -2.1980  \u001b[32m12422536822.4000\u001b[0m  \u001b[35m13011001655.3662\u001b[0m  0.0045  3.9547\n",
      "      3  -2.0877  \u001b[32m12053052211.2000\u001b[0m  \u001b[35m12562283960.3379\u001b[0m  0.0045  3.5053\n",
      "      4  -1.9481  \u001b[32m11527295603.2000\u001b[0m  \u001b[35m11994199061.7279\u001b[0m  0.0045  3.5308\n",
      "      5  -1.7770  \u001b[32m10847103574.4000\u001b[0m  \u001b[35m11297993736.7210\u001b[0m  0.0045  3.4400\n",
      "      6  -1.6093  \u001b[32m10102565750.4000\u001b[0m  \u001b[35m10615628726.0456\u001b[0m  0.0045  3.4318\n",
      "      7  -1.3818  \u001b[32m9273718278.4000\u001b[0m  \u001b[35m9690076704.4423\u001b[0m  0.0045  3.5725\n",
      "      8  -1.2176  \u001b[32m8412105824.0000\u001b[0m  \u001b[35m9022154154.7829\u001b[0m  0.0045  3.8913\n",
      "      9  -1.0322  \u001b[32m7543355622.4000\u001b[0m  \u001b[35m8267979817.2630\u001b[0m  0.0045  4.2425\n",
      "     10  -0.8424  \u001b[32m6682005985.6000\u001b[0m  \u001b[35m7495812568.3317\u001b[0m  0.0045  3.5297\n",
      "     11  -0.6307  \u001b[32m5851614982.4000\u001b[0m  \u001b[35m6634379726.6887\u001b[0m  0.0045  4.1226\n",
      "     12  -0.4756  \u001b[32m5074999459.2000\u001b[0m  \u001b[35m6003568399.4985\u001b[0m  0.0045  3.9757\n",
      "     13  -0.3180  \u001b[32m4368382918.4000\u001b[0m  \u001b[35m5362319123.6099\u001b[0m  0.0045  4.2837\n",
      "     14  -0.2173  \u001b[32m3729575967.2000\u001b[0m  \u001b[35m4952472934.1857\u001b[0m  0.0045  3.4808\n",
      "     15  -0.0710  \u001b[32m3156080664.0000\u001b[0m  \u001b[35m4357449795.3265\u001b[0m  0.0045  3.5416\n",
      "     16  0.0694  \u001b[32m2645440239.2000\u001b[0m  \u001b[35m3785918490.3749\u001b[0m  0.0045  3.7725\n",
      "     17  0.1634  \u001b[32m2204945148.0000\u001b[0m  \u001b[35m3403560012.7078\u001b[0m  0.0045  3.5499\n",
      "     18  0.2167  \u001b[32m1870225290.4000\u001b[0m  \u001b[35m3186684146.0775\u001b[0m  0.0045  4.2537\n",
      "     19  0.3131  \u001b[32m1581323025.6000\u001b[0m  \u001b[35m2794670056.0483\u001b[0m  0.0045  4.0035\n",
      "     20  0.3718  \u001b[32m1319002239.6000\u001b[0m  \u001b[35m2555859454.6233\u001b[0m  0.0045  4.2459\n",
      "     21  0.3822  \u001b[32m1135966888.0000\u001b[0m  \u001b[35m2513433579.3000\u001b[0m  0.0045  3.7203\n",
      "     22  0.4546  \u001b[32m966753050.2000\u001b[0m  \u001b[35m2218812989.6391\u001b[0m  0.0045  3.8382\n",
      "     23  0.4655  \u001b[32m851995693.8000\u001b[0m  \u001b[35m2174628424.1137\u001b[0m  0.0045  3.2619\n",
      "     24  0.5012  \u001b[32m771375247.6000\u001b[0m  \u001b[35m2029407766.6155\u001b[0m  0.0045  3.8386\n",
      "     25  0.5108  \u001b[32m662426780.4000\u001b[0m  \u001b[35m1990391137.7909\u001b[0m  0.0045  4.0125\n",
      "     26  0.5174  \u001b[32m612876725.6000\u001b[0m  \u001b[35m1963605562.4621\u001b[0m  0.0045  3.3146\n",
      "     27  0.5332  \u001b[32m573528895.6000\u001b[0m  \u001b[35m1898959806.7697\u001b[0m  0.0045  3.3950\n",
      "     28  0.5344  \u001b[32m545459864.6000\u001b[0m  \u001b[35m1894267857.5386\u001b[0m  0.0045  3.6594\n",
      "     29  0.5423  \u001b[32m525507408.3000\u001b[0m  \u001b[35m1862259652.1986\u001b[0m  0.0045  3.7040\n",
      "     30  0.5543  \u001b[32m474602203.6000\u001b[0m  \u001b[35m1813291886.1343\u001b[0m  0.0045  4.0327\n",
      "     31  0.5543  477008832.9000  \u001b[35m1813278929.0746\u001b[0m  0.0045  3.9717\n",
      "     32  0.5615  \u001b[32m461066365.1000\u001b[0m  \u001b[35m1784136504.4407\u001b[0m  0.0045  3.3877\n",
      "     33  0.5735  \u001b[32m447276649.2000\u001b[0m  \u001b[35m1735219053.2062\u001b[0m  0.0045  3.6562\n",
      "     34  0.5717  450372713.5000  1742586182.7027  0.0045  3.5748\n",
      "     35  0.5715  \u001b[32m425481887.2000\u001b[0m  1743410411.9914  0.0045  3.4255\n",
      "     36  0.5739  \u001b[32m401187734.3000\u001b[0m  \u001b[35m1733438563.9494\u001b[0m  0.0045  3.2043\n",
      "     37  0.5699  \u001b[32m382140468.0000\u001b[0m  1749938832.3675  0.0045  3.2931\n",
      "     38  0.5804  \u001b[32m380084912.7000\u001b[0m  \u001b[35m1707318655.3895\u001b[0m  0.0045  2.9995\n",
      "     39  0.5859  \u001b[32m337895886.5000\u001b[0m  \u001b[35m1684858583.9798\u001b[0m  0.0045  2.8951\n",
      "     40  0.5804  339908020.6000  1707308836.9928  0.0045  3.6133\n",
      "     41  0.5887  \u001b[32m317129779.2000\u001b[0m  \u001b[35m1673355080.4906\u001b[0m  0.0045  2.9219\n",
      "     42  0.5791  \u001b[32m304716558.1500\u001b[0m  1712547825.3331  0.0045  3.2537\n",
      "     43  0.5921  316107951.1000  \u001b[35m1659655366.5221\u001b[0m  0.0045  2.8944\n",
      "     44  0.5876  320488459.5000  1677699885.5207  0.0045  3.0710\n",
      "     45  0.5736  324073740.0000  1734919569.2178  0.0045  3.4102\n",
      "     46  0.5934  \u001b[32m293549492.9000\u001b[0m  \u001b[35m1654066140.3683\u001b[0m  0.0045  2.8543\n",
      "     47  0.5916  297389989.6000  1661733023.4456  0.0045  3.2442\n",
      "     48  0.5891  \u001b[32m291107964.3000\u001b[0m  1671769873.6445  0.0045  2.9544\n",
      "     49  0.5879  304512509.4000  1676441853.0566  0.0045  3.3745\n",
      "     50  0.5867  322629995.6000  1681468451.5663  0.0045  2.8392\n",
      "     51  0.5944  319825116.1500  \u001b[35m1650090688.3177\u001b[0m  0.0045  4.2745\n",
      "     52  0.5962  297521381.0000  \u001b[35m1643033188.7685\u001b[0m  0.0045  3.1883\n",
      "     53  0.5977  297395790.6000  \u001b[35m1636705237.5347\u001b[0m  0.0045  4.0531\n",
      "     54  0.5871  \u001b[32m283289215.9000\u001b[0m  1679805291.6426  0.0045  3.9297\n",
      "     55  0.5903  284827334.6000  1666798148.5536  0.0045  3.6790\n",
      "     56  0.5967  284581564.5500  1640869145.8953  0.0045  4.2980\n",
      "     57  0.5872  \u001b[32m257878657.1500\u001b[0m  1679609021.6173  0.0045  3.9776\n",
      "     58  0.6041  280152061.5500  \u001b[35m1610847506.5260\u001b[0m  0.0045  3.8987\n",
      "     59  0.5974  278698978.4500  1638071373.7107  0.0045  3.5293\n",
      "     60  0.5959  267549549.1500  1644092729.8330  0.0045  3.6869\n",
      "     61  0.5905  285585589.8500  1666052056.9297  0.0045  3.2933\n",
      "     62  0.5948  273652959.4500  1648598189.7979  0.0045  3.2259\n",
      "     63  0.5931  272522932.8500  1655321818.8359  0.0045  3.1211\n",
      "     64  0.5952  279957680.7500  1647007909.0956  0.0045  3.1252\n",
      "     65  0.5918  258672947.9000  1660651323.6457  0.0022  2.9714\n",
      "     66  0.5958  \u001b[32m254897825.2000\u001b[0m  1644617829.3759  0.0022  2.9057\n",
      "     67  0.6007  \u001b[32m249319437.4500\u001b[0m  1624615481.0356  0.0022  3.2829\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 58.\n",
      "[CV 5/5] END batch_size=128, lambda1=0.0001, lr=0.00447, module__dropout_rate=0.5, module__n_units_last=256, module__num_hidden_layers=3;, score=0.690 total time= 4.0min\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-2.3329\u001b[0m  \u001b[32m12706319795.2000\u001b[0m  \u001b[35m13007331887.9228\u001b[0m  0.0045  4.4697\n",
      "      2  -2.2191  \u001b[32m12377656929.2800\u001b[0m  \u001b[35m12563127240.6616\u001b[0m  0.0045  3.8916\n",
      "      3  -2.0466  \u001b[32m11845894005.7600\u001b[0m  \u001b[35m11889995430.5734\u001b[0m  0.0045  4.1023\n",
      "      4  -1.8314  \u001b[32m11057916922.8800\u001b[0m  \u001b[35m11050161584.8198\u001b[0m  0.0045  4.0312\n",
      "      5  -1.6292  \u001b[32m10117888652.8000\u001b[0m  \u001b[35m10260926351.7284\u001b[0m  0.0045  4.0280\n",
      "      6  -1.3510  \u001b[32m9071190528.0000\u001b[0m  \u001b[35m9175021594.3137\u001b[0m  0.0045  3.9716\n",
      "      7  -1.0976  \u001b[32m7971495552.0000\u001b[0m  \u001b[35m8186166535.1366\u001b[0m  0.0045  3.6868\n",
      "      8  -0.8412  \u001b[32m6900648800.0000\u001b[0m  \u001b[35m7185752067.6680\u001b[0m  0.0045  4.0995\n",
      "      9  -0.5987  \u001b[32m5874362278.4000\u001b[0m  \u001b[35m6239101831.8343\u001b[0m  0.0045  3.5963\n",
      "     10  -0.4383  \u001b[32m4909138494.7200\u001b[0m  \u001b[35m5613372274.9428\u001b[0m  0.0045  4.2108\n",
      "     11  -0.2062  \u001b[32m4060213601.2800\u001b[0m  \u001b[35m4707466023.6698\u001b[0m  0.0045  3.8261\n",
      "     12  -0.0847  \u001b[32m3320107020.1600\u001b[0m  \u001b[35m4233415563.7415\u001b[0m  0.0045  3.9942\n",
      "     13  0.1566  \u001b[32m2691695167.0400\u001b[0m  \u001b[35m3291344045.8296\u001b[0m  0.0045  3.9950\n",
      "     14  0.2197  \u001b[32m2175445142.7200\u001b[0m  \u001b[35m3045278625.1911\u001b[0m  0.0045  4.2697\n",
      "     15  0.3340  \u001b[32m1757915249.2800\u001b[0m  \u001b[35m2599329077.6639\u001b[0m  0.0045  4.0282\n",
      "     16  0.3707  \u001b[32m1423912593.2800\u001b[0m  \u001b[35m2455886688.5233\u001b[0m  0.0045  3.9169\n",
      "     17  0.4474  \u001b[32m1187171723.2000\u001b[0m  \u001b[35m2156701825.4552\u001b[0m  0.0045  3.9528\n",
      "     18  0.4919  \u001b[32m991475654.7200\u001b[0m  \u001b[35m1982813528.0710\u001b[0m  0.0045  3.6463\n",
      "     19  0.5180  \u001b[32m850168296.4800\u001b[0m  \u001b[35m1881137413.6614\u001b[0m  0.0045  4.5988\n",
      "     20  0.5428  \u001b[32m759344991.2000\u001b[0m  \u001b[35m1784298683.9433\u001b[0m  0.0045  3.9202\n",
      "     21  0.5873  \u001b[32m654379830.2400\u001b[0m  \u001b[35m1610778098.9628\u001b[0m  0.0045  4.2310\n",
      "     22  0.5905  \u001b[32m611423672.4800\u001b[0m  \u001b[35m1598166159.3496\u001b[0m  0.0045  3.6713\n",
      "     23  0.5789  \u001b[32m568470387.6800\u001b[0m  1643357935.1353  0.0045  4.0944\n",
      "     24  0.5908  \u001b[32m522697803.1200\u001b[0m  \u001b[35m1596958299.6592\u001b[0m  0.0045  3.8702\n",
      "     25  0.6057  \u001b[32m519234205.7600\u001b[0m  \u001b[35m1538689524.5974\u001b[0m  0.0045  3.9628\n",
      "     26  0.6113  \u001b[32m484928836.2400\u001b[0m  \u001b[35m1516886239.8056\u001b[0m  0.0045  6.3957\n",
      "     27  0.6092  \u001b[32m455521387.1200\u001b[0m  1525334549.0509  0.0045  4.1160\n",
      "     28  0.6130  \u001b[32m437636247.2000\u001b[0m  \u001b[35m1510488319.6013\u001b[0m  0.0045  4.8204\n",
      "     29  0.6135  \u001b[32m398701272.8800\u001b[0m  \u001b[35m1508329417.9175\u001b[0m  0.0045  4.1577\n",
      "     30  0.6252  \u001b[32m375559468.2400\u001b[0m  \u001b[35m1462834056.3725\u001b[0m  0.0045  4.1558\n",
      "     31  0.6218  384150395.5200  1475806682.3834  0.0045  4.6937\n",
      "     32  0.6235  \u001b[32m353400794.2400\u001b[0m  1469179282.8183  0.0045  3.9218\n",
      "     33  0.6341  357111617.2000  \u001b[35m1428169075.8000\u001b[0m  0.0045  4.3524\n",
      "     34  0.6172  \u001b[32m344635570.6400\u001b[0m  1493776634.5778  0.0045  4.8439\n",
      "     35  0.6294  355270756.0000  1446238248.4872  0.0045  4.8097\n",
      "     36  0.6295  \u001b[32m337101585.7600\u001b[0m  1446133833.6184  0.0045  4.9784\n",
      "     37  0.6311  340631762.4800  1439839968.3638  0.0045  4.8161\n",
      "     38  0.6248  \u001b[32m325276510.0800\u001b[0m  1464286057.8925  0.0045  4.7343\n",
      "     39  0.6329  340501873.1200  1432841118.8189  0.0045  4.3060\n",
      "     40  0.6362  \u001b[32m301682101.9200\u001b[0m  \u001b[35m1419648760.7837\u001b[0m  0.0022  4.7243\n",
      "     41  0.6369  \u001b[32m287536181.2400\u001b[0m  \u001b[35m1417191112.6815\u001b[0m  0.0022  6.7154\n",
      "     42  0.6265  \u001b[32m281069161.6000\u001b[0m  1457730248.5220  0.0022  4.9533\n",
      "     43  0.6281  \u001b[32m273976244.2400\u001b[0m  1451229111.3384  0.0022  5.1145\n",
      "     44  0.6345  276979489.6000  1426486275.6281  0.0022  4.3318\n",
      "     45  0.6379  277327647.5200  \u001b[35m1413269091.6929\u001b[0m  0.0022  4.4487\n",
      "     46  0.6390  \u001b[32m261268679.7200\u001b[0m  \u001b[35m1408964672.2492\u001b[0m  0.0022  4.5646\n",
      "     47  0.6362  \u001b[32m259686777.6000\u001b[0m  1419983462.8824  0.0022  4.7176\n",
      "     48  0.6319  268703596.2800  1436704066.5217  0.0022  5.4091\n",
      "     49  0.6383  \u001b[32m253594910.2000\u001b[0m  1411472044.5538  0.0022  4.0698\n",
      "     50  0.6233  271686410.9200  1470245819.7041  0.0022  4.6011\n",
      "     51  0.6224  262776349.2000  1473765337.1874  0.0022  4.3750\n",
      "     52  0.6248  \u001b[32m251342570.6400\u001b[0m  1464429322.5055  0.0022  4.5619\n",
      "     53  0.6290  253594842.0400  1447776310.7603  0.0011  4.8677\n",
      "     54  0.6325  \u001b[32m236743881.6000\u001b[0m  1434168823.0494  0.0011  4.7267\n",
      "     55  0.6330  \u001b[32m230949817.5600\u001b[0m  1432174267.3453  0.0011  4.1534\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 46.\n"
     ]
    }
   ],
   "source": [
    "y_train_tensor = torch.tensor(np.array(y_train).reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "model = make_pipeline(\n",
    "    clone(preprocessor), \n",
    "    TensorTransformer(),\n",
    "    BayesSearchCV(\n",
    "        CustomNeuralNetRegressor(\n",
    "            Model,\n",
    "            max_epochs=100,\n",
    "            torch_load_kwargs={'weights_only': True},\n",
    "            criterion=nn.MSELoss,\n",
    "            optimizer=optim.AdamW,\n",
    "            iterator_train__shuffle=True,\n",
    "            iterator_train__drop_last=True,\n",
    "            train_split=dataset.ValidSplit(cv=5),\n",
    "            callbacks=[\n",
    "                EarlyStopping(patience=10, monitor='valid_loss', load_best=True),\n",
    "                LRScheduler(policy=optim.lr_scheduler.ReduceLROnPlateau, patience=5, factor=0.5, monitor='valid_loss'),  # type: ignore\n",
    "                EpochScoring(scoring='r2', on_train=False),\n",
    "            ],\n",
    "            device=DEVICE,\n",
    "        ),\n",
    "        # Comment to use tuned hyperparameters\n",
    "        {\n",
    "            'lambda1': [0.0001],\n",
    "            'lr': [0.00447],\n",
    "            'batch_size': [128],\n",
    "            'module__num_hidden_layers': [3],\n",
    "            'module__n_units_last': [256],\n",
    "            'module__dropout_rate': [0.5],\n",
    "        },\n",
    "        # Uncomment to tune hyperparameters\n",
    "        # { \n",
    "        #     'lambda1': (1e-4, 1e-1, 'log-uniform'),\n",
    "        #     'lr': (1e-4, 1e-1, 'log-uniform'),\n",
    "        #     'batch_size': [32, 64, 128, 256],\n",
    "        #     'module__num_hidden_layers': [1, 2, 3, 4],\n",
    "        #     'module__n_units_last': [16, 32, 64, 128, 256],\n",
    "        #     'module__dropout_rate': (0.1, 0.5, 'uniform'),\n",
    "        # },\n",
    "        verbose=3,\n",
    "        scoring='r2',\n",
    "        n_iter=1,\n",
    "        # n_iter=50,\n",
    "        random_state=42,\n",
    "        cv=KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    )\n",
    ").fit(X_train, y_train_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ce3f0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([240.29413304]),\n",
       " 'std_fit_time': array([12.30270861]),\n",
       " 'mean_score_time': array([0.24128766]),\n",
       " 'std_score_time': array([0.07713372]),\n",
       " 'param_batch_size': masked_array(data=[128],\n",
       "              mask=[False],\n",
       "        fill_value=999999),\n",
       " 'param_lambda1': masked_array(data=[0.0001],\n",
       "              mask=[False],\n",
       "        fill_value=1e+20),\n",
       " 'param_lr': masked_array(data=[0.00447],\n",
       "              mask=[False],\n",
       "        fill_value=1e+20),\n",
       " 'param_module__dropout_rate': masked_array(data=[0.5],\n",
       "              mask=[False],\n",
       "        fill_value=1e+20),\n",
       " 'param_module__n_units_last': masked_array(data=[256],\n",
       "              mask=[False],\n",
       "        fill_value=999999),\n",
       " 'param_module__num_hidden_layers': masked_array(data=[3],\n",
       "              mask=[False],\n",
       "        fill_value=999999),\n",
       " 'params': [OrderedDict([('batch_size', 128),\n",
       "               ('lambda1', 0.0001),\n",
       "               ('lr', 0.00447),\n",
       "               ('module__dropout_rate', 0.5),\n",
       "               ('module__n_units_last', 256),\n",
       "               ('module__num_hidden_layers', 3)])],\n",
       " 'split0_test_score': array([0.68948007]),\n",
       " 'split1_test_score': array([0.64286399]),\n",
       " 'split2_test_score': array([0.68415093]),\n",
       " 'split3_test_score': array([0.643534]),\n",
       " 'split4_test_score': array([0.69017935]),\n",
       " 'mean_test_score': array([0.67004167]),\n",
       " 'std_test_score': array([0.02201695]),\n",
       " 'rank_test_score': array([1], dtype=int32)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = model[-1]\n",
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffb657f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('batch_size', 128),\n",
       "             ('lambda1', 0.0001),\n",
       "             ('lr', 0.00447),\n",
       "             ('module__dropout_rate', 0.5),\n",
       "             ('module__n_units_last', 256),\n",
       "             ('module__num_hidden_layers', 3)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5e350ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 32103\n",
      "Train R2: 0.9043\n",
      "Train RMSE: 18642.3884\n",
      "Train MAE: 8384.3884\n"
     ]
    }
   ],
   "source": [
    "result_train = salary.evaluate_train_predictions(model.predict(X_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29620562",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f1bb3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_test, y_test) = salary.get_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7d3c255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 10000\n",
      "Test R2: 0.6765\n",
      "Test RMSE: 34085.9679\n",
      "Test MAE: 19781.8583\n"
     ]
    }
   ],
   "source": [
    "result_test = salary.evaluate_test_predictions(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9115cfa1",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4703ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_preprocessor = model[0]\n",
    "with open('models/preprocessor.cloudpickle', 'wb') as f:\n",
    "    cloudpickle.dump(trained_preprocessor, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2915a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = model[-1].best_estimator_\n",
    "net.save_params(f_params='models/mlp_params.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
