{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc2fc48f-dbcc-4e6c-bc0e-8d1e72e670a4",
   "metadata": {},
   "source": [
    "# Train Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3375fb27-0be7-4da8-9378-9137587226e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nginyc/repos/job_posting_salary_prediction/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import salary\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from skopt import BayesSearchCV\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import random\n",
    "import joblib\n",
    "from skorch import NeuralNetRegressor, dataset\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, EpochScoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eab292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be00b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train) = salary.get_train_dataset(include_extracted_salaries=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fba5d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32103, 3670)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = salary.get_preprocessor()\n",
    "(train_size, num_features) = clone(preprocessor).fit_transform(X_train, y_train).shape\n",
    "(train_size, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc83ff-8948-46b2-94bd-096093c31122",
   "metadata": {},
   "source": [
    "## Train & Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ec93e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_hidden_layers: int, n_units_last: int, dropout_rate: float):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_hidden_layers < 1 or num_hidden_layers > 4:\n",
    "            raise ValueError(\"num_hidden_layers must be between 1 and 4\")\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        layer_sizes = [n_units_last * (2 ** i) for i in reversed(range(num_hidden_layers))]\n",
    "        \n",
    "        # Add hidden layers based on num_hidden_layers parameter\n",
    "        for i in range(num_hidden_layers):\n",
    "            layer_size = layer_sizes[i]\n",
    "            self.layers.append(nn.LazyLinear(layer_size))\n",
    "            self.layers.append(nn.BatchNorm1d(layer_size))\n",
    "            self.layers.append(nn.LeakyReLU())\n",
    "            self.layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "         # Output layer\n",
    "        self.layers.append(nn.LazyLinear(1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "            \n",
    "        return X\n",
    "\n",
    "\n",
    "class CustomNeuralNetRegressor(NeuralNetRegressor):\n",
    "    def __init__(self, *args, lambda1=0.01, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.lambda1 = lambda1\n",
    "\n",
    "    def get_loss(self, y_pred, y_true, X=None, training=False):\n",
    "        loss = super().get_loss(y_pred, y_true, X=X, training=training)\n",
    "        # L1 regularization for only the first layer\n",
    "        loss += self.lambda1 * sum([w.abs().sum() for w in self.module_.layers[0].parameters()])\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9844899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-2.4580\u001b[0m  \u001b[32m12863820505.6000\u001b[0m  \u001b[35m12860708981.0115\u001b[0m  0.0045  2.2399\n",
      "      2  -2.3888  \u001b[32m12686198182.4000\u001b[0m  \u001b[35m12603213351.5686\u001b[0m  0.0045  2.4460\n",
      "      3  -2.2731  \u001b[32m12311920371.2000\u001b[0m  \u001b[35m12172921234.4637\u001b[0m  0.0045  2.2827\n",
      "      4  -2.1104  \u001b[32m11772511161.6000\u001b[0m  \u001b[35m11567903282.3329\u001b[0m  0.0045  2.2145\n",
      "      5  -1.9158  \u001b[32m11093443270.4000\u001b[0m  \u001b[35m10844347179.9042\u001b[0m  0.0045  2.4947\n",
      "      6  -1.7501  \u001b[32m10332852268.8000\u001b[0m  \u001b[35m10227930152.5653\u001b[0m  0.0045  2.1523\n",
      "      7  -1.5002  \u001b[32m9505489782.4000\u001b[0m  \u001b[35m9298722113.9311\u001b[0m  0.0045  2.1174\n",
      "      8  -1.3333  \u001b[32m8629473446.4000\u001b[0m  \u001b[35m8677978995.1676\u001b[0m  0.0045  2.6664\n",
      "      9  -1.1410  \u001b[32m7747103155.2000\u001b[0m  \u001b[35m7962777569.5013\u001b[0m  0.0045  2.1460\n",
      "     10  -0.8749  \u001b[32m6880349568.0000\u001b[0m  \u001b[35m6973019782.4536\u001b[0m  0.0045  2.4658\n",
      "     11  -0.6944  \u001b[32m6040608992.0000\u001b[0m  \u001b[35m6301668145.2365\u001b[0m  0.0045  2.4476\n",
      "     12  -0.5313  \u001b[32m5252903596.8000\u001b[0m  \u001b[35m5694931856.4205\u001b[0m  0.0045  2.1458\n",
      "     13  -0.3276  \u001b[32m4529115017.6000\u001b[0m  \u001b[35m4937333380.6595\u001b[0m  0.0045  2.1693\n",
      "     14  -0.1568  \u001b[32m3874538593.6000\u001b[0m  \u001b[35m4302111873.1711\u001b[0m  0.0045  2.4150\n",
      "     15  -0.0553  \u001b[32m3288537979.2000\u001b[0m  \u001b[35m3924920256.9095\u001b[0m  0.0045  2.1632\n",
      "     16  0.0545  \u001b[32m2774572080.8000\u001b[0m  \u001b[35m3516565515.7360\u001b[0m  0.0045  2.2041\n",
      "     17  0.1426  \u001b[32m2342559612.8000\u001b[0m  \u001b[35m3188621142.3882\u001b[0m  0.0045  2.3885\n",
      "     18  0.2170  \u001b[32m1957362800.8000\u001b[0m  \u001b[35m2912116645.8493\u001b[0m  0.0045  2.1845\n",
      "     19  0.3417  \u001b[32m1669135244.4000\u001b[0m  \u001b[35m2448311697.7037\u001b[0m  0.0045  2.1706\n",
      "     20  0.3834  \u001b[32m1414436598.4000\u001b[0m  \u001b[35m2293303141.4507\u001b[0m  0.0045  2.4869\n",
      "     21  0.4579  \u001b[32m1170389487.6000\u001b[0m  \u001b[35m2016200024.3815\u001b[0m  0.0045  2.1655\n",
      "     22  0.4762  \u001b[32m1048548762.2000\u001b[0m  \u001b[35m1948079007.1341\u001b[0m  0.0045  2.1814\n",
      "     23  0.4756  \u001b[32m917687722.2000\u001b[0m  1950219841.9809  0.0045  2.4104\n",
      "     24  0.5308  \u001b[32m840218608.0000\u001b[0m  \u001b[35m1744981999.7290\u001b[0m  0.0045  2.2750\n",
      "     25  0.5572  \u001b[32m733386266.2000\u001b[0m  \u001b[35m1646646821.1454\u001b[0m  0.0045  2.4006\n",
      "     26  0.5655  \u001b[32m659435690.0000\u001b[0m  \u001b[35m1616079987.5476\u001b[0m  0.0045  2.6938\n",
      "     27  0.5924  \u001b[32m603035243.4000\u001b[0m  \u001b[35m1515897305.4904\u001b[0m  0.0045  2.4766\n",
      "     28  0.5815  \u001b[32m580947365.4000\u001b[0m  1556532482.4481  0.0045  2.5170\n",
      "     29  0.5823  \u001b[32m565949593.9000\u001b[0m  1553646996.3200  0.0045  2.3686\n",
      "     30  0.5989  \u001b[32m512996369.3000\u001b[0m  \u001b[35m1491834615.9829\u001b[0m  0.0045  2.2933\n",
      "     31  0.6056  \u001b[32m468788712.4000\u001b[0m  \u001b[35m1466782605.3058\u001b[0m  0.0045  2.4100\n",
      "     32  0.6245  \u001b[32m442230773.3000\u001b[0m  \u001b[35m1396371049.7520\u001b[0m  0.0045  2.4053\n",
      "     33  0.6001  \u001b[32m441085528.6000\u001b[0m  1487335769.4686  0.0045  2.3309\n",
      "     34  0.6118  \u001b[32m428198794.4000\u001b[0m  1443597181.0722  0.0045  2.4368\n",
      "     35  0.5829  \u001b[32m412377383.1000\u001b[0m  1551249213.1470  0.0045  2.3797\n",
      "     36  0.6253  \u001b[32m386370530.2000\u001b[0m  \u001b[35m1393554763.9136\u001b[0m  0.0045  2.4233\n",
      "     37  0.6388  \u001b[32m357348783.0000\u001b[0m  \u001b[35m1343311446.5750\u001b[0m  0.0045  2.4725\n",
      "     38  0.6325  375558536.2000  1366788689.5854  0.0045  2.3086\n",
      "     39  0.6323  \u001b[32m345859384.7000\u001b[0m  1367614294.4255  0.0045  2.3403\n",
      "     40  0.6299  \u001b[32m345850976.8000\u001b[0m  1376449339.8170  0.0045  2.4749\n",
      "     41  0.6329  \u001b[32m330304338.7000\u001b[0m  1365367756.6081  0.0045  2.3334\n",
      "     42  0.6318  \u001b[32m322980621.9000\u001b[0m  1369284378.5992  0.0045  2.3337\n",
      "     43  0.6255  326489504.8000  1392761135.7820  0.0045  2.5783\n",
      "     44  0.6400  \u001b[32m309771478.4500\u001b[0m  \u001b[35m1339002639.1933\u001b[0m  0.0022  2.2924\n",
      "     45  0.6406  \u001b[32m278509634.5000\u001b[0m  \u001b[35m1336729749.0925\u001b[0m  0.0022  2.3085\n",
      "     46  0.6408  287709027.6000  \u001b[35m1335794161.2147\u001b[0m  0.0022  2.5980\n",
      "     47  0.6418  \u001b[32m267421856.0000\u001b[0m  \u001b[35m1332336085.2326\u001b[0m  0.0022  2.3451\n",
      "     48  0.6364  278710779.2500  1352107428.8495  0.0022  2.4243\n",
      "     49  0.6436  \u001b[32m262948009.5000\u001b[0m  \u001b[35m1325422404.4945\u001b[0m  0.0022  2.5352\n",
      "     50  0.6501  268440657.7500  \u001b[35m1301171008.4361\u001b[0m  0.0022  2.4914\n",
      "     51  0.6453  \u001b[32m261977335.7000\u001b[0m  1319276645.8836  0.0022  2.4854\n",
      "     52  0.6458  \u001b[32m256123090.6000\u001b[0m  1317224289.2116  0.0022  2.5774\n",
      "     53  0.6447  256198738.2000  1321339782.7619  0.0022  2.3539\n",
      "     54  0.6504  \u001b[32m234978161.4500\u001b[0m  \u001b[35m1300222770.2519\u001b[0m  0.0022  2.4865\n",
      "     55  0.6423  251855773.1000  1330414634.0510  0.0022  2.3411\n",
      "     56  0.6406  260276010.0000  1336761170.6412  0.0022  2.3156\n",
      "     57  0.6419  271065065.4500  1331846768.5201  0.0022  2.3763\n",
      "     58  0.6358  259939775.9500  1354373431.0329  0.0022  2.3309\n",
      "     59  0.6347  247034768.9500  1358575318.5782  0.0022  2.5480\n",
      "     60  0.6464  253490511.1500  1315181481.1259  0.0022  2.5362\n",
      "     61  0.6437  267887852.7500  1325052001.5511  0.0011  2.3060\n",
      "     62  0.6379  251617953.6500  1346875091.1832  0.0011  2.2645\n",
      "     63  0.6362  \u001b[32m224336088.6500\u001b[0m  1352913019.3841  0.0011  2.4228\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 54.\n",
      "[CV 1/5] END batch_size=128, lambda1=0.0001, lr=0.00447, module__dropout_rate=0.5, module__n_units_last=256, module__num_hidden_layers=3;, score=0.688 total time= 2.5min\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-2.3718\u001b[0m  \u001b[32m12715964985.6000\u001b[0m  \u001b[35m13019477955.0025\u001b[0m  0.0045  2.3182\n",
      "      2  -2.3026  \u001b[32m12535167808.0000\u001b[0m  \u001b[35m12752243099.8326\u001b[0m  0.0045  3.4840\n",
      "      3  -2.1823  \u001b[32m12157066828.8000\u001b[0m  \u001b[35m12287953479.6621\u001b[0m  0.0045  2.6070\n",
      "      4  -2.0442  \u001b[32m11621024499.2000\u001b[0m  \u001b[35m11754734575.0563\u001b[0m  0.0045  2.4447\n",
      "      5  -1.8576  \u001b[32m10956040396.8000\u001b[0m  \u001b[35m11033932940.5334\u001b[0m  0.0045  2.3606\n",
      "      6  -1.6586  \u001b[32m10167723484.8000\u001b[0m  \u001b[35m10265562675.9276\u001b[0m  0.0045  2.1974\n",
      "      7  -1.4889  \u001b[32m9336049667.2000\u001b[0m  \u001b[35m9610437898.1164\u001b[0m  0.0045  2.4485\n",
      "      8  -1.2373  \u001b[32m8467640268.8000\u001b[0m  \u001b[35m8638756474.6926\u001b[0m  0.0045  2.3295\n",
      "      9  -1.0774  \u001b[32m7574920592.0000\u001b[0m  \u001b[35m8021635333.1330\u001b[0m  0.0045  2.2240\n",
      "     10  -0.8419  \u001b[32m6710416443.2000\u001b[0m  \u001b[35m7111973820.0257\u001b[0m  0.0045  2.5114\n",
      "     11  -0.7045  \u001b[32m5902260344.0000\u001b[0m  \u001b[35m6581706275.7812\u001b[0m  0.0045  2.3569\n",
      "     12  -0.4527  \u001b[32m5131724614.4000\u001b[0m  \u001b[35m5609395547.4464\u001b[0m  0.0045  2.3239\n",
      "     13  -0.2857  \u001b[32m4400399113.6000\u001b[0m  \u001b[35m4964315262.6794\u001b[0m  0.0045  2.5463\n",
      "     14  -0.1825  \u001b[32m3750973969.6000\u001b[0m  \u001b[35m4565808925.6515\u001b[0m  0.0045  2.2311\n",
      "     15  -0.0194  \u001b[32m3172018630.4000\u001b[0m  \u001b[35m3936072776.3597\u001b[0m  0.0045  2.2776\n",
      "     16  0.0686  \u001b[32m2663159520.0000\u001b[0m  \u001b[35m3596483792.1588\u001b[0m  0.0045  2.5326\n",
      "     17  0.1625  \u001b[32m2247703802.0000\u001b[0m  \u001b[35m3233965170.4699\u001b[0m  0.0045  2.2662\n",
      "     18  0.2468  \u001b[32m1850830653.2000\u001b[0m  \u001b[35m2908258618.7549\u001b[0m  0.0045  2.2493\n",
      "     19  0.2658  \u001b[32m1568526852.0000\u001b[0m  \u001b[35m2834996425.1820\u001b[0m  0.0045  2.6413\n",
      "     20  0.3727  \u001b[32m1311379628.4000\u001b[0m  \u001b[35m2422074791.4939\u001b[0m  0.0045  2.2450\n",
      "     21  0.4026  \u001b[32m1124933635.8000\u001b[0m  \u001b[35m2306721106.6257\u001b[0m  0.0045  2.2446\n",
      "     22  0.4506  \u001b[32m955313262.6000\u001b[0m  \u001b[35m2121333087.8816\u001b[0m  0.0045  2.6096\n",
      "     23  0.4758  \u001b[32m860034831.2000\u001b[0m  \u001b[35m2023964409.0730\u001b[0m  0.0045  2.2033\n",
      "     24  0.5034  \u001b[32m740024343.6000\u001b[0m  \u001b[35m1917474138.4497\u001b[0m  0.0045  2.3721\n",
      "     25  0.5535  \u001b[32m653924622.8000\u001b[0m  \u001b[35m1723975807.7508\u001b[0m  0.0045  2.4552\n",
      "     26  0.5468  \u001b[32m599068080.2000\u001b[0m  1750001168.6697  0.0045  2.2392\n",
      "     27  0.5208  \u001b[32m546834184.5000\u001b[0m  1850525172.8121  0.0045  2.2987\n",
      "     28  0.5567  \u001b[32m508026241.1000\u001b[0m  \u001b[35m1711538393.1291\u001b[0m  0.0045  2.4959\n",
      "     29  0.5623  \u001b[32m479401968.5000\u001b[0m  \u001b[35m1689987911.6621\u001b[0m  0.0045  2.2242\n",
      "     30  0.5756  \u001b[32m465737185.3000\u001b[0m  \u001b[35m1638792586.2410\u001b[0m  0.0045  2.3754\n",
      "     31  0.5987  \u001b[32m436660516.1000\u001b[0m  \u001b[35m1549433086.1561\u001b[0m  0.0045  2.3999\n",
      "     32  0.5836  \u001b[32m436112082.2000\u001b[0m  1607777885.0162  0.0045  2.2080\n",
      "     33  0.5916  \u001b[32m393824476.2000\u001b[0m  1576814796.3963  0.0045  2.3742\n",
      "     34  0.5888  \u001b[32m380694120.6000\u001b[0m  1587916172.3091  0.0045  2.3713\n",
      "     35  0.6007  392416668.1000  \u001b[35m1541731590.7775\u001b[0m  0.0045  2.2288\n",
      "     36  0.5931  \u001b[32m363962523.5000\u001b[0m  1571220884.9056  0.0045  2.3529\n",
      "     37  0.6056  \u001b[32m329801569.2000\u001b[0m  \u001b[35m1522874038.8927\u001b[0m  0.0045  2.3853\n",
      "     38  0.6051  331792681.1000  1524799206.1608  0.0045  2.4325\n",
      "     39  0.5986  \u001b[32m312878216.6000\u001b[0m  1549816711.5250  0.0045  2.5841\n",
      "     40  0.6121  \u001b[32m305601782.0000\u001b[0m  \u001b[35m1497892695.4596\u001b[0m  0.0045  2.2204\n",
      "     41  0.6144  \u001b[32m296411935.0000\u001b[0m  \u001b[35m1488807879.8365\u001b[0m  0.0045  2.3056\n",
      "     42  0.6070  \u001b[32m291768944.9500\u001b[0m  1517613094.9706  0.0045  2.5571\n",
      "     43  0.6154  296393617.6000  \u001b[35m1485084791.4534\u001b[0m  0.0045  2.2458\n",
      "     44  0.6073  \u001b[32m287518003.0000\u001b[0m  1516201584.9001  0.0045  2.2311\n",
      "     45  0.6161  287750320.2000  \u001b[35m1482377417.4561\u001b[0m  0.0045  2.6117\n",
      "     46  0.6093  299782079.1000  1508638162.6506  0.0045  2.2122\n",
      "     47  0.6089  \u001b[32m273211145.6500\u001b[0m  1510254194.3952  0.0045  2.2367\n",
      "     48  0.6151  278279384.3000  1486160815.6667  0.0045  2.6194\n",
      "     49  0.6105  288026694.6000  1503830307.6317  0.0045  2.3391\n",
      "     50  0.6137  280872707.7500  1491776851.7968  0.0045  2.2492\n",
      "     51  0.6127  279827058.2000  1495345884.1690  0.0045  2.4997\n",
      "     52  0.6185  \u001b[32m254435752.8500\u001b[0m  \u001b[35m1473257688.3068\u001b[0m  0.0022  2.1991\n",
      "     53  0.6135  257881315.1000  1492242096.8129  0.0022  2.3347\n",
      "     54  0.6241  \u001b[32m249896968.9500\u001b[0m  \u001b[35m1451414576.2398\u001b[0m  0.0022  2.5532\n",
      "     55  0.6183  255232835.5000  1473794592.3675  0.0022  2.2123\n",
      "     56  0.6228  253815994.5000  1456415174.2667  0.0022  2.2923\n",
      "     57  0.6180  \u001b[32m223405984.9500\u001b[0m  1474996140.2032  0.0022  2.5351\n",
      "     58  0.6183  \u001b[32m222890818.0000\u001b[0m  1474029806.4582  0.0022  2.4257\n",
      "     59  0.6204  231855241.8000  1465751253.8898  0.0022  2.3860\n",
      "     60  0.6209  232449681.4500  1463823858.9683  0.0022  2.3813\n",
      "     61  0.6198  230986064.8500  1467961246.4738  0.0011  2.2425\n",
      "     62  0.6148  \u001b[32m218335579.7500\u001b[0m  1487453239.4658  0.0011  2.5523\n",
      "     63  0.6206  \u001b[32m202881877.6000\u001b[0m  1464978405.5379  0.0011  2.2672\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 54.\n",
      "[CV 2/5] END batch_size=128, lambda1=0.0001, lr=0.00447, module__dropout_rate=0.5, module__n_units_last=256, module__num_hidden_layers=3;, score=0.646 total time= 2.5min\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-2.3544\u001b[0m  \u001b[32m12716774278.4000\u001b[0m  \u001b[35m13029145434.9480\u001b[0m  0.0045  2.7941\n",
      "      2  -2.2828  \u001b[32m12533555558.4000\u001b[0m  \u001b[35m12750970754.2176\u001b[0m  0.0045  2.2609\n",
      "      3  -2.1708  \u001b[32m12171967020.8000\u001b[0m  \u001b[35m12316020743.1762\u001b[0m  0.0045  2.2575\n",
      "      4  -2.0159  \u001b[32m11633324294.4000\u001b[0m  \u001b[35m11714343438.2527\u001b[0m  0.0045  2.6690\n",
      "      5  -1.8463  \u001b[32m10976890208.0000\u001b[0m  \u001b[35m11055794717.4024\u001b[0m  0.0045  2.2480\n",
      "      6  -1.6759  \u001b[32m10207051136.0000\u001b[0m  \u001b[35m10393981731.1333\u001b[0m  0.0045  2.3701\n",
      "      7  -1.4359  \u001b[32m9374942851.2000\u001b[0m  \u001b[35m9461761404.9352\u001b[0m  0.0045  2.5108\n",
      "      8  -1.2318  \u001b[32m8496675580.8000\u001b[0m  \u001b[35m8668999144.5778\u001b[0m  0.0045  2.2501\n",
      "      9  -1.0399  \u001b[32m7626161196.8000\u001b[0m  \u001b[35m7923327754.8141\u001b[0m  0.0045  2.3582\n",
      "     10  -0.8465  \u001b[32m6754920588.8000\u001b[0m  \u001b[35m7172399506.8624\u001b[0m  0.0045  2.4286\n",
      "     11  -0.6612  \u001b[32m5918569512.0000\u001b[0m  \u001b[35m6452580736.3239\u001b[0m  0.0045  2.3193\n",
      "     12  -0.4901  \u001b[32m5146320371.2000\u001b[0m  \u001b[35m5787848654.4645\u001b[0m  0.0045  2.4733\n",
      "     13  -0.3259  \u001b[32m4425611133.6000\u001b[0m  \u001b[35m5150117846.7370\u001b[0m  0.0045  2.4039\n",
      "     14  -0.2104  \u001b[32m3773983737.6000\u001b[0m  \u001b[35m4701438195.9899\u001b[0m  0.0045  2.2995\n",
      "     15  -0.0444  \u001b[32m3217970782.4000\u001b[0m  \u001b[35m4056536766.5673\u001b[0m  0.0045  2.6628\n",
      "     16  0.1181  \u001b[32m2697890946.4000\u001b[0m  \u001b[35m3425371685.5752\u001b[0m  0.0045  2.2972\n",
      "     17  0.1694  \u001b[32m2263490716.4000\u001b[0m  \u001b[35m3226365831.9984\u001b[0m  0.0045  2.2316\n",
      "     18  0.2753  \u001b[32m1927489176.8000\u001b[0m  \u001b[35m2814994584.7927\u001b[0m  0.0045  2.5954\n",
      "     19  0.3018  \u001b[32m1617081491.2000\u001b[0m  \u001b[35m2712070353.3050\u001b[0m  0.0045  2.2537\n",
      "     20  0.4093  \u001b[32m1352397099.6000\u001b[0m  \u001b[35m2294596192.5295\u001b[0m  0.0045  2.2672\n",
      "     21  0.4742  \u001b[32m1181471169.2000\u001b[0m  \u001b[35m2042321845.2980\u001b[0m  0.0045  2.5699\n",
      "     22  0.4635  \u001b[32m999964076.4000\u001b[0m  2083834791.4939  0.0045  2.4726\n",
      "     23  0.5091  \u001b[32m867956507.4000\u001b[0m  \u001b[35m1906793363.9587\u001b[0m  0.0045  2.2710\n",
      "     24  0.5301  \u001b[32m773054778.6000\u001b[0m  \u001b[35m1825156211.4915\u001b[0m  0.0045  2.5959\n",
      "     25  0.5008  \u001b[32m699902458.2000\u001b[0m  1939028620.4337  0.0045  2.2529\n",
      "     26  0.5580  \u001b[32m642484145.6000\u001b[0m  \u001b[35m1716839398.9083\u001b[0m  0.0045  2.4798\n",
      "     27  0.5773  \u001b[32m581223626.8000\u001b[0m  \u001b[35m1641755256.5497\u001b[0m  0.0045  2.5410\n",
      "     28  0.5594  \u001b[32m535072688.3000\u001b[0m  1711495165.6079  0.0045  2.2211\n",
      "     29  0.5823  \u001b[32m532254280.5000\u001b[0m  \u001b[35m1622376274.0526\u001b[0m  0.0045  2.3814\n",
      "     30  0.5998  \u001b[32m522584193.5000\u001b[0m  \u001b[35m1554276000.4672\u001b[0m  0.0045  2.4080\n",
      "     31  0.6022  \u001b[32m480029138.0000\u001b[0m  \u001b[35m1545316074.8452\u001b[0m  0.0045  2.2639\n",
      "     32  0.6126  \u001b[32m461623380.8000\u001b[0m  \u001b[35m1504832345.0045\u001b[0m  0.0045  2.8015\n",
      "     33  0.6079  \u001b[32m456960506.0000\u001b[0m  1523165084.0319  0.0045  2.3423\n",
      "     34  0.6106  \u001b[32m430533961.0000\u001b[0m  1512528945.9591  0.0045  2.2718\n",
      "     35  0.6207  \u001b[32m425497167.1000\u001b[0m  \u001b[35m1473108056.7553\u001b[0m  0.0045  2.6175\n",
      "     36  0.6156  427818444.5000  1493143787.2938  0.0045  2.2271\n",
      "     37  0.6177  \u001b[32m408544693.5000\u001b[0m  1484895123.9089  0.0045  2.2420\n",
      "     38  0.6069  \u001b[32m398063309.2000\u001b[0m  1526903799.3786  0.0045  2.5617\n",
      "     39  0.6223  \u001b[32m371910071.4000\u001b[0m  \u001b[35m1467125205.6406\u001b[0m  0.0045  2.2476\n",
      "     40  0.6112  \u001b[32m361882527.6000\u001b[0m  1510072624.2149  0.0045  2.2516\n",
      "     41  0.6182  \u001b[32m328204644.4000\u001b[0m  1483074612.4010  0.0045  2.6803\n",
      "     42  0.6308  330945091.4000  \u001b[35m1434145749.2420\u001b[0m  0.0045  2.2808\n",
      "     43  0.6284  328451643.8000  1443537219.2766  0.0045  2.2479\n",
      "     44  0.6282  341762770.4000  1444125813.0364  0.0045  2.5891\n",
      "     45  0.6250  \u001b[32m320134493.5500\u001b[0m  1456494208.5233  0.0045  2.3249\n",
      "     46  0.6220  325763600.8000  1468366592.1993  0.0045  2.2568\n",
      "     47  0.6350  \u001b[32m308290851.5500\u001b[0m  \u001b[35m1417651534.5392\u001b[0m  0.0045  2.5505\n",
      "     48  0.6327  320275487.1000  1426833601.6072  0.0045  2.3595\n",
      "     49  0.6248  315547646.2000  1457438515.5538  0.0045  2.5001\n",
      "     50  0.6269  315565322.6000  1449109561.9576  0.0045  2.4750\n",
      "     51  0.6244  \u001b[32m294262121.7000\u001b[0m  1458736046.8445  0.0045  2.2681\n",
      "     52  0.6347  299866115.4500  1418963655.8116  0.0045  2.6918\n",
      "     53  0.6304  297752988.9500  1435712085.0675  0.0045  2.3104\n",
      "     54  0.6291  \u001b[32m288292629.7000\u001b[0m  1440531716.7592  0.0022  2.2734\n",
      "     55  0.6326  \u001b[32m257035538.0500\u001b[0m  1427129699.7937  0.0022  2.6175\n",
      "     56  0.6359  262889002.8000  \u001b[35m1414327034.5680\u001b[0m  0.0022  2.2511\n",
      "     57  0.6372  270749925.3500  \u001b[35m1409362749.0971\u001b[0m  0.0022  2.2239\n",
      "     58  0.6365  \u001b[32m251428962.6500\u001b[0m  1412085243.3156  0.0022  2.7921\n",
      "     59  0.6359  253680980.1000  1414424440.1760  0.0022  2.2272\n",
      "     60  0.6348  \u001b[32m246896692.4500\u001b[0m  1418512084.4945  0.0022  2.2507\n",
      "     61  0.6335  248491681.7000  1423705840.8005  0.0022  2.5619\n",
      "     62  0.6291  \u001b[32m242487780.8500\u001b[0m  1440835055.7539  0.0022  2.2425\n",
      "     63  0.6311  247476561.6500  1432770114.7284  0.0022  2.5523\n",
      "     64  0.6346  250913862.3500  1419347360.7164  0.0011  2.4847\n",
      "     65  0.6299  \u001b[32m238426133.2000\u001b[0m  1437549120.4859  0.0011  2.2635\n",
      "     66  0.6315  242984614.9000  1431288221.0785  0.0011  2.5152\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 57.\n",
      "[CV 3/5] END batch_size=128, lambda1=0.0001, lr=0.00447, module__dropout_rate=0.5, module__n_units_last=256, module__num_hidden_layers=3;, score=0.686 total time= 2.7min\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-2.2893\u001b[0m  \u001b[32m12680100313.6000\u001b[0m  \u001b[35m13019065563.4713\u001b[0m  0.0045  2.5489\n",
      "      2  -2.2153  \u001b[32m12500300627.2000\u001b[0m  \u001b[35m12726267575.2914\u001b[0m  0.0045  2.5877\n",
      "      3  -2.1053  \u001b[32m12130374380.8000\u001b[0m  \u001b[35m12290748454.3726\u001b[0m  0.0045  2.2501\n",
      "      4  -1.9681  \u001b[32m11603066995.2000\u001b[0m  \u001b[35m11747712666.6864\u001b[0m  0.0045  2.2643\n",
      "      5  -1.7860  \u001b[32m10943125942.4000\u001b[0m  \u001b[35m11027177126.6467\u001b[0m  0.0045  2.6384\n",
      "      6  -1.6263  \u001b[32m10162841536.0000\u001b[0m  \u001b[35m10394994632.5840\u001b[0m  0.0045  2.2432\n",
      "      7  -1.4074  \u001b[32m9334898003.2000\u001b[0m  \u001b[35m9528755852.8324\u001b[0m  0.0045  2.2923\n",
      "      8  -1.2156  \u001b[32m8470037456.0000\u001b[0m  \u001b[35m8769525150.9223\u001b[0m  0.0045  2.6150\n",
      "      9  -0.9787  \u001b[32m7589622940.8000\u001b[0m  \u001b[35m7831695985.8719\u001b[0m  0.0045  2.2552\n",
      "     10  -0.7791  \u001b[32m6708646521.6000\u001b[0m  \u001b[35m7041620762.4123\u001b[0m  0.0045  2.2744\n",
      "     11  -0.6426  \u001b[32m5882211657.6000\u001b[0m  \u001b[35m6501447041.6695\u001b[0m  0.0045  2.6162\n",
      "     12  -0.4659  \u001b[32m5120709193.6000\u001b[0m  \u001b[35m5802290838.6996\u001b[0m  0.0045  2.2843\n",
      "     13  -0.2541  \u001b[32m4408628369.6000\u001b[0m  \u001b[35m4963887096.9733\u001b[0m  0.0045  2.4535\n",
      "     14  -0.1400  \u001b[32m3760424387.2000\u001b[0m  \u001b[35m4512152930.5727\u001b[0m  0.0045  2.3969\n",
      "     15  0.0446  \u001b[32m3160604113.6000\u001b[0m  \u001b[35m3781503436.7452\u001b[0m  0.0045  2.2451\n",
      "     16  0.1136  \u001b[32m2700744980.0000\u001b[0m  \u001b[35m3508239752.0483\u001b[0m  0.0045  2.5438\n",
      "     17  0.2114  \u001b[32m2272523037.6000\u001b[0m  \u001b[35m3121350256.7008\u001b[0m  0.0045  2.2928\n",
      "     18  0.2680  \u001b[32m1897212283.2000\u001b[0m  \u001b[35m2897396845.0380\u001b[0m  0.0045  2.2375\n",
      "     19  0.3552  \u001b[32m1582463883.6000\u001b[0m  \u001b[35m2552100472.2383\u001b[0m  0.0045  2.5159\n",
      "     20  0.4163  \u001b[32m1362969101.4000\u001b[0m  \u001b[35m2310235038.7604\u001b[0m  0.0045  2.2784\n",
      "     21  0.4451  \u001b[32m1140731333.0000\u001b[0m  \u001b[35m2196360212.7685\u001b[0m  0.0045  2.2509\n",
      "     22  0.5167  \u001b[32m998525163.6000\u001b[0m  \u001b[35m1912886316.4773\u001b[0m  0.0045  2.7317\n",
      "     23  0.5238  \u001b[32m895566091.6000\u001b[0m  \u001b[35m1884848034.3734\u001b[0m  0.0045  2.2429\n",
      "     24  0.5604  \u001b[32m770897040.4000\u001b[0m  \u001b[35m1740084211.8777\u001b[0m  0.0045  2.2341\n",
      "     25  0.5623  \u001b[32m692860896.2000\u001b[0m  \u001b[35m1732620978.1585\u001b[0m  0.0045  2.6309\n",
      "     26  0.5541  \u001b[32m625637036.0000\u001b[0m  1765064631.4409  0.0045  2.2913\n",
      "     27  0.5829  \u001b[32m571690614.6000\u001b[0m  \u001b[35m1650842971.2034\u001b[0m  0.0045  2.2834\n",
      "     28  0.5791  \u001b[32m553414976.2000\u001b[0m  1665771029.2482  0.0045  2.5581\n",
      "     29  0.5894  \u001b[32m530984843.6000\u001b[0m  \u001b[35m1625292829.7886\u001b[0m  0.0045  2.2866\n",
      "     30  0.6022  \u001b[32m511198981.0000\u001b[0m  \u001b[35m1574494766.3212\u001b[0m  0.0045  2.3546\n",
      "     31  0.5834  \u001b[32m479369341.5000\u001b[0m  1648872987.0851  0.0045  2.5942\n",
      "     32  0.6237  \u001b[32m465381800.9000\u001b[0m  \u001b[35m1489374131.9961\u001b[0m  0.0045  2.2565\n",
      "     33  0.6069  \u001b[32m441775438.1000\u001b[0m  1556080150.5065  0.0045  2.4175\n",
      "     34  0.6143  \u001b[32m414811153.8000\u001b[0m  1526596160.0000  0.0045  2.4425\n",
      "     35  0.6119  \u001b[32m392927664.8000\u001b[0m  1536153181.6142  0.0045  2.2477\n",
      "     36  0.6188  394411058.6000  1508766982.0860  0.0045  2.5305\n",
      "     37  0.6256  \u001b[32m370960310.3000\u001b[0m  \u001b[35m1481771130.2441\u001b[0m  0.0045  2.6012\n",
      "     38  0.6342  \u001b[32m343702450.9000\u001b[0m  \u001b[35m1447823855.7539\u001b[0m  0.0045  2.3092\n",
      "     39  0.6329  \u001b[32m328952522.2000\u001b[0m  1452805159.7555  0.0045  2.7413\n",
      "     40  0.6280  330637897.3000  1472397417.2568  0.0045  2.2490\n",
      "     41  0.6301  333027848.4000  1464253989.2264  0.0045  2.2800\n",
      "     42  0.6245  \u001b[32m322536322.2000\u001b[0m  1486315680.1370  0.0045  2.6147\n",
      "     43  0.6383  324025856.7000  \u001b[35m1431457590.8554\u001b[0m  0.0045  2.3050\n",
      "     44  0.6386  \u001b[32m309598086.7000\u001b[0m  \u001b[35m1430524169.4063\u001b[0m  0.0045  2.2826\n",
      "     45  0.6193  313766418.3000  1506925721.7022  0.0045  2.6409\n",
      "     46  0.6381  \u001b[32m301018784.3000\u001b[0m  1432540830.5984  0.0045  2.2854\n",
      "     47  0.6362  305575312.6000  1439745343.8816  0.0045  2.3627\n",
      "     48  0.6347  \u001b[32m292879354.2000\u001b[0m  1445872653.4242  0.0045  2.6671\n",
      "     49  0.6338  311092939.6000  1449323363.9245  0.0045  2.2860\n",
      "     50  0.6361  \u001b[32m289878632.7500\u001b[0m  1440256010.7393  0.0045  2.5538\n",
      "     51  0.6387  \u001b[32m280446559.9500\u001b[0m  \u001b[35m1430071415.8707\u001b[0m  0.0022  2.4368\n",
      "     52  0.6341  \u001b[32m239994973.9500\u001b[0m  1448174737.9217  0.0022  2.2456\n",
      "     53  0.6393  247231881.5500  \u001b[35m1427542980.7717\u001b[0m  0.0022  2.5299\n",
      "     54  0.6394  240786841.0500  \u001b[35m1427245401.9202\u001b[0m  0.0022  2.3255\n",
      "     55  0.6401  249060110.6000  \u001b[35m1424449713.6663\u001b[0m  0.0022  2.2261\n",
      "     56  0.6428  258551837.7000  \u001b[35m1413905569.7691\u001b[0m  0.0022  2.4862\n",
      "     57  0.6367  242399855.4000  1437882580.9430  0.0022  2.3778\n",
      "     58  0.6381  \u001b[32m239561853.3000\u001b[0m  1432267873.0528  0.0022  2.2823\n",
      "     59  0.6365  252524346.5000  1438771155.0056  0.0022  2.5376\n",
      "     60  0.6404  \u001b[32m234648290.3000\u001b[0m  1423354797.5425  0.0022  2.3085\n",
      "     61  0.6345  \u001b[32m226611766.0000\u001b[0m  1446482979.2517  0.0022  2.2576\n",
      "     62  0.6411  245631517.5000  1420598274.8530  0.0022  2.7753\n",
      "     63  0.6415  \u001b[32m220261547.0000\u001b[0m  1418982248.3224  0.0011  2.4605\n",
      "     64  0.6444  \u001b[32m220097613.8500\u001b[0m  \u001b[35m1407563953.5231\u001b[0m  0.0011  2.3197\n",
      "     65  0.6444  227256584.9000  \u001b[35m1407461733.2949\u001b[0m  0.0011  2.5992\n",
      "     66  0.6346  221273129.8500  1446397877.1485  0.0011  2.2599\n",
      "     67  0.6397  224971068.9000  1426246896.0093  0.0011  2.2881\n",
      "     68  0.6456  227613081.1500  \u001b[35m1402834677.6157\u001b[0m  0.0011  2.6382\n",
      "     69  0.6406  244485115.6000  1422421694.4925  0.0011  2.2524\n",
      "     70  0.6408  \u001b[32m216182605.9500\u001b[0m  1421915646.8725  0.0011  2.3277\n",
      "     71  0.6369  237945757.1000  1437210838.7183  0.0011  2.4567\n",
      "     72  0.6426  \u001b[32m213923822.1000\u001b[0m  1414791354.9356  0.0011  2.2926\n",
      "     73  0.6341  226286898.3500  1448206428.7919  0.0011  2.4546\n",
      "     74  0.6415  214930941.3500  1418808750.9877  0.0011  2.4211\n",
      "     75  0.6372  226317391.4500  1436166951.4316  0.0006  2.2747\n",
      "     76  0.6341  \u001b[32m205139812.2000\u001b[0m  1448146926.7136  0.0006  2.6773\n",
      "     77  0.6451  \u001b[32m201246152.6500\u001b[0m  1404825845.3790  0.0006  2.3509\n",
      "     78  0.6457  226658865.1000  \u001b[35m1402138564.6035\u001b[0m  0.0006  2.2864\n",
      "     79  0.6401  \u001b[32m199682920.9500\u001b[0m  1424336005.0644  0.0006  2.5907\n",
      "     80  0.6431  226586491.9500  1412519274.6521  0.0006  2.3323\n",
      "     81  0.6404  \u001b[32m196258846.5500\u001b[0m  1423179286.9550  0.0006  2.2759\n",
      "     82  0.6365  211042609.0000  1438602970.4933  0.0006  2.5830\n",
      "     83  0.6384  208997206.7000  1431114289.1057  0.0006  2.2760\n",
      "     84  0.6415  208637671.1000  1418797360.9687  0.0006  2.2708\n",
      "     85  0.6395  203116686.1500  1426894071.5219  0.0003  2.6617\n",
      "     86  0.6196  \u001b[32m195310949.7000\u001b[0m  1505466973.5270  0.0003  2.2567\n",
      "     87  0.6437  204483455.8500  1410113007.2307  0.0003  2.3109\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 78.\n",
      "[CV 4/5] END batch_size=128, lambda1=0.0001, lr=0.00447, module__dropout_rate=0.5, module__n_units_last=256, module__num_hidden_layers=3;, score=0.644 total time= 3.5min\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-2.2669\u001b[0m  \u001b[32m12609184550.4000\u001b[0m  \u001b[35m13291240604.1814\u001b[0m  0.0045  2.5709\n",
      "      2  -2.1989  \u001b[32m12428729107.2000\u001b[0m  \u001b[35m13014665451.4183\u001b[0m  0.0045  2.4796\n",
      "      3  -2.0911  \u001b[32m12059943273.6000\u001b[0m  \u001b[35m12576045982.0253\u001b[0m  0.0045  2.4101\n",
      "      4  -1.9487  \u001b[32m11533675824.0000\u001b[0m  \u001b[35m11996477852.1316\u001b[0m  0.0045  2.2675\n",
      "      5  -1.7709  \u001b[32m10869937324.8000\u001b[0m  \u001b[35m11273408699.3778\u001b[0m  0.0045  2.7301\n",
      "      6  -1.6032  \u001b[32m10104676160.0000\u001b[0m  \u001b[35m10590828353.7816\u001b[0m  0.0045  2.2912\n",
      "      7  -1.4120  \u001b[32m9274311110.4000\u001b[0m  \u001b[35m9813259014.0798\u001b[0m  0.0045  2.2549\n",
      "      8  -1.2122  \u001b[32m8406585353.6000\u001b[0m  \u001b[35m9000400060.9726\u001b[0m  0.0045  2.5561\n",
      "      9  -1.0039  \u001b[32m7528705904.0000\u001b[0m  \u001b[35m8152736427.5305\u001b[0m  0.0045  2.2971\n",
      "     10  -0.8386  \u001b[32m6676779350.4000\u001b[0m  \u001b[35m7480320177.6103\u001b[0m  0.0045  2.2468\n",
      "     11  -0.6737  \u001b[32m5849537937.6000\u001b[0m  \u001b[35m6809378373.5192\u001b[0m  0.0045  2.6642\n",
      "     12  -0.4569  \u001b[32m5068543686.4000\u001b[0m  \u001b[35m5927504699.7267\u001b[0m  0.0045  2.2956\n",
      "     13  -0.3422  \u001b[32m4365087492.8000\u001b[0m  \u001b[35m5460583870.1437\u001b[0m  0.0045  2.2941\n",
      "     14  -0.1541  \u001b[32m3732817276.8000\u001b[0m  \u001b[35m4695588319.1590\u001b[0m  0.0045  2.5793\n",
      "     15  -0.0641  \u001b[32m3153551464.8000\u001b[0m  \u001b[35m4329386771.1240\u001b[0m  0.0045  2.3130\n",
      "     16  0.0098  \u001b[32m2655944064.0000\u001b[0m  \u001b[35m4028744323.3015\u001b[0m  0.0045  2.3355\n",
      "     17  0.1619  \u001b[32m2250434439.2000\u001b[0m  \u001b[35m3409865139.9525\u001b[0m  0.0045  2.4606\n",
      "     18  0.2293  \u001b[32m1876449957.2000\u001b[0m  \u001b[35m3135549051.2283\u001b[0m  0.0045  2.2511\n",
      "     19  0.2909  \u001b[32m1570808528.8000\u001b[0m  \u001b[35m2884773156.5474\u001b[0m  0.0045  2.5107\n",
      "     20  0.3336  \u001b[32m1347760772.8000\u001b[0m  \u001b[35m2711310165.1298\u001b[0m  0.0045  2.4021\n",
      "     21  0.3993  \u001b[32m1144710236.0000\u001b[0m  \u001b[35m2443925427.3919\u001b[0m  0.0045  2.2294\n",
      "     22  0.4434  \u001b[32m970677544.4000\u001b[0m  \u001b[35m2264530490.7674\u001b[0m  0.0045  2.4516\n",
      "     23  0.4376  \u001b[32m832893339.8000\u001b[0m  2288151451.7080  0.0045  2.3144\n",
      "     24  0.5054  \u001b[32m767604004.2000\u001b[0m  \u001b[35m2012061929.8454\u001b[0m  0.0045  2.3619\n",
      "     25  0.5299  \u001b[32m711349637.4000\u001b[0m  \u001b[35m1912609362.4076\u001b[0m  0.0045  2.5358\n",
      "     26  0.5285  \u001b[32m629909363.1000\u001b[0m  1918159508.3481  0.0045  2.8730\n",
      "     27  0.5316  \u001b[32m571102247.2000\u001b[0m  \u001b[35m1905697660.5022\u001b[0m  0.0045  2.4036\n",
      "     28  0.5410  \u001b[32m528453298.8000\u001b[0m  \u001b[35m1867555099.1224\u001b[0m  0.0045  2.5813\n",
      "     29  0.5453  \u001b[32m498321236.5000\u001b[0m  \u001b[35m1850108197.2716\u001b[0m  0.0045  2.2605\n",
      "     30  0.5635  \u001b[32m488052162.9000\u001b[0m  \u001b[35m1775803506.3235\u001b[0m  0.0045  2.3764\n",
      "     31  0.5599  \u001b[32m447149464.2000\u001b[0m  1790444168.8269  0.0045  2.7149\n",
      "     32  0.5701  464909249.5000  \u001b[35m1748952277.9459\u001b[0m  0.0045  2.4720\n",
      "     33  0.5669  \u001b[32m428775892.6000\u001b[0m  1762201291.5772  0.0045  2.4685\n",
      "     34  0.5786  432181021.0000  \u001b[35m1714439198.7791\u001b[0m  0.0045  2.3347\n",
      "     35  0.5558  \u001b[32m401119396.0000\u001b[0m  1807262319.4674  0.0045  2.2295\n",
      "     36  0.5830  \u001b[32m378403807.4000\u001b[0m  \u001b[35m1696577491.6099\u001b[0m  0.0045  2.5108\n",
      "     37  0.5801  \u001b[32m342527873.4000\u001b[0m  1708157362.9714  0.0045  2.2591\n",
      "     38  0.5685  \u001b[32m307461623.7000\u001b[0m  1755445016.6151  0.0045  2.3482\n",
      "     39  0.5880  336693882.6000  \u001b[35m1676375425.4296\u001b[0m  0.0045  2.5226\n",
      "     40  0.5938  327579979.2000  \u001b[35m1652739439.7103\u001b[0m  0.0045  2.2903\n",
      "     41  0.5912  314958670.3000  1663341653.9490  0.0045  2.2583\n",
      "     42  0.5831  313802963.4000  1696196289.4172  0.0045  2.5476\n",
      "     43  0.5996  \u001b[32m283408741.0000\u001b[0m  \u001b[35m1629008152.9142\u001b[0m  0.0045  2.2415\n",
      "     44  0.5931  318416914.5000  1655374081.5106  0.0045  2.2588\n",
      "     45  0.5921  288822453.8000  1659624069.3136  0.0045  2.5675\n",
      "     46  0.5955  286194128.5000  1645594774.1982  0.0045  2.2415\n",
      "     47  0.5966  290528897.1500  1641237172.4882  0.0045  2.2617\n",
      "     48  0.5976  \u001b[32m278033259.2500\u001b[0m  1637281406.6918  0.0045  2.5736\n",
      "     49  0.5963  285039173.6000  1642453782.9114  0.0045  2.2394\n",
      "     50  0.5977  \u001b[32m272236613.5000\u001b[0m  1636906346.1818  0.0022  2.3367\n",
      "     51  0.5922  277737825.6500  1659243343.7446  0.0022  2.6289\n",
      "     52  0.5905  \u001b[32m257718979.8500\u001b[0m  1666041060.5505  0.0022  2.4298\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 43.\n",
      "[CV 5/5] END batch_size=128, lambda1=0.0001, lr=0.00447, module__dropout_rate=0.5, module__n_units_last=256, module__num_hidden_layers=3;, score=0.677 total time= 2.1min\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-2.3312\u001b[0m  \u001b[32m12693187353.6000\u001b[0m  \u001b[35m13000536869.1979\u001b[0m  0.0045  3.0829\n",
      "      2  -2.2279  \u001b[32m12414995225.6000\u001b[0m  \u001b[35m12597301260.9176\u001b[0m  0.0045  2.8724\n",
      "      3  -2.0546  \u001b[32m11835506984.9600\u001b[0m  \u001b[35m11921057981.2989\u001b[0m  0.0045  3.1485\n",
      "      4  -1.8242  \u001b[32m11062783029.7600\u001b[0m  \u001b[35m11021941218.4171\u001b[0m  0.0045  2.7580\n",
      "      5  -1.5912  \u001b[32m10118210362.8800\u001b[0m  \u001b[35m10112748596.6273\u001b[0m  0.0045  3.1184\n",
      "      6  -1.3095  \u001b[32m9066732817.9200\u001b[0m  \u001b[35m9013285627.4150\u001b[0m  0.0045  2.8022\n",
      "      7  -1.0678  \u001b[32m7981161551.3600\u001b[0m  \u001b[35m8070070482.5093\u001b[0m  0.0045  3.0957\n",
      "      8  -0.7766  \u001b[32m6892472829.4400\u001b[0m  \u001b[35m6933389619.4711\u001b[0m  0.0045  2.8681\n",
      "      9  -0.5937  \u001b[32m5863744128.0000\u001b[0m  \u001b[35m6219793721.1325\u001b[0m  0.0045  3.0620\n",
      "     10  -0.3685  \u001b[32m4919410880.0000\u001b[0m  \u001b[35m5340808515.8972\u001b[0m  0.0045  2.9050\n",
      "     11  -0.2070  \u001b[32m4081710551.0400\u001b[0m  \u001b[35m4710530261.1406\u001b[0m  0.0045  2.8424\n",
      "     12  -0.0463  \u001b[32m3327688480.6400\u001b[0m  \u001b[35m4083526243.4337\u001b[0m  0.0045  3.0596\n",
      "     13  0.0995  \u001b[32m2697860834.5600\u001b[0m  \u001b[35m3514381632.9469\u001b[0m  0.0045  2.7974\n",
      "     14  0.2415  \u001b[32m2169619004.4800\u001b[0m  \u001b[35m2960154223.6736\u001b[0m  0.0045  3.0891\n",
      "     15  0.3608  \u001b[32m1758869820.4800\u001b[0m  \u001b[35m2494488875.6169\u001b[0m  0.0045  2.8258\n",
      "     16  0.4164  \u001b[32m1426666780.4800\u001b[0m  \u001b[35m2277717995.1484\u001b[0m  0.0045  3.0107\n",
      "     17  0.4511  \u001b[32m1188873808.9600\u001b[0m  \u001b[35m2142004787.4711\u001b[0m  0.0045  2.8215\n",
      "     18  0.4961  \u001b[32m980525723.3600\u001b[0m  \u001b[35m1966653571.0500\u001b[0m  0.0045  2.8719\n",
      "     19  0.4988  \u001b[32m842519084.6400\u001b[0m  \u001b[35m1955893489.5474\u001b[0m  0.0045  3.0804\n",
      "     20  0.5304  \u001b[32m734797861.7600\u001b[0m  \u001b[35m1832876379.6194\u001b[0m  0.0045  2.9747\n",
      "     21  0.5717  \u001b[32m673781912.3200\u001b[0m  \u001b[35m1671358674.3697\u001b[0m  0.0045  3.2531\n",
      "     22  0.5813  \u001b[32m638606979.2000\u001b[0m  \u001b[35m1634047188.0841\u001b[0m  0.0045  2.7941\n",
      "     23  0.5796  \u001b[32m587574860.0000\u001b[0m  1640604711.7296  0.0045  3.1376\n",
      "     24  0.5880  \u001b[32m529469886.4800\u001b[0m  \u001b[35m1607797052.5613\u001b[0m  0.0045  2.7541\n",
      "     25  0.6204  \u001b[32m512462794.0000\u001b[0m  \u001b[35m1481344239.7134\u001b[0m  0.0045  2.9573\n",
      "     26  0.6148  \u001b[32m485669875.6800\u001b[0m  1503377364.5825  0.0045  2.8991\n",
      "     27  0.6014  \u001b[32m454130934.8000\u001b[0m  1555508507.1509  0.0045  2.8211\n",
      "     28  0.6165  \u001b[32m452603057.8400\u001b[0m  1496706625.9835  0.0045  3.0924\n",
      "     29  0.6088  \u001b[32m405334397.7600\u001b[0m  1526689824.4734  0.0045  2.8189\n",
      "     30  0.6231  \u001b[32m403186745.2800\u001b[0m  \u001b[35m1470732405.8334\u001b[0m  0.0045  3.1339\n",
      "     31  0.6219  \u001b[32m373512057.8400\u001b[0m  1475708641.4801  0.0045  2.8246\n",
      "     32  0.6295  \u001b[32m368457975.5200\u001b[0m  \u001b[35m1446043746.8357\u001b[0m  0.0045  2.9623\n",
      "     33  0.6344  \u001b[32m360255498.4000\u001b[0m  \u001b[35m1426664412.9949\u001b[0m  0.0045  2.8781\n",
      "     34  0.6205  \u001b[32m342944350.7200\u001b[0m  1481246880.9718  0.0045  2.8146\n",
      "     35  0.6226  \u001b[32m329547796.8000\u001b[0m  1472740995.6879  0.0045  3.0927\n",
      "     36  0.6282  344478128.2400  1450876178.4395  0.0045  2.8127\n",
      "     37  0.6249  346766899.6000  1463915478.0975  0.0045  3.1530\n",
      "     38  0.6310  331815822.4800  1440082726.2545  0.0045  2.8240\n",
      "     39  0.6438  354945878.3200  \u001b[35m1390121991.7944\u001b[0m  0.0045  3.0851\n",
      "     40  0.6367  339715608.3200  1417832500.5874  0.0045  2.8085\n",
      "     41  0.6260  343090360.8000  1459422361.4365  0.0045  3.0946\n",
      "     42  0.6326  \u001b[32m316661446.4800\u001b[0m  1433913922.5815  0.0045  3.1188\n",
      "     43  0.6301  322333250.2400  1443733625.4814  0.0045  2.8482\n",
      "     44  0.6320  \u001b[32m298031814.4400\u001b[0m  1436319013.7362  0.0045  3.1209\n",
      "     45  0.6276  316083301.6800  1453173294.0888  0.0045  2.8250\n",
      "     46  0.6422  298090669.2800  1396203818.1816  0.0022  3.0935\n",
      "     47  0.6248  \u001b[32m253239390.1200\u001b[0m  1464093979.0313  0.0022  2.7896\n",
      "     48  0.6303  264858452.7600  1442903363.8374  0.0022  3.0550\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 39.\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class TensorTransformer(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return torch.tensor(X, dtype=torch.float32)\n",
    "    \n",
    "y_train_tensor = torch.tensor(np.array(y_train).reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "model = make_pipeline(\n",
    "    clone(preprocessor), \n",
    "    TensorTransformer(),\n",
    "    BayesSearchCV(\n",
    "        CustomNeuralNetRegressor(\n",
    "            Model,\n",
    "            max_epochs=100,\n",
    "            criterion=nn.MSELoss,\n",
    "            optimizer=optim.AdamW,\n",
    "            iterator_train__shuffle=True,\n",
    "            iterator_train__drop_last=True,\n",
    "            train_split=dataset.ValidSplit(cv=5),\n",
    "            callbacks=[\n",
    "                EarlyStopping(patience=10, monitor='valid_loss', load_best=True),\n",
    "                LRScheduler(policy=optim.lr_scheduler.ReduceLROnPlateau, patience=5, factor=0.5, monitor='valid_loss'),  # type: ignore\n",
    "                EpochScoring(scoring='r2', on_train=False),\n",
    "            ],\n",
    "            device=DEVICE,\n",
    "        ),\n",
    "        # Comment to use tuned hyperparameters\n",
    "        {\n",
    "            'lambda1': [0.0001],\n",
    "            'lr': [0.00447],\n",
    "            'batch_size': [128],\n",
    "            'module__num_hidden_layers': [3],\n",
    "            'module__n_units_last': [256],\n",
    "            'module__dropout_rate': [0.5],\n",
    "        },\n",
    "        # Uncomment to tune hyperparameters\n",
    "        # { \n",
    "        #     'lambda1': (1e-4, 1e-1, 'log-uniform'),\n",
    "        #     'lr': (1e-4, 1e-1, 'log-uniform'),\n",
    "        #     'batch_size': [32, 64, 128, 256],\n",
    "        #     'module__num_hidden_layers': [1, 2, 3, 4],\n",
    "        #     'module__n_units_last': [16, 32, 64, 128, 256],\n",
    "        #     'module__dropout_rate': (0.1, 0.5, 'uniform'),\n",
    "        # },\n",
    "        verbose=3,\n",
    "        scoring='r2',\n",
    "        n_iter=1,\n",
    "        # n_iter=50,\n",
    "        random_state=42,\n",
    "        cv=KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    )\n",
    ").fit(X_train, y_train_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ce3f0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([161.20745792]),\n",
       " 'std_fit_time': array([27.78315415]),\n",
       " 'mean_score_time': array([0.16876402]),\n",
       " 'std_score_time': array([0.00729483]),\n",
       " 'param_batch_size': masked_array(data=[128],\n",
       "              mask=[False],\n",
       "        fill_value=999999),\n",
       " 'param_lambda1': masked_array(data=[0.0001],\n",
       "              mask=[False],\n",
       "        fill_value=1e+20),\n",
       " 'param_lr': masked_array(data=[0.00447],\n",
       "              mask=[False],\n",
       "        fill_value=1e+20),\n",
       " 'param_module__dropout_rate': masked_array(data=[0.5],\n",
       "              mask=[False],\n",
       "        fill_value=1e+20),\n",
       " 'param_module__n_units_last': masked_array(data=[256],\n",
       "              mask=[False],\n",
       "        fill_value=999999),\n",
       " 'param_module__num_hidden_layers': masked_array(data=[3],\n",
       "              mask=[False],\n",
       "        fill_value=999999),\n",
       " 'params': [OrderedDict([('batch_size', 128),\n",
       "               ('lambda1', 0.0001),\n",
       "               ('lr', 0.00447),\n",
       "               ('module__dropout_rate', 0.5),\n",
       "               ('module__n_units_last', 256),\n",
       "               ('module__num_hidden_layers', 3)])],\n",
       " 'split0_test_score': array([0.68820858]),\n",
       " 'split1_test_score': array([0.6457184]),\n",
       " 'split2_test_score': array([0.68603957]),\n",
       " 'split3_test_score': array([0.6436944]),\n",
       " 'split4_test_score': array([0.67742282]),\n",
       " 'mean_test_score': array([0.66821675]),\n",
       " 'std_test_score': array([0.01954277]),\n",
       " 'rank_test_score': array([1], dtype=int32)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = model[-1]\n",
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffb657f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('batch_size', 128),\n",
       "             ('lambda1', 0.0001),\n",
       "             ('lr', 0.00447),\n",
       "             ('module__dropout_rate', 0.5),\n",
       "             ('module__n_units_last', 256),\n",
       "             ('module__num_hidden_layers', 3)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5e350ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 32103\n",
      "Train R2: 0.8977\n",
      "Train RMSE: 19280.7058\n",
      "Train MAE: 9552.3990\n"
     ]
    }
   ],
   "source": [
    "result_train = salary.evaluate_train_predictions(model.predict(X_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29620562",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f1bb3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_test, y_test) = salary.get_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7d3c255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 10000\n",
      "Test R2: 0.6726\n",
      "Test RMSE: 34290.0580\n",
      "Test MAE: 20141.6577\n"
     ]
    }
   ],
   "source": [
    "result_test = salary.evaluate_test_predictions(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7486e5e8",
   "metadata": {},
   "source": [
    "## Export Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ce51926",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7e12558",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_preds = pd.concat([X_test[['job_id']], pd.Series(y_test_preds.reshape(-1), name='predicted_salary')], axis=1)\n",
    "X_test_preds.to_csv('data/test_preds_mlp.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9115cfa1",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2915a060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/model_mlp.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'models/model_mlp.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
