{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc2fc48f-dbcc-4e6c-bc0e-8d1e72e670a4",
   "metadata": {},
   "source": [
    "# Salary Prediction from LinkedIn Job Postings - Train Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3375fb27-0be7-4da8-9378-9137587226e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nginyc/repos/job_posting_salary_prediction/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import salary\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, TargetEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import random\n",
    "from skorch import NeuralNetRegressor, dataset\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, EpochScoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eab292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc83ff-8948-46b2-94bd-096093c31122",
   "metadata": {},
   "source": [
    "## Train & Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfcb495",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train) = salary.get_train_dataset()\n",
    "(X_test, y_test) = salary.get_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06ea4d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('sbert_pca_encoder', make_pipeline(\n",
    "                salary.SentenceBertEncoder(),\n",
    "                StandardScaler(),\n",
    "                PCA(n_components=0.9, random_state=42) \n",
    "            ), ['title']),\n",
    "            ('one_hot_encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), ['clustered_edu_req', 'clustered_pref_qual', 'clustered_req_skill', 'location_state', 'company_industries', 'formatted_experience_level', 'formatted_work_type']),\n",
    "            ('target_encoder', make_pipeline(\n",
    "                TargetEncoder(random_state=42),\n",
    "                StandardScaler(),\n",
    "            ), ['norm_title', 'clustered_edu_req', 'clustered_pref_qual', 'clustered_req_skill', 'location_state', 'company_industries', 'formatted_experience_level', 'formatted_work_type']),\n",
    "            ('experience_level', salary.experience_level_encoder, ['formatted_experience_level']),\n",
    "            ('work_type', salary.work_type_encoder, ['formatted_work_type']),\n",
    "            ('remote_allowed', 'passthrough', ['remote_allowed']),\n",
    "            ('company_employee_count', make_pipeline(\n",
    "                SimpleImputer(strategy='median'),\n",
    "                StandardScaler(),\n",
    "            ), ['company_employee_count']),\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bee0b32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27885, 416)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_size, num_features) = clone(preprocessor).fit_transform(X_train, y_train).shape\n",
    "(train_size, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ec93e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_units_1=128, n_units_2=64, n_units_3=32,\n",
    "                dropout_rate=0.3, leaky_relu_slope=0.2):\n",
    "        super().__init__()\n",
    "        # Layer 1\n",
    "        self.linear1 = nn.Linear(num_features, n_units_1).double()\n",
    "        self.bn1 = nn.BatchNorm1d(n_units_1).double()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate).double()\n",
    "\n",
    "        # Layer 2\n",
    "        self.linear2 = nn.Linear(n_units_1, n_units_2).double()\n",
    "        self.bn2 = nn.BatchNorm1d(n_units_2).double()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate).double()\n",
    "\n",
    "        # Layer 3\n",
    "        self.linear3 = nn.Linear(n_units_2, n_units_3).double()\n",
    "        self.bn3 = nn.BatchNorm1d(n_units_3).double()\n",
    "        self.dropout3 = nn.Dropout(dropout_rate).double()\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(n_units_3, 1).double()\n",
    "\n",
    "        # Activation function\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=leaky_relu_slope).double()\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Layer 1\n",
    "        X = self.leaky_relu(self.linear1(X))\n",
    "        X = self.bn1(X)\n",
    "        X = self.dropout1(X)\n",
    "\n",
    "        # Layer 2\n",
    "        X = self.leaky_relu(self.linear2(X))\n",
    "        X = self.bn2(X)\n",
    "        X = self.dropout2(X)\n",
    "\n",
    "        # Layer 3\n",
    "        X = self.leaky_relu(self.linear3(X))\n",
    "        X = self.bn3(X)\n",
    "        X = self.dropout3(X)\n",
    "\n",
    "        # Output layer\n",
    "        X = self.output(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9844899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nginyc/repos/job_posting_salary_prediction/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-1.5616\u001b[0m  \u001b[32m12214716175.6372\u001b[0m  \u001b[35m10243652971.8926\u001b[0m  0.0500  0.8837\n",
      "      2  -0.3997  \u001b[32m7763566347.2989\u001b[0m  \u001b[35m5597123205.5185\u001b[0m  0.0500  0.7853\n",
      "      3  0.1612  \u001b[32m4353052402.4303\u001b[0m  \u001b[35m3354309960.0587\u001b[0m  0.0500  0.7356\n",
      "      4  0.3467  \u001b[32m2927128071.0143\u001b[0m  \u001b[35m2612572143.3684\u001b[0m  0.0500  0.6585\n",
      "      5  0.4717  \u001b[32m2407537233.4736\u001b[0m  \u001b[35m2112515329.5892\u001b[0m  0.0500  0.6905\n",
      "      6  0.4657  \u001b[32m2221162960.7383\u001b[0m  2136601366.6568  0.0500  0.7230\n",
      "      7  0.4539  \u001b[32m2144760811.9689\u001b[0m  2183656105.5181  0.0500  0.8007\n",
      "      8  0.4923  \u001b[32m2074109868.9030\u001b[0m  \u001b[35m2030251776.9537\u001b[0m  0.0500  0.8047\n",
      "      9  0.5092  \u001b[32m2033463272.6732\u001b[0m  \u001b[35m1962469826.6615\u001b[0m  0.0500  0.7950\n",
      "     10  0.5259  \u001b[32m2026239007.4539\u001b[0m  \u001b[35m1895881934.5715\u001b[0m  0.0500  0.8315\n",
      "     11  0.5243  \u001b[32m1961709617.6771\u001b[0m  1902369987.0009  0.0500  0.8321\n",
      "     12  0.5546  \u001b[32m1939334647.2045\u001b[0m  \u001b[35m1781209955.5896\u001b[0m  0.0500  0.8420\n",
      "     13  0.5510  \u001b[32m1900834548.7816\u001b[0m  1795649112.9343  0.0500  0.6242\n",
      "     14  0.5361  1927601077.0490  1854906266.6843  0.0500  0.6691\n",
      "     15  0.5412  \u001b[32m1870928264.4456\u001b[0m  1834698179.6859  0.0500  0.6611\n",
      "     16  0.5338  1880033843.4588  1864298682.9771  0.0500  0.5837\n",
      "     17  0.5358  1891235872.3664  1856133091.0622  0.0500  0.7195\n",
      "     18  0.5336  1876632543.9165  1865040910.6447  0.0500  0.9168\n",
      "     19  0.5546  \u001b[32m1762857947.0299\u001b[0m  \u001b[35m1781069826.9200\u001b[0m  0.0250  0.8473\n",
      "     20  0.5600  \u001b[32m1737782343.0169\u001b[0m  \u001b[35m1759506004.8826\u001b[0m  0.0250  0.8429\n",
      "     21  0.5700  \u001b[32m1730762402.1101\u001b[0m  \u001b[35m1719494428.3105\u001b[0m  0.0250  0.8532\n",
      "     22  0.5506  \u001b[32m1700899722.4573\u001b[0m  1797169392.0942  0.0250  0.9642\n",
      "     23  0.5430  1706019039.7289  1827449275.7251  0.0250  0.8324\n",
      "     24  0.5746  \u001b[32m1665188137.3292\u001b[0m  \u001b[35m1700949217.1681\u001b[0m  0.0250  0.7830\n",
      "     25  0.5670  1696461423.4913  1731503811.7704  0.0250  0.5692\n",
      "     26  0.5793  1721710380.4982  \u001b[35m1682437771.8291\u001b[0m  0.0250  0.6238\n",
      "     27  0.5512  \u001b[32m1660073178.9130\u001b[0m  1794643982.9575  0.0250  0.5923\n",
      "     28  0.5511  \u001b[32m1648256356.0514\u001b[0m  1794926917.3643  0.0250  0.6954\n",
      "     29  0.5584  1708658668.5916  1765777369.9367  0.0250  0.5126\n",
      "     30  0.5555  1666078676.2235  1777447522.0474  0.0250  0.6062\n",
      "     31  0.5682  1665789856.1159  1726919410.2384  0.0250  0.5641\n",
      "     32  0.5599  1662798429.8191  1760052697.9390  0.0250  0.6569\n",
      "     33  0.5626  \u001b[32m1619510162.0162\u001b[0m  1749010764.7864  0.0125  0.6097\n",
      "     34  0.5626  \u001b[32m1561108272.6630\u001b[0m  1749161096.3737  0.0125  0.6277\n",
      "     35  0.5728  \u001b[32m1528838351.1284\u001b[0m  1708267733.9329  0.0125  0.6523\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 26.\n",
      "  epoch       r2        train_loss       valid_loss      lr     dur\n",
      "-------  -------  ----------------  ---------------  ------  ------\n",
      "      1  \u001b[36m-1.7874\u001b[0m  \u001b[32m12014131047.6745\u001b[0m  \u001b[35m9893779610.5909\u001b[0m  0.0500  0.6270\n",
      "      2  -0.4374  \u001b[32m7566531057.8679\u001b[0m  \u001b[35m5102238926.3046\u001b[0m  0.0500  0.5907\n",
      "      3  0.1449  \u001b[32m4198632340.1521\u001b[0m  \u001b[35m3035098411.0651\u001b[0m  0.0500  0.6238\n",
      "      4  0.3740  \u001b[32m2813423616.7046\u001b[0m  \u001b[35m2222163629.5978\u001b[0m  0.0500  0.7129\n",
      "      5  0.4618  \u001b[32m2336933025.0140\u001b[0m  \u001b[35m1910443542.8213\u001b[0m  0.0500  0.7721\n",
      "      6  0.4760  \u001b[32m2149304429.4803\u001b[0m  \u001b[35m1859889474.6162\u001b[0m  0.0500  0.6169\n",
      "      7  0.4658  \u001b[32m2038172558.2634\u001b[0m  1896030226.5574  0.0500  0.6384\n",
      "      8  0.5001  \u001b[32m2000068592.4626\u001b[0m  \u001b[35m1774273727.2635\u001b[0m  0.0500  0.6133\n",
      "      9  0.5237  \u001b[32m1941911761.4704\u001b[0m  \u001b[35m1690784981.0659\u001b[0m  0.0500  0.5204\n",
      "     10  0.5366  \u001b[32m1894319831.4045\u001b[0m  \u001b[35m1644808134.7992\u001b[0m  0.0500  0.6950\n",
      "     11  0.5370  1906135243.9762  \u001b[35m1643264971.5885\u001b[0m  0.0500  0.5664\n",
      "     12  0.5449  \u001b[32m1858301489.6266\u001b[0m  \u001b[35m1615276653.1598\u001b[0m  0.0500  0.8446\n",
      "     13  0.5404  \u001b[32m1836720279.8001\u001b[0m  1631437160.4692  0.0500  0.5528\n",
      "     14  0.4921  \u001b[32m1832465152.8362\u001b[0m  1802900323.8054  0.0500  0.6193\n",
      "     15  0.5531  \u001b[32m1816543191.0574\u001b[0m  \u001b[35m1586384551.4304\u001b[0m  0.0500  0.5667\n",
      "     16  0.5386  1853172196.4511  1637643229.3168  0.0500  0.7990\n",
      "     17  0.5337  1819126451.2891  1655208125.3982  0.0500  0.7417\n",
      "     18  0.5673  \u001b[32m1805600471.9319\u001b[0m  \u001b[35m1535833949.5772\u001b[0m  0.0500  0.5982\n",
      "     19  0.5491  1814233877.1989  1600509494.4918  0.0500  0.5868\n",
      "     20  0.5542  1822222465.8780  1582297269.4539  0.0500  0.9225\n",
      "     21  0.5421  \u001b[32m1794014225.0240\u001b[0m  1625480916.3217  0.0500  0.6348\n",
      "     22  0.5603  \u001b[32m1777111694.4000\u001b[0m  1560611903.9761  0.0500  0.5601\n",
      "     23  0.5040  1832435317.6527  1760648501.0557  0.0500  0.6067\n",
      "     24  0.5692  1783797669.3350  \u001b[35m1529044141.4151\u001b[0m  0.0500  0.6596\n",
      "     25  0.5429  1783335316.2969  1622616081.0397  0.0500  0.5439\n",
      "     26  0.5721  1802263496.8963  \u001b[35m1518887150.3788\u001b[0m  0.0500  0.6000\n",
      "     27  0.5572  1816996030.5770  1571896815.1857  0.0500  0.5933\n",
      "     28  0.5518  \u001b[32m1750170653.9506\u001b[0m  1590753153.7829  0.0500  0.8442\n",
      "     29  0.5540  1779864648.1149  1582977896.5432  0.0500  0.7362\n",
      "     30  0.4916  1767064430.8929  1804668208.7712  0.0500  0.6214\n",
      "     31  0.5706  1783900953.3520  1524135665.7792  0.0500  0.5620\n",
      "     32  0.5737  1792626046.9708  \u001b[35m1513106330.0747\u001b[0m  0.0500  0.5358\n",
      "     33  0.5506  1786916863.4195  1595000784.8567  0.0500  0.5458\n",
      "     34  0.5591  1772908833.7860  1565027811.2315  0.0500  0.5303\n",
      "     35  0.5634  1802970860.1011  1549696137.7330  0.0500  0.4604\n",
      "     36  0.5756  1779252944.2745  \u001b[35m1506247562.5450\u001b[0m  0.0500  0.5867\n",
      "     37  0.5554  1787945594.6224  1578062759.9627  0.0500  0.6024\n",
      "     38  0.5398  1777772838.1516  1633475875.2303  0.0500  0.6697\n",
      "     39  0.5652  1800705367.8802  1543229490.0190  0.0500  0.5062\n",
      "     40  0.5524  1768781068.0161  1588638096.4920  0.0500  0.5186\n",
      "     41  0.5378  1792401104.4980  1640497776.5168  0.0500  0.4917\n",
      "     42  0.5662  1794208635.5277  1539865602.0444  0.0500  0.5235\n",
      "     43  0.5616  \u001b[32m1688672504.4713\u001b[0m  1556102071.0112  0.0250  0.5630\n",
      "     44  0.5832  \u001b[32m1643418702.1930\u001b[0m  \u001b[35m1479362085.4038\u001b[0m  0.0250  0.5148\n",
      "     45  0.5637  \u001b[32m1630728993.5775\u001b[0m  1548607689.0410  0.0250  0.5381\n",
      "     46  0.5819  \u001b[32m1615819373.1163\u001b[0m  1484149261.3720  0.0250  0.5271\n",
      "     47  0.5548  \u001b[32m1604794980.0805\u001b[0m  1580372176.8522  0.0250  0.8754\n",
      "     48  0.5678  \u001b[32m1593419050.9597\u001b[0m  1534060462.9671  0.0250  0.8053\n",
      "     49  0.5637  \u001b[32m1579106854.4072\u001b[0m  1548622833.9025  0.0250  0.5476\n",
      "     50  0.5835  1581947081.0813  \u001b[35m1478255357.0196\u001b[0m  0.0250  0.5537\n",
      "     51  0.5586  1585364736.4270  1566821942.8747  0.0250  0.4873\n",
      "     52  0.5877  1621313272.0964  \u001b[35m1463477911.5309\u001b[0m  0.0250  0.4966\n",
      "     53  0.5471  1583963674.5308  1607552064.1341  0.0250  0.4532\n",
      "     54  0.5588  \u001b[32m1574046138.7516\u001b[0m  1566071755.7661  0.0250  0.6092\n",
      "     55  0.5758  1612777200.5764  1505538280.7169  0.0250  0.6843\n",
      "     56  0.5462  1603741450.0526  1610916940.0992  0.0250  0.6047\n",
      "     57  0.5521  1592494933.7173  1589753246.5687  0.0250  0.4605\n",
      "     58  0.5423  1595927718.5988  1624678312.8139  0.0250  0.5838\n",
      "     59  0.5865  \u001b[32m1526153298.2203\u001b[0m  1467634523.6279  0.0125  0.4809\n",
      "     60  0.5650  \u001b[32m1452289052.9633\u001b[0m  1543906558.8339  0.0125  0.5290\n",
      "     61  0.5654  1452553972.1301  1542592900.2635  0.0125  0.4568\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 52.\n",
      "  epoch       r2        train_loss       valid_loss      lr     dur\n",
      "-------  -------  ----------------  ---------------  ------  ------\n",
      "      1  \u001b[36m-1.6771\u001b[0m  \u001b[32m12155819762.9532\u001b[0m  \u001b[35m9891485181.3565\u001b[0m  0.0500  0.5674\n",
      "      2  -0.5543  \u001b[32m7690157815.0704\u001b[0m  \u001b[35m5743083839.6302\u001b[0m  0.0500  0.5487\n",
      "      3  0.1392  \u001b[32m4260630545.9856\u001b[0m  \u001b[35m3180668516.7553\u001b[0m  0.0500  0.4533\n",
      "      4  0.3687  \u001b[32m2836038836.4612\u001b[0m  \u001b[35m2332623927.3461\u001b[0m  0.0500  0.6883\n",
      "      5  0.4509  \u001b[32m2366059461.7865\u001b[0m  \u001b[35m2028755971.4945\u001b[0m  0.0500  0.4523\n",
      "      6  0.5363  \u001b[32m2137341276.3351\u001b[0m  \u001b[35m1713334881.5783\u001b[0m  0.0500  0.4789\n",
      "      7  0.5153  \u001b[32m2057715406.5179\u001b[0m  1790803123.9419  0.0500  0.5306\n",
      "      8  0.5348  \u001b[32m2011279329.4672\u001b[0m  1718704933.6406  0.0500  0.4578\n",
      "      9  0.5339  \u001b[32m1995151664.2657\u001b[0m  1722170466.7495  0.0500  0.5097\n",
      "     10  0.5226  \u001b[32m1929191960.6240\u001b[0m  1763820653.5063  0.0500  0.4698\n",
      "     11  0.5402  \u001b[32m1911912702.5771\u001b[0m  \u001b[35m1698903639.8310\u001b[0m  0.0500  0.4746\n",
      "     12  0.5687  \u001b[32m1883422143.2557\u001b[0m  \u001b[35m1593472192.4957\u001b[0m  0.0500  0.6256\n",
      "     13  0.5684  \u001b[32m1850324946.9517\u001b[0m  1594885432.7059  0.0500  0.4816\n",
      "     14  0.5504  1879369242.0706  1661171019.7804  0.0500  0.5259\n",
      "     15  0.5341  1857473648.5411  1721496024.3333  0.0500  0.4780\n",
      "     16  0.5866  \u001b[32m1832984659.8661\u001b[0m  \u001b[35m1527532508.8575\u001b[0m  0.0500  0.4848\n",
      "     17  0.5730  1857515879.1316  1577886447.7087  0.0500  0.4568\n",
      "     18  0.5868  1862027854.7929  \u001b[35m1526641709.2889\u001b[0m  0.0500  0.4759\n",
      "     19  0.5777  1844063730.9789  1560419790.4542  0.0500  0.4803\n",
      "     20  0.5671  \u001b[32m1808720458.3963\u001b[0m  1599360353.9909  0.0500  0.5835\n",
      "     21  0.5658  1809520774.0008  1604400695.0328  0.0500  0.4507\n",
      "     22  0.5817  1845348732.7233  1545522265.9250  0.0500  0.5091\n",
      "     23  0.5903  \u001b[32m1807542588.3726\u001b[0m  \u001b[35m1513711075.0187\u001b[0m  0.0500  0.4615\n",
      "     24  0.5742  1833190531.8929  1573298974.4084  0.0500  0.5102\n",
      "     25  0.5847  1818418828.1582  1534409617.8321  0.0500  0.6106\n",
      "     26  0.5925  \u001b[32m1793723431.1674\u001b[0m  \u001b[35m1505624010.4636\u001b[0m  0.0500  0.4515\n",
      "     27  0.5760  1818871661.4269  1566688826.8114  0.0500  0.5023\n",
      "     28  0.5530  1828304107.4204  1651529732.4495  0.0500  0.4765\n",
      "     29  0.5838  1825530464.8651  1537756427.6220  0.0500  0.4859\n",
      "     30  0.5710  1826889448.4264  1584946043.8987  0.0500  0.4748\n",
      "     31  0.5735  1796320215.8183  1575767013.3882  0.0500  0.4451\n",
      "     32  0.5793  1814340070.8795  1554516627.4268  0.0500  0.6545\n",
      "     33  0.5687  \u001b[32m1731145395.3346\u001b[0m  1593770729.0175  0.0250  0.5142\n",
      "     34  0.5927  \u001b[32m1656238438.6409\u001b[0m  \u001b[35m1504912542.1579\u001b[0m  0.0250  0.5083\n",
      "     35  0.5478  1683031176.0266  1670765523.8231  0.0250  0.5713\n",
      "     36  0.6013  \u001b[32m1648134000.8046\u001b[0m  \u001b[35m1473328182.8865\u001b[0m  0.0250  0.5259\n",
      "     37  0.5952  1649452223.9295  1495838607.1956  0.0250  0.4648\n",
      "     38  0.5838  \u001b[32m1632761314.1684\u001b[0m  1537734514.1782  0.0250  0.4820\n",
      "     39  0.5905  \u001b[32m1595010282.4541\u001b[0m  1513075879.9175  0.0250  0.4608\n",
      "     40  0.5986  1599928730.5292  1483041039.9235  0.0250  0.5181\n",
      "     41  0.5877  1607041737.2261  1523376299.8432  0.0250  0.4951\n",
      "     42  0.5649  1595106629.6403  1607624315.8909  0.0250  0.5107\n",
      "     43  0.6047  \u001b[32m1539445266.3912\u001b[0m  \u001b[35m1460642559.0862\u001b[0m  0.0125  0.4737\n",
      "     44  0.5934  \u001b[32m1510998164.0862\u001b[0m  1502344029.5143  0.0125  0.4992\n",
      "     45  0.5969  \u001b[32m1455025312.0084\u001b[0m  1489545454.7815  0.0125  0.5741\n",
      "     46  0.5906  1464838048.4614  1512738022.8304  0.0125  0.4724\n",
      "     47  0.6086  1455675330.6583  \u001b[35m1446082358.4137\u001b[0m  0.0125  0.4726\n",
      "     48  0.6198  \u001b[32m1448044384.4714\u001b[0m  \u001b[35m1404696166.0825\u001b[0m  0.0125  0.4806\n",
      "     49  0.5680  \u001b[32m1434641687.4766\u001b[0m  1596221991.4946  0.0125  0.4546\n",
      "     50  0.6003  1459638610.7232  1476805913.5473  0.0125  0.5620\n",
      "     51  0.6006  1462893558.1157  1475775584.8187  0.0125  0.4702\n",
      "     52  0.5656  1457115984.1605  1605121745.4092  0.0125  0.5713\n",
      "     53  0.5887  \u001b[32m1432900541.8482\u001b[0m  1519829673.1263  0.0125  0.4520\n",
      "     54  0.6233  1464401999.6486  \u001b[35m1391863958.3454\u001b[0m  0.0125  0.4645\n",
      "     55  0.5910  \u001b[32m1410285464.6234\u001b[0m  1511024883.5070  0.0125  0.4435\n",
      "     56  0.5811  1442998204.5197  1547840564.5378  0.0125  0.6242\n",
      "     57  0.6039  \u001b[32m1394967651.4132\u001b[0m  1463372721.2513  0.0125  0.4583\n",
      "     58  0.5483  1416691519.2120  1668822686.3991  0.0125  0.5034\n",
      "     59  0.5964  1414168181.4311  1491125849.3732  0.0125  0.4814\n",
      "     60  0.6120  1406000493.6199  1433729580.6811  0.0125  0.5499\n",
      "     61  0.6316  1406332143.2682  \u001b[35m1361273726.3120\u001b[0m  0.0063  0.4878\n",
      "     62  0.6146  \u001b[32m1351632310.8694\u001b[0m  1423994613.6521  0.0063  0.4653\n",
      "     63  0.6219  \u001b[32m1313586680.5904\u001b[0m  1397054125.2675  0.0063  0.4607\n",
      "     64  0.5992  1334947783.7810  1480737377.6084  0.0063  0.5660\n",
      "     65  0.5968  \u001b[32m1288126448.9675\u001b[0m  1489724001.1770  0.0063  0.4572\n",
      "     66  0.6222  1340012667.3380  1395753739.5461  0.0063  0.5941\n",
      "     67  0.6286  1311453591.7622  1372294787.4543  0.0063  0.4528\n",
      "     68  0.5888  1304700315.5395  1519473350.2761  0.0031  0.5285\n",
      "     69  0.6027  \u001b[32m1254472412.0192\u001b[0m  1467910124.7710  0.0031  0.4516\n",
      "     70  0.6045  \u001b[32m1237628864.8886\u001b[0m  1461440879.5310  0.0031  0.5126\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 61.\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-1.5770\u001b[0m  \u001b[32m12156364842.8188\u001b[0m  \u001b[35m10200874941.0166\u001b[0m  0.0500  0.5710\n",
      "      2  -0.4248  \u001b[32m7679833679.8177\u001b[0m  \u001b[35m5640015255.8228\u001b[0m  0.0500  0.4956\n",
      "      3  0.0855  \u001b[32m4284107844.9280\u001b[0m  \u001b[35m3619878517.3253\u001b[0m  0.0500  0.5018\n",
      "      4  0.3910  \u001b[32m2877837079.9764\u001b[0m  \u001b[35m2410763588.0799\u001b[0m  0.0500  0.5264\n",
      "      5  0.3994  \u001b[32m2372627697.7833\u001b[0m  \u001b[35m2377420080.9203\u001b[0m  0.0500  0.6140\n",
      "      6  0.4804  \u001b[32m2206936270.6621\u001b[0m  \u001b[35m2056665148.2672\u001b[0m  0.0500  0.5127\n",
      "      7  0.4680  \u001b[32m2099677623.9804\u001b[0m  2105754975.9626  0.0500  0.4789\n",
      "      8  0.5032  \u001b[32m2039617493.1394\u001b[0m  \u001b[35m1966666916.1550\u001b[0m  0.0500  0.5552\n",
      "      9  0.5039  \u001b[32m1952381093.5912\u001b[0m  \u001b[35m1963832761.7848\u001b[0m  0.0500  0.5345\n",
      "     10  0.5131  \u001b[32m1949684042.0018\u001b[0m  \u001b[35m1927388861.4648\u001b[0m  0.0500  0.5779\n",
      "     11  0.5015  \u001b[32m1939347160.2621\u001b[0m  1973455386.4687  0.0500  0.4871\n",
      "     12  0.5240  \u001b[32m1893006247.6601\u001b[0m  \u001b[35m1884158751.8066\u001b[0m  0.0500  0.5206\n",
      "     13  0.5269  \u001b[32m1885731600.8907\u001b[0m  \u001b[35m1872861439.7114\u001b[0m  0.0500  0.4816\n",
      "     14  0.5057  1894928204.5341  1956669942.3535  0.0500  0.5945\n",
      "     15  0.5164  \u001b[32m1879806223.6535\u001b[0m  1914363090.6972  0.0500  0.5667\n",
      "     16  0.5249  \u001b[32m1871010605.1863\u001b[0m  1880831294.3236  0.0500  0.5224\n",
      "     17  0.5139  \u001b[32m1859047583.5433\u001b[0m  1924312019.2972  0.0500  0.4935\n",
      "     18  0.5404  \u001b[32m1855745005.2692\u001b[0m  \u001b[35m1819225297.2781\u001b[0m  0.0500  0.5260\n",
      "     19  0.5263  1882817682.6982  1875035281.7607  0.0500  0.5305\n",
      "     20  0.5379  \u001b[32m1841715221.3710\u001b[0m  1829338436.6970  0.0500  0.4956\n",
      "     21  0.5387  1852002993.4493  1825914460.2921  0.0500  0.5025\n",
      "     22  0.5273  1870477988.5137  1871071580.3393  0.0500  0.4692\n",
      "     23  0.4917  \u001b[32m1815536524.9709\u001b[0m  2012230229.0213  0.0500  0.5199\n",
      "     24  0.5459  1840143472.6967  \u001b[35m1797437382.6372\u001b[0m  0.0500  0.4971\n",
      "     25  0.5413  1834711660.8680  1815737612.5955  0.0500  0.6339\n",
      "     26  0.5400  1850463960.1377  1820982526.8280  0.0500  0.4773\n",
      "     27  0.5613  1854666313.6485  \u001b[35m1736715810.8840\u001b[0m  0.0500  0.5986\n",
      "     28  0.5320  1826011901.3069  1852758282.9299  0.0500  0.4799\n",
      "     29  0.5433  1822387559.0501  1807909052.4968  0.0500  0.5509\n",
      "     30  0.5241  1825089820.2951  1883924477.2782  0.0500  0.4748\n",
      "     31  0.5393  1853036344.3955  1823581566.8022  0.0500  0.5503\n",
      "     32  0.5358  1841330877.6015  1837476808.5098  0.0500  0.8054\n",
      "     33  0.5502  1844377974.0003  1780554947.9445  0.0500  0.6607\n",
      "     34  0.5542  \u001b[32m1710110020.6612\u001b[0m  1764622650.7766  0.0250  0.8739\n",
      "     35  0.5713  \u001b[32m1688094896.6263\u001b[0m  \u001b[35m1696883053.9912\u001b[0m  0.0250  0.6300\n",
      "     36  0.5526  \u001b[32m1670677115.8684\u001b[0m  1770841365.1368  0.0250  0.9348\n",
      "     37  0.5896  1693961703.5048  \u001b[35m1624560911.2767\u001b[0m  0.0250  0.6372\n",
      "     38  0.5792  \u001b[32m1645817518.2592\u001b[0m  1665850963.8047  0.0250  0.7611\n",
      "     39  0.5054  1648565661.4518  1957997245.7379  0.0250  0.6720\n",
      "     40  0.5466  1650144067.0662  1794949359.3675  0.0250  0.8221\n",
      "     41  0.5384  \u001b[32m1642273422.6906\u001b[0m  1827056053.8931  0.0250  0.8480\n",
      "     42  0.5478  \u001b[32m1631959002.8015\u001b[0m  1790159324.7952  0.0250  0.8093\n",
      "     43  0.5658  1652463305.2252  1718627614.7918  0.0250  0.7221\n",
      "     44  0.5721  \u001b[32m1570048838.7108\u001b[0m  1693675705.7405  0.0125  0.7623\n",
      "     45  0.5726  \u001b[32m1531755738.1755\u001b[0m  1691888664.1330  0.0125  0.6489\n",
      "     46  0.5569  \u001b[32m1508904461.6662\u001b[0m  1754018763.0666  0.0125  0.7531\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 37.\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-1.6787\u001b[0m  \u001b[32m12243424929.9527\u001b[0m  \u001b[35m10281683244.2948\u001b[0m  0.0500  0.9431\n",
      "      2  -0.5172  \u001b[32m7766827807.2153\u001b[0m  \u001b[35m5823372849.4408\u001b[0m  0.0500  0.7136\n",
      "      3  0.1659  \u001b[32m4324154476.4786\u001b[0m  \u001b[35m3201487133.0243\u001b[0m  0.0500  0.7271\n",
      "      4  0.3547  \u001b[32m2905780982.0359\u001b[0m  \u001b[35m2476967829.5276\u001b[0m  0.0500  0.6831\n",
      "      5  0.4687  \u001b[32m2424406569.9524\u001b[0m  \u001b[35m2039270453.0684\u001b[0m  0.0500  0.5234\n",
      "      6  0.4516  \u001b[32m2205779062.6467\u001b[0m  2104893191.1241  0.0500  0.7049\n",
      "      7  0.5328  \u001b[32m2143646950.7658\u001b[0m  \u001b[35m1793308625.7690\u001b[0m  0.0500  0.5236\n",
      "      8  0.5101  \u001b[32m2088469482.7339\u001b[0m  1880310348.3299  0.0500  0.7143\n",
      "      9  0.5204  \u001b[32m2010881176.8128\u001b[0m  1840997373.9396  0.0500  0.7026\n",
      "     10  0.5112  \u001b[32m2000746512.0104\u001b[0m  1876218289.9048  0.0500  0.6591\n",
      "     11  0.5481  \u001b[32m1978696755.0573\u001b[0m  \u001b[35m1734496343.3835\u001b[0m  0.0500  0.5515\n",
      "     12  0.5168  \u001b[32m1942329646.8949\u001b[0m  1854531982.7775  0.0500  0.5460\n",
      "     13  0.5278  \u001b[32m1933337003.0580\u001b[0m  1812520711.6858  0.0500  0.6584\n",
      "     14  0.5394  1944536266.4411  1767953355.9496  0.0500  0.6006\n",
      "     15  0.5559  \u001b[32m1912750192.1549\u001b[0m  \u001b[35m1704470696.5233\u001b[0m  0.0500  0.5555\n",
      "     16  0.5604  \u001b[32m1884149816.7554\u001b[0m  \u001b[35m1687369737.7691\u001b[0m  0.0500  0.5177\n",
      "     17  0.5193  1896103043.3043  1845116384.6392  0.0500  0.7201\n",
      "     18  0.5188  1916952027.7229  1847023968.7071  0.0500  0.6076\n",
      "     19  0.5643  \u001b[32m1879933842.2954\u001b[0m  \u001b[35m1672237697.2397\u001b[0m  0.0500  0.5706\n",
      "     20  0.5093  1899723459.1736  1883257772.1312  0.0500  0.5296\n",
      "     21  0.5566  1882637857.2501  1701960087.4221  0.0500  0.6346\n",
      "     22  0.5600  1896401291.5652  1688710609.3101  0.0500  0.5431\n",
      "     23  0.5350  \u001b[32m1873494082.4167\u001b[0m  1784857925.0519  0.0500  0.5237\n",
      "     24  0.5458  1897099092.8853  1743155549.1983  0.0500  0.4948\n",
      "     25  0.5528  \u001b[32m1857291695.2028\u001b[0m  1716573998.5269  0.0500  0.5852\n",
      "     26  0.5321  \u001b[32m1791776985.0119\u001b[0m  1795966877.2294  0.0250  0.5109\n",
      "     27  0.5653  \u001b[32m1726690975.8449\u001b[0m  \u001b[35m1668353880.0891\u001b[0m  0.0250  0.7356\n",
      "     28  0.5406  1727515254.5436  1763189666.3754  0.0250  0.4827\n",
      "     29  0.5667  1739924571.8865  \u001b[35m1663223069.9945\u001b[0m  0.0250  0.5123\n",
      "     30  0.5377  \u001b[32m1699295037.6765\u001b[0m  1774475733.0372  0.0250  0.5068\n",
      "     31  0.5637  \u001b[32m1655819045.8243\u001b[0m  1674625506.0946  0.0250  0.5326\n",
      "     32  0.5744  1688256740.7509  \u001b[35m1633655392.2988\u001b[0m  0.0250  0.5091\n",
      "     33  0.5714  1690486306.1074  1644998650.7748  0.0250  0.5645\n",
      "     34  0.5844  \u001b[32m1620330815.7570\u001b[0m  \u001b[35m1595058749.1936\u001b[0m  0.0250  0.6094\n",
      "     35  0.5801  1667851549.7595  1611870757.4114  0.0250  0.4920\n",
      "     36  0.5718  1677582535.2508  1643577743.5190  0.0250  0.6713\n",
      "     37  0.5417  1701509019.0487  1759079816.6453  0.0250  0.5650\n",
      "     38  0.5631  1686550441.8141  1676779694.1349  0.0250  0.4852\n",
      "     39  0.5710  1654562343.9752  1646569621.4283  0.0250  0.4968\n",
      "     40  0.5708  1692335148.8306  1647346653.5278  0.0250  0.5335\n",
      "     41  0.5569  \u001b[32m1586147925.1425\u001b[0m  1700639901.1841  0.0125  0.5270\n",
      "     42  0.5694  \u001b[32m1540332690.3730\u001b[0m  1652888957.3813  0.0125  0.5072\n",
      "     43  0.5584  \u001b[32m1505776480.0991\u001b[0m  1695024749.4621  0.0125  0.5040\n",
      "     44  0.5898  1538138485.6586  \u001b[35m1574591866.8766\u001b[0m  0.0125  0.6231\n",
      "     45  0.5824  \u001b[32m1495970927.7263\u001b[0m  1602734116.6564  0.0125  0.5548\n",
      "     46  0.5706  \u001b[32m1484897351.9071\u001b[0m  1648306333.9399  0.0125  0.6574\n",
      "     47  0.5660  \u001b[32m1469385269.6400\u001b[0m  1665887935.6297  0.0125  0.5461\n",
      "     48  0.5786  1477420533.1487  1617599920.1821  0.0125  0.5651\n",
      "     49  0.5742  \u001b[32m1464934981.8351\u001b[0m  1634426218.3339  0.0125  0.5011\n",
      "     50  0.5768  1473928327.9918  1624214853.5376  0.0125  0.5194\n",
      "     51  0.5762  \u001b[32m1390802735.3308\u001b[0m  1626695063.7271  0.0063  0.4903\n",
      "     52  0.5663  \u001b[32m1382774839.7517\u001b[0m  1664737554.3280  0.0063  0.5913\n",
      "     53  0.5864  \u001b[32m1358200518.2218\u001b[0m  1587319924.6413  0.0063  0.5278\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 44.\n",
      "  epoch       r2        train_loss       valid_loss      lr     dur\n",
      "-------  -------  ----------------  ---------------  ------  ------\n",
      "      1  \u001b[36m-1.2292\u001b[0m  \u001b[32m11624478474.6349\u001b[0m  \u001b[35m8485309519.7872\u001b[0m  0.0500  0.7609\n",
      "      2  -0.1070  \u001b[32m6195511271.2079\u001b[0m  \u001b[35m4213902935.2299\u001b[0m  0.0500  0.6664\n",
      "      3  0.3277  \u001b[32m3312739553.9113\u001b[0m  \u001b[35m2559253379.2144\u001b[0m  0.0500  0.6396\n",
      "      4  0.4863  \u001b[32m2467371201.9226\u001b[0m  \u001b[35m1955195071.4565\u001b[0m  0.0500  0.7327\n",
      "      5  0.4856  \u001b[32m2217959537.9692\u001b[0m  1957927448.1997  0.0500  0.6598\n",
      "      6  0.4607  \u001b[32m2113525288.1609\u001b[0m  2052897964.0160  0.0500  0.6616\n",
      "      7  0.4923  \u001b[32m2077771261.4256\u001b[0m  \u001b[35m1932434357.0687\u001b[0m  0.0500  0.7882\n",
      "      8  0.5340  \u001b[32m2038697391.1631\u001b[0m  \u001b[35m1773927125.8869\u001b[0m  0.0500  0.7134\n",
      "      9  0.5547  \u001b[32m1982804914.4988\u001b[0m  \u001b[35m1694879221.4505\u001b[0m  0.0500  1.0795\n",
      "     10  0.5495  \u001b[32m1962462079.7431\u001b[0m  1714621815.5941  0.0500  0.9908\n",
      "     11  0.5664  \u001b[32m1946504457.3471\u001b[0m  \u001b[35m1650587765.4001\u001b[0m  0.0500  0.8461\n",
      "     12  0.5458  \u001b[32m1941649787.6297\u001b[0m  1729071339.9966  0.0500  1.0418\n",
      "     13  0.5598  \u001b[32m1906429842.7558\u001b[0m  1675761178.5815  0.0500  1.0945\n",
      "     14  0.5671  1917576844.4314  \u001b[35m1647713740.6002\u001b[0m  0.0500  1.1010\n",
      "     15  0.5597  1925543806.5718  1675808802.6920  0.0500  1.3034\n",
      "     16  0.5615  \u001b[32m1898926995.5734\u001b[0m  1668971013.7807  0.0500  0.9255\n",
      "     17  0.5391  1903462191.4721  1754574159.4633  0.0500  0.9179\n",
      "     18  0.5332  \u001b[32m1898263071.1833\u001b[0m  1777028876.1525  0.0500  0.5948\n",
      "     19  0.5658  \u001b[32m1896030676.5604\u001b[0m  1652850926.9147  0.0500  0.6099\n",
      "     20  0.5363  \u001b[32m1880028227.6116\u001b[0m  1765127651.7307  0.0500  0.7030\n",
      "     21  0.5724  \u001b[32m1800908011.3498\u001b[0m  \u001b[35m1627584065.3294\u001b[0m  0.0250  0.8816\n",
      "     22  0.5335  \u001b[32m1736668290.2061\u001b[0m  1775799769.9751  0.0250  0.8427\n",
      "     23  0.5583  1740142713.1066  1681452489.7607  0.0250  0.6636\n",
      "     24  0.5588  1753105412.8948  1679546812.1490  0.0250  0.6864\n",
      "     25  0.5671  \u001b[32m1720542079.4084\u001b[0m  1647947133.3938  0.0250  0.6811\n",
      "     26  0.5953  \u001b[32m1691593035.9240\u001b[0m  \u001b[35m1540637072.4280\u001b[0m  0.0250  0.6831\n",
      "     27  0.5732  1730741332.5224  1624511803.4793  0.0250  0.7741\n",
      "     28  0.5651  1732608570.4321  1655598663.2317  0.0250  0.5925\n",
      "     29  0.5893  1722930784.5427  1563293954.7358  0.0250  0.6303\n",
      "     30  0.5750  1718926592.1401  1617675119.5060  0.0250  0.6883\n",
      "     31  0.5771  \u001b[32m1690066770.2709\u001b[0m  1609802836.9801  0.0250  0.6100\n",
      "     32  0.5879  1734532264.3933  1568581287.7081  0.0250  0.6168\n",
      "     33  0.5778  \u001b[32m1612732242.2466\u001b[0m  1607177722.8693  0.0125  0.6086\n",
      "     34  0.5653  \u001b[32m1589364923.7348\u001b[0m  1654690722.4483  0.0125  0.6211\n",
      "     35  0.5803  \u001b[32m1570210615.6043\u001b[0m  1597402159.2107  0.0125  0.8988\n",
      "     36  0.6011  \u001b[32m1563889898.5323\u001b[0m  \u001b[35m1518368444.4731\u001b[0m  0.0125  0.6412\n",
      "     37  0.5607  1565024267.6674  1672042750.2306  0.0125  0.6512\n",
      "     38  0.5878  \u001b[32m1504601854.1851\u001b[0m  1569060631.5478  0.0125  0.6246\n",
      "     39  0.5693  1574373062.0855  1639403825.7269  0.0125  0.6799\n",
      "     40  0.5701  1515302367.0185  1636365459.3557  0.0125  0.6216\n",
      "     41  0.5955  1529749753.8240  1539719009.8379  0.0125  0.6378\n",
      "     42  0.5209  1552330574.7553  1823595275.8271  0.0125  0.5941\n",
      "     43  0.5796  \u001b[32m1475481647.6446\u001b[0m  1600278753.3741  0.0063  0.7384\n",
      "     44  0.5606  1481051241.3660  1672565558.4952  0.0063  0.6129\n",
      "     45  0.5601  \u001b[32m1417208467.7441\u001b[0m  1674308925.0444  0.0063  0.6629\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 36.\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(\n",
    "    clone(preprocessor), \n",
    "    GridSearchCV(\n",
    "        NeuralNetRegressor(\n",
    "            Model,\n",
    "            max_epochs=150,\n",
    "            criterion=nn.MSELoss,\n",
    "            batch_size=64,\n",
    "            optimizer=optim.AdamW,\n",
    "            iterator_train__shuffle=True,\n",
    "            train_split=dataset.ValidSplit(cv=5),\n",
    "            callbacks=[\n",
    "                EarlyStopping(patience=10, monitor='valid_loss', load_best=True),\n",
    "                LRScheduler(policy=optim.lr_scheduler.ReduceLROnPlateau, patience=5, factor=0.5, monitor='valid_loss'),  # type: ignore\n",
    "                EpochScoring(scoring='r2', on_train=False),\n",
    "            ]\n",
    "        ),\n",
    "        { 'lr': [5e-2] },\n",
    "        scoring='r2',\n",
    "        cv=KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    )\n",
    ").fit(X_train, np.array(y_train).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ce3f0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([31.9399004]),\n",
       " 'std_fit_time': array([4.38391876]),\n",
       " 'mean_score_time': array([0.05998826]),\n",
       " 'std_score_time': array([0.00811996]),\n",
       " 'param_lr': masked_array(data=[0.05],\n",
       "              mask=[False],\n",
       "        fill_value=1e+20),\n",
       " 'params': [{'lr': 0.05}],\n",
       " 'split0_test_score': array([0.5619257]),\n",
       " 'split1_test_score': array([0.54816932]),\n",
       " 'split2_test_score': array([0.53213595]),\n",
       " 'split3_test_score': array([0.56185541]),\n",
       " 'split4_test_score': array([0.58780601]),\n",
       " 'mean_test_score': array([0.55837848]),\n",
       " 'std_test_score': array([0.01834965]),\n",
       " 'rank_test_score': array([1], dtype=int32)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = model[-1]\n",
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5e350ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.7054\n",
      "Train RMSE: 33390.5534\n",
      "Train MAE: 20270.4114\n"
     ]
    }
   ],
   "source": [
    "result_train = salary.evaluate_train_predictions(model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7d3c255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R2: 0.6133\n",
      "Test RMSE: 36255.3517\n",
      "Test MAE: 22850.6605\n"
     ]
    }
   ],
   "source": [
    "result_test = salary.evaluate_test_predictions(model.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
