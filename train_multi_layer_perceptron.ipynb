{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc2fc48f-dbcc-4e6c-bc0e-8d1e72e670a4",
   "metadata": {},
   "source": [
    "# Salary Prediction from LinkedIn Job Postings - Train Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3375fb27-0be7-4da8-9378-9137587226e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nginyc/repos/job_posting_salary_prediction/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import salary\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, TargetEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import random\n",
    "from skorch import NeuralNetRegressor, dataset\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, EpochScoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eab292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc83ff-8948-46b2-94bd-096093c31122",
   "metadata": {},
   "source": [
    "## Train & Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfcb495",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train) = salary.get_train_dataset()\n",
    "(X_test, y_test) = salary.get_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06ea4d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('one_hot_encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), ['norm_title', 'clustered_edu_req', 'clustered_pref_qual', 'clustered_req_skill', 'location_state', 'company_industries', 'formatted_experience_level', 'formatted_work_type']),\n",
    "            ('target_encoder', TargetEncoder(random_state=42), ['norm_title', 'clustered_edu_req', 'clustered_pref_qual', 'clustered_req_skill', 'location_state', 'company_industries', 'formatted_experience_level', 'formatted_work_type']),\n",
    "            ('experience_level', salary.experience_level_encoder, ['formatted_experience_level']),\n",
    "            ('work_type', salary.work_type_encoder, ['formatted_work_type']),\n",
    "            ('remote_allowed', 'passthrough', ['remote_allowed']),\n",
    "            ('company_employee_count', SimpleImputer(strategy='median'), ['company_employee_count']),\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    ),\n",
    "    StandardScaler(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ec93e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_units_1=256, n_units_2=192, n_units_3=64, n_units_4=32,\n",
    "                dropout_rate=0.3, leaky_relu_slope=0.2):\n",
    "        super().__init__()\n",
    "        # Layer 1\n",
    "        self.linear1 = nn.Linear(318, n_units_1).double()\n",
    "        self.bn1 = nn.BatchNorm1d(n_units_1).double()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate).double()\n",
    "\n",
    "        # Layer 2\n",
    "        self.linear2 = nn.Linear(n_units_1, n_units_2).double()\n",
    "        self.bn2 = nn.BatchNorm1d(n_units_2).double()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate).double()\n",
    "\n",
    "        # Layer 3\n",
    "        self.linear3 = nn.Linear(n_units_2, n_units_3).double()\n",
    "        self.bn3 = nn.BatchNorm1d(n_units_3).double()\n",
    "        self.dropout3 = nn.Dropout(dropout_rate).double()\n",
    "\n",
    "        # Layer 4\n",
    "        self.linear4 = nn.Linear(n_units_3, n_units_4).double()\n",
    "        self.bn4 = nn.BatchNorm1d(n_units_4).double()\n",
    "        self.dropout4 = nn.Dropout(dropout_rate).double()\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(n_units_4, 1).double()\n",
    "\n",
    "        # Activation function\n",
    "        self.leaky_relu = nn.LeakyReLU(leaky_relu_slope).double()\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Layer 1\n",
    "        X = self.leaky_relu(self.linear1(X))\n",
    "        X = self.bn1(X)\n",
    "        X = self.dropout1(X)\n",
    "\n",
    "        # Layer 2\n",
    "        X = self.leaky_relu(self.linear2(X))\n",
    "        X = self.bn2(X)\n",
    "        X = self.dropout2(X)\n",
    "\n",
    "        # Layer 3\n",
    "        X = self.leaky_relu(self.linear3(X))\n",
    "        X = self.bn3(X)\n",
    "        X = self.dropout3(X)\n",
    "\n",
    "        # Layer 4\n",
    "        X = self.leaky_relu(self.linear4(X))\n",
    "        X = self.bn4(X)\n",
    "        X = self.dropout4(X)\n",
    "\n",
    "        # Output layer\n",
    "        X = self.output(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9844899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nginyc/repos/job_posting_salary_prediction/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-1.5844\u001b[0m  \u001b[32m12241183997.2655\u001b[0m  \u001b[35m10334733339.5899\u001b[0m  0.0500  1.0196\n",
      "      2  -0.5073  \u001b[32m7908640791.1345\u001b[0m  \u001b[35m6027698767.4430\u001b[0m  0.0500  0.9402\n",
      "      3  -0.1633  \u001b[32m4595999017.4259\u001b[0m  \u001b[35m4651986914.3073\u001b[0m  0.0500  0.7745\n",
      "      4  0.1020  \u001b[32m3241416652.2363\u001b[0m  \u001b[35m3590952239.3042\u001b[0m  0.0500  0.7950\n",
      "      5  0.0139  \u001b[32m2729053634.3478\u001b[0m  3943381777.5763  0.0500  0.9503\n",
      "      6  0.0893  \u001b[32m2599307056.8922\u001b[0m  3641668810.6091  0.0500  0.9463\n",
      "      7  0.0893  \u001b[32m2450339603.0276\u001b[0m  3641909785.9649  0.0500  0.9222\n",
      "      8  0.1194  2462910727.6348  \u001b[35m3521583475.5094\u001b[0m  0.0500  0.9933\n",
      "      9  0.2367  \u001b[32m2401801479.0568\u001b[0m  \u001b[35m3052294773.2477\u001b[0m  0.0500  0.9371\n",
      "     10  0.2134  \u001b[32m2355540460.8375\u001b[0m  3145681048.3908  0.0500  0.9600\n",
      "     11  0.1543  \u001b[32m2354710390.7656\u001b[0m  3382040777.9308  0.0500  1.0403\n",
      "     12  0.1505  \u001b[32m2352247741.0002\u001b[0m  3396971564.0048  0.0500  1.0965\n",
      "     13  0.1334  \u001b[32m2302536846.2675\u001b[0m  3465610553.3356  0.0500  0.9727\n",
      "     14  0.3184  \u001b[32m2295263045.2133\u001b[0m  \u001b[35m2725802208.6638\u001b[0m  0.0500  0.8058\n",
      "     15  0.3515  \u001b[32m2278205734.8887\u001b[0m  \u001b[35m2593280440.0974\u001b[0m  0.0500  0.9863\n",
      "     16  0.3572  \u001b[32m2249056809.3883\u001b[0m  \u001b[35m2570521377.3997\u001b[0m  0.0500  1.0002\n",
      "     17  0.3532  2270128993.8998  2586682956.2273  0.0500  0.8257\n",
      "     18  0.3459  2261214902.2023  2615629849.0322  0.0500  0.9256\n",
      "     19  0.3934  2265755645.2389  \u001b[35m2425590221.5168\u001b[0m  0.0500  0.7719\n",
      "     20  0.3618  \u001b[32m2248712599.5004\u001b[0m  2552064408.2076  0.0500  0.7686\n",
      "     21  0.3847  \u001b[32m2219449779.7051\u001b[0m  2460647803.5950  0.0500  0.7842\n",
      "     22  0.4090  2221279744.8146  \u001b[35m2363495720.6433\u001b[0m  0.0500  0.8657\n",
      "     23  0.3886  2225512198.4936  2444800500.1974  0.0500  0.7843\n",
      "     24  0.3855  \u001b[32m2206943936.7295\u001b[0m  2457181727.9183  0.0500  0.8280\n",
      "     25  0.4061  2215382812.7082  2374862116.3502  0.0500  0.7468\n",
      "     26  0.4104  2228592927.7729  \u001b[35m2357607175.1291\u001b[0m  0.0500  0.7687\n",
      "     27  0.3586  \u001b[32m2202410067.3443\u001b[0m  2564971109.6634  0.0500  0.8038\n",
      "     28  0.4023  \u001b[32m2196842886.4298\u001b[0m  2390335421.3364  0.0500  0.8637\n",
      "     29  0.3925  2227779085.0608  2429420799.0719  0.0500  1.0341\n",
      "     30  0.3972  2224132803.0699  2410378118.1989  0.0500  0.8727\n",
      "     31  0.4011  2210779610.5119  2394874476.4189  0.0500  0.7890\n",
      "     32  0.3754  2214740966.3948  2497655507.9126  0.0500  1.2502\n",
      "     33  0.4060  \u001b[32m2121503653.4853\u001b[0m  2375301898.0648  0.0250  1.1148\n",
      "     34  0.4086  \u001b[32m2083957029.7530\u001b[0m  2365013403.6390  0.0250  1.0321\n",
      "     35  0.4125  \u001b[32m2068101600.5418\u001b[0m  \u001b[35m2349425199.1781\u001b[0m  0.0250  1.1982\n",
      "     36  0.4152  \u001b[32m2050672039.0468\u001b[0m  \u001b[35m2338626472.9634\u001b[0m  0.0250  1.2884\n",
      "     37  0.3781  \u001b[32m2036900954.8814\u001b[0m  2486909619.9026  0.0250  1.1301\n",
      "     38  0.4088  \u001b[32m2024041506.0702\u001b[0m  2363984639.5663  0.0250  1.2639\n",
      "     39  0.4196  2035588235.8105  \u001b[35m2320949483.4190\u001b[0m  0.0250  1.2243\n",
      "     40  0.4134  \u001b[32m2007324921.2907\u001b[0m  2345600204.6053  0.0250  1.1426\n",
      "     41  0.4255  \u001b[32m2004560144.8968\u001b[0m  \u001b[35m2297277134.1213\u001b[0m  0.0250  1.1994\n",
      "     42  0.4120  2017276934.5931  2351515265.5097  0.0250  1.4509\n",
      "     43  0.4267  2006913423.1326  \u001b[35m2292423745.6168\u001b[0m  0.0250  1.2596\n",
      "     44  0.4247  2021290122.7449  2300411456.8073  0.0250  1.0190\n",
      "     45  0.4240  \u001b[32m1990550177.8133\u001b[0m  2303423312.4937  0.0250  1.0204\n",
      "     46  0.4113  1994220950.3168  2354048374.2952  0.0250  1.0485\n",
      "     47  0.4042  1992148662.4377  2382734813.8296  0.0250  0.9432\n",
      "     48  0.4261  2014647718.9624  2294981506.6161  0.0250  1.3349\n",
      "     49  0.4224  2004998188.3098  2309752030.3634  0.0250  1.2393\n",
      "     50  0.4171  \u001b[32m1907820351.8820\u001b[0m  2330769569.4549  0.0125  1.2084\n",
      "     51  0.4337  \u001b[32m1842745391.5554\u001b[0m  \u001b[35m2264675581.5087\u001b[0m  0.0125  1.8845\n",
      "     52  0.4259  1866210930.7568  2295641214.4962  0.0125  1.3705\n",
      "     53  0.4131  \u001b[32m1775339246.7630\u001b[0m  2346840811.8754  0.0125  1.1427\n",
      "     54  0.4004  1790859273.9869  2397606536.3646  0.0125  1.3490\n",
      "     55  0.4100  1779143617.1044  2359421114.5327  0.0125  1.4811\n",
      "     56  0.4313  \u001b[32m1766846542.6391\u001b[0m  2274092428.0210  0.0125  1.0513\n",
      "     57  0.4426  \u001b[32m1757509511.8336\u001b[0m  \u001b[35m2229001628.7028\u001b[0m  0.0125  1.0718\n",
      "     58  0.4296  1770685111.8162  2280916093.4132  0.0125  1.0249\n",
      "     59  0.4284  \u001b[32m1746602417.1329\u001b[0m  2285660155.9958  0.0125  1.2183\n",
      "     60  0.3898  \u001b[32m1704942458.7485\u001b[0m  2440032362.2091  0.0125  0.9885\n",
      "     61  0.3984  1758841593.3370  2405895828.4049  0.0125  0.9911\n",
      "     62  0.4233  1759060183.6919  2306218541.7174  0.0125  1.0673\n",
      "     63  0.3979  1726407350.0119  2407732947.0332  0.0125  1.0555\n",
      "     64  0.4229  1708211293.9858  2307948896.4218  0.0063  1.0873\n",
      "     65  0.4190  \u001b[32m1638994118.1363\u001b[0m  2323290437.5339  0.0063  1.0429\n",
      "     66  0.4332  \u001b[32m1622389886.4783\u001b[0m  2266737707.0246  0.0063  0.9926\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 57.\n",
      "  epoch       r2        train_loss       valid_loss      lr     dur\n",
      "-------  -------  ----------------  ---------------  ------  ------\n",
      "      1  \u001b[36m-1.7842\u001b[0m  \u001b[32m12028364980.2343\u001b[0m  \u001b[35m9882501971.4245\u001b[0m  0.0500  0.9639\n",
      "      2  -0.6075  \u001b[32m7746844999.8961\u001b[0m  \u001b[35m5705934073.8414\u001b[0m  0.0500  3.1019\n",
      "      3  -0.0618  \u001b[32m4507268752.3301\u001b[0m  \u001b[35m3768964512.5995\u001b[0m  0.0500  1.8995\n",
      "      4  0.1690  \u001b[32m3184651054.6852\u001b[0m  \u001b[35m2949619387.3430\u001b[0m  0.0500  1.3804\n",
      "      5  0.2200  \u001b[32m2680675852.4265\u001b[0m  \u001b[35m2768710951.2825\u001b[0m  0.0500  1.0890\n",
      "      6  0.2700  \u001b[32m2512467583.7592\u001b[0m  \u001b[35m2591045587.0013\u001b[0m  0.0500  1.4408\n",
      "      7  0.2356  \u001b[32m2432881778.6092\u001b[0m  2713229092.4903  0.0500  1.4368\n",
      "      8  0.1617  \u001b[32m2379573100.9289\u001b[0m  2975430848.6897  0.0500  1.6760\n",
      "      9  0.2708  \u001b[32m2360864252.8706\u001b[0m  \u001b[35m2588475708.0877\u001b[0m  0.0500  1.5352\n",
      "     10  0.2103  \u001b[32m2343488511.1237\u001b[0m  2803152200.1622  0.0500  1.1294\n",
      "     11  0.2590  \u001b[32m2295074264.2091\u001b[0m  2630103950.3634  0.0500  1.0609\n",
      "     12  0.2322  \u001b[32m2283399652.2260\u001b[0m  2725255595.1477  0.0500  1.0270\n",
      "     13  0.2520  \u001b[32m2274708325.7339\u001b[0m  2655104245.0598  0.0500  1.0864\n",
      "     14  0.3244  \u001b[32m2222251305.4609\u001b[0m  \u001b[35m2398224735.6245\u001b[0m  0.0500  1.3156\n",
      "     15  0.3730  \u001b[32m2221125682.4050\u001b[0m  \u001b[35m2225434180.3422\u001b[0m  0.0500  1.2279\n",
      "     16  0.3861  \u001b[32m2219308067.2421\u001b[0m  \u001b[35m2179202729.8343\u001b[0m  0.0500  1.3475\n",
      "     17  0.3369  \u001b[32m2219198577.7892\u001b[0m  2353702291.1345  0.0500  1.1129\n",
      "     18  0.4056  \u001b[32m2185439557.7855\u001b[0m  \u001b[35m2109673048.3783\u001b[0m  0.0500  1.1446\n",
      "     19  0.3341  \u001b[32m2176749036.4279\u001b[0m  2363532811.2018  0.0500  1.0950\n",
      "     20  0.4041  \u001b[32m2161787390.6231\u001b[0m  2115077418.4509  0.0500  1.2232\n",
      "     21  0.3868  \u001b[32m2146910047.6761\u001b[0m  2176407882.2752  0.0500  1.0712\n",
      "     22  0.3626  \u001b[32m2145872184.7415\u001b[0m  2262498769.4402  0.0500  1.1232\n",
      "     23  0.4229  2180926740.5722  \u001b[35m2048428591.9554\u001b[0m  0.0500  1.1633\n",
      "     24  0.3332  2154296905.4933  2366661160.0227  0.0500  1.0653\n",
      "     25  0.3935  2149836750.9664  2152822339.1064  0.0500  1.0744\n",
      "     26  0.3981  2163110509.2337  2136468900.2738  0.0500  1.3478\n",
      "     27  0.4257  \u001b[32m2128750051.7015\u001b[0m  \u001b[35m2038352024.0704\u001b[0m  0.0500  1.1055\n",
      "     28  0.3900  2141858805.0614  2165100827.3677  0.0500  1.1960\n",
      "     29  0.4159  2163893513.1983  2073413006.9138  0.0500  1.9980\n",
      "     30  0.3936  2144133971.4975  2152282232.9040  0.0500  1.6258\n",
      "     31  0.4087  2145240860.2551  2098792427.0913  0.0500  1.3533\n",
      "     32  0.4176  \u001b[32m2116604940.2589\u001b[0m  2067328748.3670  0.0500  1.2535\n",
      "     33  0.4163  2142532610.7582  2071775368.4735  0.0500  0.9467\n",
      "     34  0.4436  \u001b[32m2043011674.7761\u001b[0m  \u001b[35m1974933068.9947\u001b[0m  0.0250  1.8037\n",
      "     35  0.4227  \u001b[32m2021955710.8681\u001b[0m  2049237226.6658  0.0250  1.9054\n",
      "     36  0.4449  \u001b[32m1989204774.4139\u001b[0m  \u001b[35m1970295186.7181\u001b[0m  0.0250  1.2712\n",
      "     37  0.4318  \u001b[32m1956191994.6801\u001b[0m  2016925600.3694  0.0250  1.0129\n",
      "     38  0.4366  1992529200.2962  1999941044.9998  0.0250  1.2277\n",
      "     39  0.4356  1961014825.9384  2003442483.5136  0.0250  1.2469\n",
      "     40  0.4210  1962504465.4664  2055304914.0395  0.0250  0.8772\n",
      "     41  0.4258  \u001b[32m1923614241.9787\u001b[0m  2038195987.3192  0.0250  0.9495\n",
      "     42  0.4429  1940067893.3139  1977404863.9150  0.0250  1.1099\n",
      "     43  0.4550  \u001b[32m1853901757.0387\u001b[0m  \u001b[35m1934425003.9832\u001b[0m  0.0125  1.3150\n",
      "     44  0.4466  \u001b[32m1777222030.5349\u001b[0m  1964367871.6972  0.0125  1.3338\n",
      "     45  0.4349  \u001b[32m1752848769.5531\u001b[0m  2006004997.2036  0.0125  1.0993\n",
      "     46  0.4489  1753272505.6625  1956136191.0095  0.0125  1.3006\n",
      "     47  0.4289  \u001b[32m1699736259.2568\u001b[0m  2027107842.9196  0.0125  1.4111\n",
      "     48  0.4346  1723202989.1415  2007049548.5882  0.0125  1.5378\n",
      "     49  0.4436  1737026313.4468  1974774525.6664  0.0125  1.4462\n",
      "     50  0.4208  \u001b[32m1660939468.9359\u001b[0m  2055823965.4924  0.0063  1.3148\n",
      "     51  0.4402  \u001b[32m1637757185.0733\u001b[0m  1986898613.1607  0.0063  1.4380\n",
      "     52  0.4387  \u001b[32m1604679608.3029\u001b[0m  1992187800.6648  0.0063  1.3496\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 43.\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-1.7249\u001b[0m  \u001b[32m12187270802.3341\u001b[0m  \u001b[35m10068014964.5951\u001b[0m  0.0500  1.4149\n",
      "      2  -0.6541  \u001b[32m7867437278.1747\u001b[0m  \u001b[35m6111617640.4346\u001b[0m  0.0500  1.2974\n",
      "      3  -0.1174  \u001b[32m4599820835.5589\u001b[0m  \u001b[35m4128607495.8588\u001b[0m  0.0500  1.5048\n",
      "      4  0.0113  \u001b[32m3198059272.4818\u001b[0m  \u001b[35m3652963838.5878\u001b[0m  0.0500  1.3895\n",
      "      5  0.0419  \u001b[32m2719524566.1116\u001b[0m  \u001b[35m3540162576.5566\u001b[0m  0.0500  1.2646\n",
      "      6  0.1218  \u001b[32m2558298405.6318\u001b[0m  \u001b[35m3244747198.8380\u001b[0m  0.0500  1.4518\n",
      "      7  0.0139  \u001b[32m2444951210.6354\u001b[0m  3643464094.4930  0.0500  1.0234\n",
      "      8  -0.1249  \u001b[32m2411345140.6631\u001b[0m  4156530815.7930  0.0500  1.0562\n",
      "      9  0.1361  \u001b[32m2408412277.1360\u001b[0m  \u001b[35m3192136056.8564\u001b[0m  0.0500  1.4737\n",
      "     10  0.1306  \u001b[32m2354171329.5182\u001b[0m  3212361930.9762  0.0500  1.3041\n",
      "     11  0.2612  \u001b[32m2321539246.0347\u001b[0m  \u001b[35m2729618113.3750\u001b[0m  0.0500  1.0633\n",
      "     12  -0.0932  \u001b[32m2283495592.5449\u001b[0m  4039237394.2173  0.0500  0.8915\n",
      "     13  0.2330  \u001b[32m2279247470.6124\u001b[0m  2833909082.1583  0.0500  0.8841\n",
      "     14  0.2111  \u001b[32m2232077128.6261\u001b[0m  2914827267.1619  0.0500  1.0922\n",
      "     15  0.3276  2270743267.9048  \u001b[35m2484554221.7183\u001b[0m  0.0500  0.9308\n",
      "     16  0.2612  \u001b[32m2225187419.1281\u001b[0m  2729663286.3909  0.0500  1.6860\n",
      "     17  0.3035  \u001b[32m2212277813.0884\u001b[0m  2573604270.2090  0.0500  1.0137\n",
      "     18  0.4017  2219773254.7206  \u001b[35m2210569264.4380\u001b[0m  0.0500  1.0277\n",
      "     19  0.4271  \u001b[32m2170308136.5047\u001b[0m  \u001b[35m2116830773.9390\u001b[0m  0.0500  0.9878\n",
      "     20  0.3232  2180017796.5234  2500703409.5053  0.0500  1.1651\n",
      "     21  0.3727  2189545519.1045  2317752512.6031  0.0500  1.0983\n",
      "     22  0.4250  \u001b[32m2165772067.3867\u001b[0m  2124385559.6966  0.0500  1.3456\n",
      "     23  0.4083  2206074907.0956  2186288900.7728  0.0500  1.2554\n",
      "     24  0.3212  2168984401.3637  2508018785.0339  0.0500  1.6481\n",
      "     25  0.3750  \u001b[32m2150597828.9328\u001b[0m  2309163750.3625  0.0500  1.3073\n",
      "     26  0.4063  \u001b[32m2122647965.8470\u001b[0m  2193583605.4725  0.0250  1.2293\n",
      "     27  0.4326  \u001b[32m2056130255.4719\u001b[0m  \u001b[35m2096474327.6544\u001b[0m  0.0250  1.3465\n",
      "     28  0.4518  \u001b[32m2034298976.7237\u001b[0m  \u001b[35m2025363473.0131\u001b[0m  0.0250  1.6307\n",
      "     29  0.4441  \u001b[32m2018730734.8085\u001b[0m  2054018517.8140  0.0250  0.9452\n",
      "     30  0.4487  \u001b[32m2001744494.5582\u001b[0m  2036932130.6974  0.0250  0.9636\n",
      "     31  0.4514  \u001b[32m1996076652.0938\u001b[0m  2027005666.0507  0.0250  1.2892\n",
      "     32  0.4450  \u001b[32m1975748516.9736\u001b[0m  2050781058.9532  0.0250  1.1822\n",
      "     33  0.4580  \u001b[32m1960090750.4518\u001b[0m  \u001b[35m2002578186.9305\u001b[0m  0.0250  1.2559\n",
      "     34  0.4464  1971305236.2853  2045347592.3925  0.0250  1.1158\n",
      "     35  0.4510  \u001b[32m1958433773.4904\u001b[0m  2028380450.7814  0.0250  0.9694\n",
      "     36  0.4464  1979657683.7357  2045468923.0688  0.0250  1.2557\n",
      "     37  0.4473  1967023852.4913  2042226682.5469  0.0250  1.3886\n",
      "     38  0.4524  1960468764.4249  2023321672.6791  0.0250  1.3103\n",
      "     39  0.4455  1966804590.3161  2048669067.0242  0.0250  1.3368\n",
      "     40  0.4436  \u001b[32m1888846796.3911\u001b[0m  2055894157.9519  0.0125  1.4306\n",
      "     41  0.4580  \u001b[32m1774109139.3852\u001b[0m  2002729857.5743  0.0125  1.1912\n",
      "     42  0.4457  \u001b[32m1755772612.8229\u001b[0m  2048195650.4602  0.0125  1.2713\n",
      "     43  0.4590  1759758884.8168  \u001b[35m1998751081.5686\u001b[0m  0.0125  1.3286\n",
      "     44  0.4542  \u001b[32m1734479638.7341\u001b[0m  2016759987.4948  0.0125  1.3038\n",
      "     45  0.4449  1747961393.1273  2050908063.0686  0.0125  1.2755\n",
      "     46  0.4531  \u001b[32m1732202540.2522\u001b[0m  2020667700.4485  0.0125  1.2018\n",
      "     47  0.4530  \u001b[32m1717386102.8650\u001b[0m  2021177105.2252  0.0125  1.2627\n",
      "     48  0.4357  1721909384.6906  2085029660.9627  0.0125  1.2589\n",
      "     49  0.4601  \u001b[32m1691615939.4410\u001b[0m  \u001b[35m1994772845.0890\u001b[0m  0.0125  1.1174\n",
      "     50  0.4521  \u001b[32m1689456078.6675\u001b[0m  2024462774.2543  0.0125  1.1460\n",
      "     51  0.4236  1739909620.0914  2129731709.9613  0.0125  1.2506\n",
      "     52  0.4713  1707099050.1106  \u001b[35m1953358667.4110\u001b[0m  0.0125  1.3826\n",
      "     53  0.4580  1716763701.7696  2002712311.8669  0.0125  1.1661\n",
      "     54  0.4389  1723158578.9109  2073131205.3347  0.0125  1.1484\n",
      "     55  0.4338  \u001b[32m1680998825.1159\u001b[0m  2092185465.9027  0.0125  1.0154\n",
      "     56  0.4517  1715317404.9001  2025993096.3254  0.0125  1.1426\n",
      "     57  0.4170  1689857240.0449  2154135775.8213  0.0125  1.0490\n",
      "     58  0.4576  \u001b[32m1679804984.4730\u001b[0m  2004218043.2845  0.0125  1.3607\n",
      "     59  0.4608  \u001b[32m1629364323.7544\u001b[0m  1992123583.1526  0.0063  1.0775\n",
      "     60  0.4547  \u001b[32m1623716010.4876\u001b[0m  2014970011.6416  0.0063  1.2698\n",
      "     61  0.4492  \u001b[32m1569821121.1230\u001b[0m  2035283252.0345  0.0063  1.3829\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 52.\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-1.5894\u001b[0m  \u001b[32m12200158485.2495\u001b[0m  \u001b[35m10250034785.0902\u001b[0m  0.0500  1.2352\n",
      "      2  -0.5765  \u001b[32m7867559135.2491\u001b[0m  \u001b[35m6240571976.6050\u001b[0m  0.0500  1.1587\n",
      "      3  -0.1303  \u001b[32m4584084624.9317\u001b[0m  \u001b[35m4474076904.1863\u001b[0m  0.0500  1.0818\n",
      "      4  0.1093  \u001b[32m3228984795.0208\u001b[0m  \u001b[35m3525686192.8793\u001b[0m  0.0500  1.0184\n",
      "      5  -0.0419  \u001b[32m2717956991.2644\u001b[0m  4124390838.7576  0.0500  1.0903\n",
      "      6  0.0899  \u001b[32m2568881204.0747\u001b[0m  3602558422.8009  0.0500  1.0129\n",
      "      7  -0.4207  \u001b[32m2475333177.4822\u001b[0m  5623844500.0338  0.0500  1.1986\n",
      "      8  -0.5167  \u001b[32m2435467299.6890\u001b[0m  6003664122.9590  0.0500  1.0708\n",
      "      9  -0.2500  \u001b[32m2396164690.9922\u001b[0m  4947991838.9040  0.0500  1.2684\n",
      "     10  0.0122  2396664462.1046  3910054411.9903  0.0500  1.0651\n",
      "     11  -0.3781  \u001b[32m2276139427.8494\u001b[0m  5455281251.3389  0.0250  1.2020\n",
      "     12  0.1711  \u001b[32m2190634258.6608\u001b[0m  \u001b[35m3281172707.8109\u001b[0m  0.0250  1.1426\n",
      "     13  0.2126  2195685001.6009  \u001b[35m3116835751.6119\u001b[0m  0.0250  1.1407\n",
      "     14  0.3112  \u001b[32m2155054947.3524\u001b[0m  \u001b[35m2726672810.5984\u001b[0m  0.0250  1.2726\n",
      "     15  0.3706  2161977939.5865  \u001b[35m2491400066.8416\u001b[0m  0.0250  1.0992\n",
      "     16  0.4110  \u001b[32m2118056013.7953\u001b[0m  \u001b[35m2331617511.5131\u001b[0m  0.0250  1.2026\n",
      "     17  0.3725  \u001b[32m2090223621.5304\u001b[0m  2483801384.5696  0.0250  1.2007\n",
      "     18  0.4231  \u001b[32m2086220526.6221\u001b[0m  \u001b[35m2283752325.7049\u001b[0m  0.0250  1.3192\n",
      "     19  0.4174  \u001b[32m2072007243.1633\u001b[0m  2306317707.0045  0.0250  1.1468\n",
      "     20  0.3809  \u001b[32m2035466174.6149\u001b[0m  2450867857.3496  0.0250  1.2882\n",
      "     21  0.4138  \u001b[32m2031930363.5823\u001b[0m  2320579273.5104  0.0250  1.3568\n",
      "     22  0.4251  \u001b[32m2019631941.2884\u001b[0m  \u001b[35m2275768765.5050\u001b[0m  0.0250  1.1049\n",
      "     23  0.4268  \u001b[32m2014469723.7209\u001b[0m  \u001b[35m2268829075.7170\u001b[0m  0.0250  1.0790\n",
      "     24  0.4245  2024649639.8638  2278164505.6024  0.0250  1.2727\n",
      "     25  0.4036  \u001b[32m1995758836.7713\u001b[0m  2360773336.1601  0.0250  1.1800\n",
      "     26  0.4120  2006308749.4969  2327448670.2435  0.0250  1.1508\n",
      "     27  0.4174  2024397741.7029  2306276142.0055  0.0250  1.0575\n",
      "     28  0.4085  2004088512.9047  2341239983.1098  0.0250  1.1279\n",
      "     29  0.3961  \u001b[32m1991657461.8868\u001b[0m  2390627766.2439  0.0250  0.8888\n",
      "     30  0.4323  \u001b[32m1903435407.0755\u001b[0m  \u001b[35m2247110593.0391\u001b[0m  0.0125  0.8612\n",
      "     31  0.4065  \u001b[32m1843767820.4357\u001b[0m  2349484673.6721  0.0125  1.0258\n",
      "     32  0.4143  1845440582.1148  2318594813.5999  0.0125  0.8785\n",
      "     33  0.4210  \u001b[32m1777455179.5077\u001b[0m  2291936527.2659  0.0125  1.0871\n",
      "     34  0.4237  1823537913.9343  2281231508.6454  0.0125  0.9067\n",
      "     35  0.3947  1797853534.5962  2395990952.1971  0.0125  0.8117\n",
      "     36  0.3700  1786414906.8657  2493791286.4630  0.0125  0.9327\n",
      "     37  0.4145  \u001b[32m1686334046.4850\u001b[0m  2317630248.6545  0.0063  0.9166\n",
      "     38  0.4158  \u001b[32m1646807951.2347\u001b[0m  2312608435.1903  0.0063  0.8172\n",
      "     39  0.4035  1678811579.3930  2361224589.5809  0.0063  1.2922\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 30.\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-1.6829\u001b[0m  \u001b[32m12275080008.3113\u001b[0m  \u001b[35m10297732088.3058\u001b[0m  0.0500  0.8982\n",
      "      2  -0.6171  \u001b[32m7946277891.6148\u001b[0m  \u001b[35m6206658526.8114\u001b[0m  0.0500  0.8805\n",
      "      3  0.0076  \u001b[32m4653194851.0448\u001b[0m  \u001b[35m3809247264.0117\u001b[0m  0.0500  0.8376\n",
      "      4  0.1251  \u001b[32m3243182576.4273\u001b[0m  \u001b[35m3358101133.8911\u001b[0m  0.0500  1.0732\n",
      "      5  0.1812  \u001b[32m2748188949.2772\u001b[0m  \u001b[35m3142812438.5646\u001b[0m  0.0500  0.8470\n",
      "      6  0.2346  \u001b[32m2595081696.0126\u001b[0m  \u001b[35m2937610087.7090\u001b[0m  0.0500  0.9277\n",
      "      7  0.2933  \u001b[32m2499044252.6432\u001b[0m  \u001b[35m2712322202.7467\u001b[0m  0.0500  0.8909\n",
      "      8  0.3154  \u001b[32m2453626236.2925\u001b[0m  \u001b[35m2627820548.0195\u001b[0m  0.0500  0.8347\n",
      "      9  0.2699  \u001b[32m2423913443.0193\u001b[0m  2802449370.5614  0.0500  0.8866\n",
      "     10  0.2882  2426529193.9866  2732089917.3274  0.0500  0.9806\n",
      "     11  0.2562  \u001b[32m2365047253.4145\u001b[0m  2855029648.8207  0.0500  0.8230\n",
      "     12  0.3150  \u001b[32m2356006352.4344\u001b[0m  2629161788.7926  0.0500  0.8670\n",
      "     13  0.3661  \u001b[32m2322722342.3711\u001b[0m  \u001b[35m2433176015.4654\u001b[0m  0.0500  0.8673\n",
      "     14  0.3724  \u001b[32m2259934100.3140\u001b[0m  \u001b[35m2409032561.4631\u001b[0m  0.0500  0.8125\n",
      "     15  0.3788  2261913218.4943  \u001b[35m2384139207.3158\u001b[0m  0.0500  1.0184\n",
      "     16  0.3380  2285317758.5704  2540742164.2541  0.0500  1.2169\n",
      "     17  0.4013  \u001b[32m2239947925.6884\u001b[0m  \u001b[35m2297837688.0540\u001b[0m  0.0500  1.0890\n",
      "     18  0.4000  2246881579.3825  2302789441.4782  0.0500  1.2267\n",
      "     19  0.3742  \u001b[32m2233302096.2034\u001b[0m  2401993815.6553  0.0500  1.2119\n",
      "     20  0.4187  \u001b[32m2229566828.6822\u001b[0m  \u001b[35m2231072366.3363\u001b[0m  0.0500  1.1453\n",
      "     21  0.4209  2231154680.0039  \u001b[35m2222751939.7780\u001b[0m  0.0500  1.0460\n",
      "     22  0.4101  2240261097.3102  2263992045.2199  0.0500  1.1300\n",
      "     23  0.4049  \u001b[32m2228295260.5441\u001b[0m  2284218417.0920  0.0500  1.0828\n",
      "     24  0.4117  \u001b[32m2222386900.2032\u001b[0m  2258075666.3539  0.0500  1.1837\n",
      "     25  0.4220  \u001b[32m2205539027.2586\u001b[0m  \u001b[35m2218427486.7565\u001b[0m  0.0500  1.0912\n",
      "     26  0.4051  2220812707.1765  2283335737.3259  0.0500  1.1662\n",
      "     27  0.4265  2212318770.4802  \u001b[35m2201099941.5438\u001b[0m  0.0500  1.0336\n",
      "     28  0.4195  2217247518.7115  2227942426.7811  0.0500  1.0901\n",
      "     29  0.3941  2222649202.9009  2325576925.5082  0.0500  1.0192\n",
      "     30  0.4051  \u001b[32m2205340102.8641\u001b[0m  2283240886.5482  0.0500  1.1214\n",
      "     31  0.3808  2214734532.1182  2376828552.8593  0.0500  1.1132\n",
      "     32  0.3953  2222532044.8196  2320863919.6445  0.0500  1.0212\n",
      "     33  0.4031  2241236672.2436  2291005460.6916  0.0500  1.1061\n",
      "     34  0.4180  \u001b[32m2121037241.5833\u001b[0m  2233724558.1903  0.0250  1.0424\n",
      "     35  0.4233  \u001b[32m2073858581.4807\u001b[0m  2213353360.3928  0.0250  1.2375\n",
      "     36  0.4127  \u001b[32m2058664919.4201\u001b[0m  2254100021.4777  0.0250  1.0699\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 27.\n",
      "  epoch       r2        train_loss       valid_loss      lr     dur\n",
      "-------  -------  ----------------  ---------------  ------  ------\n",
      "      1  \u001b[36m-1.3282\u001b[0m  \u001b[32m11680981152.3135\u001b[0m  \u001b[35m8862240346.9246\u001b[0m  0.0500  1.3128\n",
      "      2  -0.1775  \u001b[32m6445505480.5570\u001b[0m  \u001b[35m4482236083.6811\u001b[0m  0.0500  1.4489\n",
      "      3  0.1618  \u001b[32m3614676274.0467\u001b[0m  \u001b[35m3190474623.5020\u001b[0m  0.0500  1.3464\n",
      "      4  0.1702  \u001b[32m2801027769.4616\u001b[0m  \u001b[35m3158753449.0625\u001b[0m  0.0500  1.4408\n",
      "      5  0.0583  \u001b[32m2556209955.4102\u001b[0m  3584356836.8516  0.0500  3.4092\n",
      "      6  0.2410  \u001b[32m2474473085.9814\u001b[0m  \u001b[35m2889018539.8675\u001b[0m  0.0500  1.6831\n",
      "      7  0.1691  \u001b[32m2437444866.4237\u001b[0m  3162663272.7645  0.0500  1.5287\n",
      "      8  0.1875  \u001b[32m2403468466.2866\u001b[0m  3092811724.8455  0.0500  1.5204\n",
      "      9  0.2441  \u001b[32m2362513917.9366\u001b[0m  \u001b[35m2877320222.0518\u001b[0m  0.0500  2.8372\n",
      "     10  0.2308  \u001b[32m2339480007.8329\u001b[0m  2927745882.6178  0.0500  1.8858\n",
      "     11  0.3323  \u001b[32m2329857526.4907\u001b[0m  \u001b[35m2541641126.1339\u001b[0m  0.0500  1.3458\n",
      "     12  0.3005  \u001b[32m2281518866.4433\u001b[0m  2662638394.7439  0.0500  1.3649\n",
      "     13  0.4081  2291550354.5882  \u001b[35m2252868993.4489\u001b[0m  0.0500  1.4218\n",
      "     14  0.3625  2292438204.8512  2426448198.6162  0.0500  1.3380\n",
      "     15  0.3015  \u001b[32m2277641780.3837\u001b[0m  2658843180.8679  0.0500  1.2780\n",
      "     16  0.3253  2294180854.0384  2568095120.7996  0.0500  1.2932\n",
      "     17  0.3853  \u001b[32m2239024739.7043\u001b[0m  2339837057.2478  0.0500  1.3171\n",
      "     18  0.3632  \u001b[32m2238989472.3259\u001b[0m  2423826128.2433  0.0500  1.3279\n",
      "     19  0.4007  2244182878.0963  2281080201.5762  0.0500  1.3259\n",
      "     20  0.4327  \u001b[32m2184489375.1863\u001b[0m  \u001b[35m2159477011.0762\u001b[0m  0.0250  1.3218\n",
      "     21  0.4078  \u001b[32m2119458932.3828\u001b[0m  2254140786.9784  0.0250  1.5999\n",
      "     22  0.4251  \u001b[32m2081625832.9849\u001b[0m  2188447555.4164  0.0250  1.2715\n",
      "     23  0.4128  2091917832.8628  2235309424.0686  0.0250  1.4794\n",
      "     24  0.4331  2102015750.6743  \u001b[35m2158028111.5944\u001b[0m  0.0250  1.2828\n",
      "     25  0.4381  2087259551.9459  \u001b[35m2138769586.4201\u001b[0m  0.0250  1.2536\n",
      "     26  0.4255  2089196059.9378  2186936802.1658  0.0250  1.6337\n",
      "     27  0.4414  2085703117.6012  \u001b[35m2126450043.0863\u001b[0m  0.0250  1.4160\n",
      "     28  0.4385  \u001b[32m2054369198.5523\u001b[0m  2137230775.0280  0.0250  1.3765\n",
      "     29  0.4552  2091940846.2016  \u001b[35m2073788811.6652\u001b[0m  0.0250  1.3709\n",
      "     30  0.4351  \u001b[32m2040084241.4584\u001b[0m  2150413564.6593  0.0250  1.3766\n",
      "     31  0.4376  2062270581.7932  2140858388.5245  0.0250  1.3921\n",
      "     32  0.4359  2054253648.6413  2147402379.5914  0.0250  1.4432\n",
      "     33  0.4478  2065864954.0811  2102003818.8212  0.0250  1.3950\n",
      "     34  0.4393  2067452428.1429  2134362253.8807  0.0250  1.3403\n",
      "     35  0.4102  2047326006.0174  2245187454.1364  0.0250  1.4980\n",
      "     36  0.4527  \u001b[32m1943161987.6088\u001b[0m  2083288765.3500  0.0125  1.3582\n",
      "     37  0.4498  \u001b[32m1914403934.6396\u001b[0m  2094153334.6085  0.0125  1.4787\n",
      "     38  0.4442  \u001b[32m1874867138.9586\u001b[0m  2115669721.5342  0.0125  1.5692\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 29.\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(\n",
    "    clone(preprocessor), \n",
    "    GridSearchCV(\n",
    "        NeuralNetRegressor(\n",
    "            Model,\n",
    "            max_epochs=150,\n",
    "            criterion=nn.MSELoss,\n",
    "            batch_size=64,\n",
    "            optimizer=optim.AdamW,\n",
    "            iterator_train__shuffle=True,\n",
    "            train_split=dataset.ValidSplit(cv=5),\n",
    "            callbacks=[\n",
    "                EarlyStopping(patience=10, monitor='valid_loss', load_best=True),\n",
    "                LRScheduler(policy=optim.lr_scheduler.ReduceLROnPlateau, patience=5, factor=0.5, monitor='valid_loss'),  # type: ignore\n",
    "                EpochScoring(scoring='r2', on_train=False),\n",
    "            ]\n",
    "        ),\n",
    "        { 'lr': [5e-2] },\n",
    "        scoring='r2',\n",
    "        cv=KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    )\n",
    ").fit(X_train, np.array(y_train).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ce3f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "seach = model[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d2afcb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([59.73524218]),\n",
       " 'std_fit_time': array([15.48263526]),\n",
       " 'mean_score_time': array([0.09017854]),\n",
       " 'std_score_time': array([0.0130369]),\n",
       " 'param_lr': masked_array(data=[0.05],\n",
       "              mask=[False],\n",
       "        fill_value=1e+20),\n",
       " 'params': [{'lr': 0.05}],\n",
       " 'split0_test_score': array([0.43931413]),\n",
       " 'split1_test_score': array([0.43628819]),\n",
       " 'split2_test_score': array([0.40893616]),\n",
       " 'split3_test_score': array([0.44645232]),\n",
       " 'split4_test_score': array([0.42688541]),\n",
       " 'mean_test_score': array([0.43157524]),\n",
       " 'std_test_score': array([0.01294568]),\n",
       " 'rank_test_score': array([1], dtype=int32)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seach.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5e350ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.5371\n",
      "Train RMSE: 41851.1526\n",
      "Train MAE: 25747.6193\n"
     ]
    }
   ],
   "source": [
    "result_train = salary.evaluate_train_predictions(model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3897fc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R2: 0.4858\n",
      "Test RMSE: 41803.7793\n",
      "Test MAE: 27076.5028\n"
     ]
    }
   ],
   "source": [
    "result_test = salary.evaluate_test_predictions(model.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
