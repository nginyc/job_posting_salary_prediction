{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc2fc48f-dbcc-4e6c-bc0e-8d1e72e670a4",
   "metadata": {},
   "source": [
    "# Train Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3375fb27-0be7-4da8-9378-9137587226e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nginyc/repos/job_posting_salary_prediction/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import salary\n",
    "import numpy as np\n",
    "from multi_layer_perceptron import Model, TensorTransformer, CustomNeuralNetRegressor\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "import cloudpickle\n",
    "from skopt import BayesSearchCV\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import random\n",
    "from skorch import dataset\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, EpochScoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eab292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be00b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train) = salary.get_train_dataset(include_extracted_salaries=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fba5d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32103, 3670)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = salary.get_preprocessor()\n",
    "(train_size, num_features) = clone(preprocessor).fit_transform(X_train, y_train).shape\n",
    "(train_size, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc83ff-8948-46b2-94bd-096093c31122",
   "metadata": {},
   "source": [
    "## Train & Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9844899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-2.4580\u001b[0m  \u001b[32m12863820505.6000\u001b[0m  \u001b[35m12860708981.0115\u001b[0m  0.0045  3.3807\n",
      "      2  -2.3888  \u001b[32m12686198182.4000\u001b[0m  \u001b[35m12603213351.5686\u001b[0m  0.0045  3.3478\n",
      "      3  -2.2731  \u001b[32m12311920371.2000\u001b[0m  \u001b[35m12172921234.4637\u001b[0m  0.0045  3.4029\n",
      "      4  -2.1104  \u001b[32m11772511161.6000\u001b[0m  \u001b[35m11567903282.3329\u001b[0m  0.0045  3.0873\n",
      "      5  -1.9158  \u001b[32m11093443270.4000\u001b[0m  \u001b[35m10844347179.9042\u001b[0m  0.0045  3.2416\n",
      "      6  -1.7501  \u001b[32m10332852268.8000\u001b[0m  \u001b[35m10227930152.5653\u001b[0m  0.0045  3.1995\n",
      "      7  -1.5002  \u001b[32m9505489782.4000\u001b[0m  \u001b[35m9298722113.9311\u001b[0m  0.0045  3.0678\n",
      "      8  -1.3333  \u001b[32m8629473446.4000\u001b[0m  \u001b[35m8677978995.1676\u001b[0m  0.0045  3.1857\n",
      "      9  -1.1410  \u001b[32m7747103155.2000\u001b[0m  \u001b[35m7962777569.5013\u001b[0m  0.0045  3.5127\n",
      "     10  -0.8749  \u001b[32m6880349568.0000\u001b[0m  \u001b[35m6973019782.4536\u001b[0m  0.0045  3.3550\n",
      "     11  -0.6944  \u001b[32m6040608992.0000\u001b[0m  \u001b[35m6301668145.2365\u001b[0m  0.0045  3.5624\n",
      "     12  -0.5313  \u001b[32m5252903596.8000\u001b[0m  \u001b[35m5694931856.4205\u001b[0m  0.0045  3.2276\n",
      "     13  -0.3276  \u001b[32m4529115017.6000\u001b[0m  \u001b[35m4937333380.6595\u001b[0m  0.0045  3.2136\n",
      "     14  -0.1568  \u001b[32m3874538593.6000\u001b[0m  \u001b[35m4302111873.1711\u001b[0m  0.0045  3.2675\n",
      "     15  -0.0553  \u001b[32m3288537979.2000\u001b[0m  \u001b[35m3924920256.9095\u001b[0m  0.0045  3.3162\n",
      "     16  0.0545  \u001b[32m2774572080.8000\u001b[0m  \u001b[35m3516565515.7360\u001b[0m  0.0045  3.1855\n",
      "     17  0.1426  \u001b[32m2342559612.8000\u001b[0m  \u001b[35m3188621142.3882\u001b[0m  0.0045  3.5284\n",
      "     18  0.2170  \u001b[32m1957362800.8000\u001b[0m  \u001b[35m2912116645.8493\u001b[0m  0.0045  3.3413\n",
      "     19  0.3417  \u001b[32m1669135244.4000\u001b[0m  \u001b[35m2448311697.7037\u001b[0m  0.0045  3.3278\n",
      "     20  0.3834  \u001b[32m1414436598.4000\u001b[0m  \u001b[35m2293303141.4507\u001b[0m  0.0045  3.5178\n",
      "     21  0.4579  \u001b[32m1170389487.6000\u001b[0m  \u001b[35m2016200024.3815\u001b[0m  0.0045  3.3916\n",
      "     22  0.4762  \u001b[32m1048548762.2000\u001b[0m  \u001b[35m1948079007.1341\u001b[0m  0.0045  3.2677\n",
      "     23  0.4756  \u001b[32m917687722.2000\u001b[0m  1950219841.9809  0.0045  3.9589\n",
      "     24  0.5308  \u001b[32m840218608.0000\u001b[0m  \u001b[35m1744981999.7290\u001b[0m  0.0045  3.1625\n",
      "     25  0.5572  \u001b[32m733386266.2000\u001b[0m  \u001b[35m1646646821.1454\u001b[0m  0.0045  3.2826\n",
      "     26  0.5655  \u001b[32m659435690.0000\u001b[0m  \u001b[35m1616079987.5476\u001b[0m  0.0045  4.1877\n",
      "     27  0.5924  \u001b[32m603035243.4000\u001b[0m  \u001b[35m1515897305.4904\u001b[0m  0.0045  3.8102\n",
      "     28  0.5815  \u001b[32m580947365.4000\u001b[0m  1556532482.4481  0.0045  3.6896\n",
      "     29  0.5823  \u001b[32m565949593.9000\u001b[0m  1553646996.3200  0.0045  5.8246\n",
      "     30  0.5989  \u001b[32m512996369.3000\u001b[0m  \u001b[35m1491834615.9829\u001b[0m  0.0045  5.7108\n",
      "     31  0.6056  \u001b[32m468788712.4000\u001b[0m  \u001b[35m1466782605.3058\u001b[0m  0.0045  5.2855\n",
      "     32  0.6245  \u001b[32m442230773.3000\u001b[0m  \u001b[35m1396371049.7520\u001b[0m  0.0045  4.4511\n",
      "     33  0.6001  \u001b[32m441085528.6000\u001b[0m  1487335769.4686  0.0045  3.1124\n",
      "     34  0.6118  \u001b[32m428198794.4000\u001b[0m  1443597181.0722  0.0045  3.6768\n",
      "     35  0.5829  \u001b[32m412377383.1000\u001b[0m  1551249213.1470  0.0045  3.1687\n",
      "     36  0.6253  \u001b[32m386370530.2000\u001b[0m  \u001b[35m1393554763.9136\u001b[0m  0.0045  3.4398\n",
      "     37  0.6388  \u001b[32m357348783.0000\u001b[0m  \u001b[35m1343311446.5750\u001b[0m  0.0045  3.5581\n",
      "     38  0.6325  375558536.2000  1366788689.5854  0.0045  3.2509\n",
      "     39  0.6323  \u001b[32m345859384.7000\u001b[0m  1367614294.4255  0.0045  3.8061\n",
      "     40  0.6299  \u001b[32m345850976.8000\u001b[0m  1376449339.8170  0.0045  3.9297\n",
      "     41  0.6329  \u001b[32m330304338.7000\u001b[0m  1365367756.6081  0.0045  3.3156\n",
      "     42  0.6318  \u001b[32m322980621.9000\u001b[0m  1369284378.5992  0.0045  3.1239\n",
      "     43  0.6255  326489504.8000  1392761135.7820  0.0045  3.1154\n",
      "     44  0.6400  \u001b[32m309771478.4500\u001b[0m  \u001b[35m1339002639.1933\u001b[0m  0.0022  3.5123\n",
      "     45  0.6406  \u001b[32m278509634.5000\u001b[0m  \u001b[35m1336729749.0925\u001b[0m  0.0022  3.2753\n",
      "     46  0.6408  287709027.6000  \u001b[35m1335794161.2147\u001b[0m  0.0022  5.6545\n",
      "     47  0.6418  \u001b[32m267421856.0000\u001b[0m  \u001b[35m1332336085.2326\u001b[0m  0.0022  4.7593\n",
      "     48  0.6364  278710779.2500  1352107428.8495  0.0022  3.5425\n",
      "     49  0.6436  \u001b[32m262948009.5000\u001b[0m  \u001b[35m1325422404.4945\u001b[0m  0.0022  3.7219\n",
      "     50  0.6501  268440657.7500  \u001b[35m1301171008.4361\u001b[0m  0.0022  3.4202\n",
      "     51  0.6453  \u001b[32m261977335.7000\u001b[0m  1319276645.8836  0.0022  3.3884\n",
      "     52  0.6458  \u001b[32m256123090.6000\u001b[0m  1317224289.2116  0.0022  3.2361\n",
      "     53  0.6447  256198738.2000  1321339782.7619  0.0022  4.1772\n",
      "     54  0.6504  \u001b[32m234978161.4500\u001b[0m  \u001b[35m1300222770.2519\u001b[0m  0.0022  3.6428\n",
      "     55  0.6423  251855773.1000  1330414634.0510  0.0022  4.0145\n",
      "     56  0.6406  260276010.0000  1336761170.6412  0.0022  3.5511\n",
      "     57  0.6419  271065065.4500  1331846768.5201  0.0022  3.7282\n",
      "     58  0.6358  259939775.9500  1354373431.0329  0.0022  3.2215\n",
      "     59  0.6347  247034768.9500  1358575318.5782  0.0022  3.4154\n",
      "     60  0.6464  253490511.1500  1315181481.1259  0.0022  3.2722\n",
      "     61  0.6437  267887852.7500  1325052001.5511  0.0011  4.0349\n",
      "     62  0.6379  251617953.6500  1346875091.1832  0.0011  4.0249\n",
      "     63  0.6362  \u001b[32m224336088.6500\u001b[0m  1352913019.3841  0.0011  3.3994\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 54.\n",
      "[CV 1/5] END batch_size=128, lambda1=0.0001, lr=0.00447, module__dropout_rate=0.5, module__n_units_last=256, module__num_hidden_layers=3;, score=0.688 total time= 3.9min\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-2.3718\u001b[0m  \u001b[32m12715964985.6000\u001b[0m  \u001b[35m13019477955.0025\u001b[0m  0.0045  4.6294\n",
      "      2  -2.3026  \u001b[32m12535167808.0000\u001b[0m  \u001b[35m12752243099.8326\u001b[0m  0.0045  4.4024\n",
      "      3  -2.1823  \u001b[32m12157066828.8000\u001b[0m  \u001b[35m12287953479.6621\u001b[0m  0.0045  3.3104\n",
      "      4  -2.0442  \u001b[32m11621024499.2000\u001b[0m  \u001b[35m11754734575.0563\u001b[0m  0.0045  3.6599\n",
      "      5  -1.8576  \u001b[32m10956040396.8000\u001b[0m  \u001b[35m11033932940.5334\u001b[0m  0.0045  3.0650\n",
      "      6  -1.6586  \u001b[32m10167723484.8000\u001b[0m  \u001b[35m10265562675.9276\u001b[0m  0.0045  3.0295\n",
      "      7  -1.4889  \u001b[32m9336049667.2000\u001b[0m  \u001b[35m9610437898.1164\u001b[0m  0.0045  3.3321\n",
      "      8  -1.2373  \u001b[32m8467640268.8000\u001b[0m  \u001b[35m8638756474.6926\u001b[0m  0.0045  3.6041\n",
      "      9  -1.0774  \u001b[32m7574920592.0000\u001b[0m  \u001b[35m8021635333.1330\u001b[0m  0.0045  4.1633\n",
      "     10  -0.8419  \u001b[32m6710416443.2000\u001b[0m  \u001b[35m7111973820.0257\u001b[0m  0.0045  3.1652\n",
      "     11  -0.7045  \u001b[32m5902260344.0000\u001b[0m  \u001b[35m6581706275.7812\u001b[0m  0.0045  3.0755\n",
      "     12  -0.4527  \u001b[32m5131724614.4000\u001b[0m  \u001b[35m5609395547.4464\u001b[0m  0.0045  3.5290\n",
      "     13  -0.2857  \u001b[32m4400399113.6000\u001b[0m  \u001b[35m4964315262.6794\u001b[0m  0.0045  3.6491\n",
      "     14  -0.1825  \u001b[32m3750973969.6000\u001b[0m  \u001b[35m4565808925.6515\u001b[0m  0.0045  3.5140\n",
      "     15  -0.0194  \u001b[32m3172018630.4000\u001b[0m  \u001b[35m3936072776.3597\u001b[0m  0.0045  3.4390\n",
      "     16  0.0686  \u001b[32m2663159520.0000\u001b[0m  \u001b[35m3596483792.1588\u001b[0m  0.0045  3.3110\n",
      "     17  0.1625  \u001b[32m2247703802.0000\u001b[0m  \u001b[35m3233965170.4699\u001b[0m  0.0045  3.2366\n",
      "     18  0.2468  \u001b[32m1850830653.2000\u001b[0m  \u001b[35m2908258618.7549\u001b[0m  0.0045  3.8912\n",
      "     19  0.2658  \u001b[32m1568526852.0000\u001b[0m  \u001b[35m2834996425.1820\u001b[0m  0.0045  3.9070\n",
      "     20  0.3727  \u001b[32m1311379628.4000\u001b[0m  \u001b[35m2422074791.4939\u001b[0m  0.0045  3.5348\n",
      "     21  0.4026  \u001b[32m1124933635.8000\u001b[0m  \u001b[35m2306721106.6257\u001b[0m  0.0045  3.7387\n",
      "     22  0.4506  \u001b[32m955313262.6000\u001b[0m  \u001b[35m2121333087.8816\u001b[0m  0.0045  3.6703\n",
      "     23  0.4758  \u001b[32m860034831.2000\u001b[0m  \u001b[35m2023964409.0730\u001b[0m  0.0045  4.1686\n",
      "     24  0.5034  \u001b[32m740024343.6000\u001b[0m  \u001b[35m1917474138.4497\u001b[0m  0.0045  3.1714\n",
      "     25  0.5535  \u001b[32m653924622.8000\u001b[0m  \u001b[35m1723975807.7508\u001b[0m  0.0045  3.7762\n",
      "     26  0.5468  \u001b[32m599068080.2000\u001b[0m  1750001168.6697  0.0045  3.5542\n",
      "     27  0.5208  \u001b[32m546834184.5000\u001b[0m  1850525172.8121  0.0045  3.7231\n",
      "     28  0.5567  \u001b[32m508026241.1000\u001b[0m  \u001b[35m1711538393.1291\u001b[0m  0.0045  4.6326\n",
      "     29  0.5623  \u001b[32m479401968.5000\u001b[0m  \u001b[35m1689987911.6621\u001b[0m  0.0045  3.7269\n",
      "     30  0.5756  \u001b[32m465737185.3000\u001b[0m  \u001b[35m1638792586.2410\u001b[0m  0.0045  4.2790\n",
      "     31  0.5987  \u001b[32m436660516.1000\u001b[0m  \u001b[35m1549433086.1561\u001b[0m  0.0045  3.8021\n",
      "     32  0.5836  \u001b[32m436112082.2000\u001b[0m  1607777885.0162  0.0045  3.6735\n",
      "     33  0.5916  \u001b[32m393824476.2000\u001b[0m  1576814796.3963  0.0045  3.8230\n",
      "     34  0.5888  \u001b[32m380694120.6000\u001b[0m  1587916172.3091  0.0045  3.8187\n",
      "     35  0.6007  392416668.1000  \u001b[35m1541731590.7775\u001b[0m  0.0045  4.1337\n",
      "     36  0.5931  \u001b[32m363962523.5000\u001b[0m  1571220884.9056  0.0045  3.3802\n",
      "     37  0.6056  \u001b[32m329801569.2000\u001b[0m  \u001b[35m1522874038.8927\u001b[0m  0.0045  2.8558\n",
      "     38  0.6051  331792681.1000  1524799206.1608  0.0045  3.2064\n",
      "     39  0.5986  \u001b[32m312878216.6000\u001b[0m  1549816711.5250  0.0045  3.3684\n",
      "     40  0.6121  \u001b[32m305601782.0000\u001b[0m  \u001b[35m1497892695.4596\u001b[0m  0.0045  5.5083\n",
      "     41  0.6144  \u001b[32m296411935.0000\u001b[0m  \u001b[35m1488807879.8365\u001b[0m  0.0045  7.4555\n",
      "     42  0.6070  \u001b[32m291768944.9500\u001b[0m  1517613094.9706  0.0045  7.2540\n",
      "     43  0.6154  296393617.6000  \u001b[35m1485084791.4534\u001b[0m  0.0045  4.9562\n",
      "     44  0.6073  \u001b[32m287518003.0000\u001b[0m  1516201584.9001  0.0045  6.9379\n",
      "     45  0.6161  287750320.2000  \u001b[35m1482377417.4561\u001b[0m  0.0045  5.6374\n",
      "     46  0.6093  299782079.1000  1508638162.6506  0.0045  4.2873\n",
      "     47  0.6089  \u001b[32m273211145.6500\u001b[0m  1510254194.3952  0.0045  5.1080\n",
      "     48  0.6151  278279384.3000  1486160815.6667  0.0045  3.9598\n",
      "     49  0.6105  288026694.6000  1503830307.6317  0.0045  3.7016\n",
      "     50  0.6137  280872707.7500  1491776851.7968  0.0045  4.1965\n",
      "     51  0.6127  279827058.2000  1495345884.1690  0.0045  4.3220\n",
      "     52  0.6185  \u001b[32m254435752.8500\u001b[0m  \u001b[35m1473257688.3068\u001b[0m  0.0022  4.2643\n",
      "     53  0.6135  257881315.1000  1492242096.8129  0.0022  3.6369\n",
      "     54  0.6241  \u001b[32m249896968.9500\u001b[0m  \u001b[35m1451414576.2398\u001b[0m  0.0022  3.4543\n",
      "     55  0.6183  255232835.5000  1473794592.3675  0.0022  3.2583\n",
      "     56  0.6228  253815994.5000  1456415174.2667  0.0022  3.8188\n",
      "     57  0.6180  \u001b[32m223405984.9500\u001b[0m  1474996140.2032  0.0022  3.5347\n",
      "     58  0.6183  \u001b[32m222890818.0000\u001b[0m  1474029806.4582  0.0022  3.9120\n",
      "     59  0.6204  231855241.8000  1465751253.8898  0.0022  4.9634\n",
      "     60  0.6209  232449681.4500  1463823858.9683  0.0022  4.0357\n",
      "     61  0.6198  230986064.8500  1467961246.4738  0.0011  3.7865\n",
      "     62  0.6148  \u001b[32m218335579.7500\u001b[0m  1487453239.4658  0.0011  3.5409\n",
      "     63  0.6206  \u001b[32m202881877.6000\u001b[0m  1464978405.5379  0.0011  4.1748\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 54.\n",
      "[CV 2/5] END batch_size=128, lambda1=0.0001, lr=0.00447, module__dropout_rate=0.5, module__n_units_last=256, module__num_hidden_layers=3;, score=0.646 total time= 4.3min\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-2.3544\u001b[0m  \u001b[32m12716774278.4000\u001b[0m  \u001b[35m13029145434.9480\u001b[0m  0.0045  3.3846\n",
      "      2  -2.2828  \u001b[32m12533555558.4000\u001b[0m  \u001b[35m12750970754.2176\u001b[0m  0.0045  3.6196\n",
      "      3  -2.1708  \u001b[32m12171967020.8000\u001b[0m  \u001b[35m12316020743.1762\u001b[0m  0.0045  4.1716\n",
      "      4  -2.0159  \u001b[32m11633324294.4000\u001b[0m  \u001b[35m11714343438.2527\u001b[0m  0.0045  4.5291\n",
      "      5  -1.8463  \u001b[32m10976890208.0000\u001b[0m  \u001b[35m11055794717.4024\u001b[0m  0.0045  4.0266\n",
      "      6  -1.6759  \u001b[32m10207051136.0000\u001b[0m  \u001b[35m10393981731.1333\u001b[0m  0.0045  3.5901\n",
      "      7  -1.4359  \u001b[32m9374942851.2000\u001b[0m  \u001b[35m9461761404.9352\u001b[0m  0.0045  4.0372\n",
      "      8  -1.2318  \u001b[32m8496675580.8000\u001b[0m  \u001b[35m8668999144.5778\u001b[0m  0.0045  3.3290\n",
      "      9  -1.0399  \u001b[32m7626161196.8000\u001b[0m  \u001b[35m7923327754.8141\u001b[0m  0.0045  3.0857\n",
      "     10  -0.8465  \u001b[32m6754920588.8000\u001b[0m  \u001b[35m7172399506.8624\u001b[0m  0.0045  3.1252\n",
      "     11  -0.6612  \u001b[32m5918569512.0000\u001b[0m  \u001b[35m6452580736.3239\u001b[0m  0.0045  3.1109\n",
      "     12  -0.4901  \u001b[32m5146320371.2000\u001b[0m  \u001b[35m5787848654.4645\u001b[0m  0.0045  4.0224\n",
      "     13  -0.3259  \u001b[32m4425611133.6000\u001b[0m  \u001b[35m5150117846.7370\u001b[0m  0.0045  3.7731\n",
      "     14  -0.2104  \u001b[32m3773983737.6000\u001b[0m  \u001b[35m4701438195.9899\u001b[0m  0.0045  3.1212\n",
      "     15  -0.0444  \u001b[32m3217970782.4000\u001b[0m  \u001b[35m4056536766.5673\u001b[0m  0.0045  3.3686\n",
      "     16  0.1181  \u001b[32m2697890946.4000\u001b[0m  \u001b[35m3425371685.5752\u001b[0m  0.0045  3.1783\n",
      "     17  0.1694  \u001b[32m2263490716.4000\u001b[0m  \u001b[35m3226365831.9984\u001b[0m  0.0045  3.6904\n",
      "     18  0.2753  \u001b[32m1927489176.8000\u001b[0m  \u001b[35m2814994584.7927\u001b[0m  0.0045  4.2148\n",
      "     19  0.3018  \u001b[32m1617081491.2000\u001b[0m  \u001b[35m2712070353.3050\u001b[0m  0.0045  3.6341\n",
      "     20  0.4093  \u001b[32m1352397099.6000\u001b[0m  \u001b[35m2294596192.5295\u001b[0m  0.0045  4.0326\n",
      "     21  0.4742  \u001b[32m1181471169.2000\u001b[0m  \u001b[35m2042321845.2980\u001b[0m  0.0045  3.8639\n",
      "     22  0.4635  \u001b[32m999964076.4000\u001b[0m  2083834791.4939  0.0045  3.6642\n",
      "     23  0.5091  \u001b[32m867956507.4000\u001b[0m  \u001b[35m1906793363.9587\u001b[0m  0.0045  3.4228\n",
      "     24  0.5301  \u001b[32m773054778.6000\u001b[0m  \u001b[35m1825156211.4915\u001b[0m  0.0045  4.1538\n",
      "     25  0.5008  \u001b[32m699902458.2000\u001b[0m  1939028620.4337  0.0045  3.2536\n",
      "     26  0.5580  \u001b[32m642484145.6000\u001b[0m  \u001b[35m1716839398.9083\u001b[0m  0.0045  3.4123\n",
      "     27  0.5773  \u001b[32m581223626.8000\u001b[0m  \u001b[35m1641755256.5497\u001b[0m  0.0045  3.2244\n",
      "     28  0.5594  \u001b[32m535072688.3000\u001b[0m  1711495165.6079  0.0045  4.1055\n",
      "     29  0.5823  \u001b[32m532254280.5000\u001b[0m  \u001b[35m1622376274.0526\u001b[0m  0.0045  3.8965\n",
      "     30  0.5998  \u001b[32m522584193.5000\u001b[0m  \u001b[35m1554276000.4672\u001b[0m  0.0045  3.9923\n",
      "     31  0.6022  \u001b[32m480029138.0000\u001b[0m  \u001b[35m1545316074.8452\u001b[0m  0.0045  3.6363\n",
      "     32  0.6126  \u001b[32m461623380.8000\u001b[0m  \u001b[35m1504832345.0045\u001b[0m  0.0045  3.1759\n",
      "     33  0.6079  \u001b[32m456960506.0000\u001b[0m  1523165084.0319  0.0045  4.2663\n",
      "     34  0.6106  \u001b[32m430533961.0000\u001b[0m  1512528945.9591  0.0045  3.4604\n",
      "     35  0.6207  \u001b[32m425497167.1000\u001b[0m  \u001b[35m1473108056.7553\u001b[0m  0.0045  3.5355\n",
      "     36  0.6156  427818444.5000  1493143787.2938  0.0045  3.2286\n",
      "     37  0.6177  \u001b[32m408544693.5000\u001b[0m  1484895123.9089  0.0045  3.2176\n",
      "     38  0.6069  \u001b[32m398063309.2000\u001b[0m  1526903799.3786  0.0045  4.2292\n",
      "     39  0.6223  \u001b[32m371910071.4000\u001b[0m  \u001b[35m1467125205.6406\u001b[0m  0.0045  3.9674\n",
      "     40  0.6112  \u001b[32m361882527.6000\u001b[0m  1510072624.2149  0.0045  3.4350\n",
      "     41  0.6182  \u001b[32m328204644.4000\u001b[0m  1483074612.4010  0.0045  3.4181\n",
      "     42  0.6308  330945091.4000  \u001b[35m1434145749.2420\u001b[0m  0.0045  3.7914\n",
      "     43  0.6284  328451643.8000  1443537219.2766  0.0045  3.6980\n",
      "     44  0.6282  341762770.4000  1444125813.0364  0.0045  3.5778\n",
      "     45  0.6250  \u001b[32m320134493.5500\u001b[0m  1456494208.5233  0.0045  4.3126\n",
      "     46  0.6220  325763600.8000  1468366592.1993  0.0045  3.3556\n",
      "     47  0.6350  \u001b[32m308290851.5500\u001b[0m  \u001b[35m1417651534.5392\u001b[0m  0.0045  4.5715\n",
      "     48  0.6327  320275487.1000  1426833601.6072  0.0045  3.6446\n",
      "     49  0.6248  315547646.2000  1457438515.5538  0.0045  3.6410\n",
      "     50  0.6269  315565322.6000  1449109561.9576  0.0045  3.9647\n",
      "     51  0.6244  \u001b[32m294262121.7000\u001b[0m  1458736046.8445  0.0045  3.7659\n",
      "     52  0.6347  299866115.4500  1418963655.8116  0.0045  4.0428\n",
      "     53  0.6304  297752988.9500  1435712085.0675  0.0045  3.3066\n",
      "     54  0.6291  \u001b[32m288292629.7000\u001b[0m  1440531716.7592  0.0022  3.8052\n",
      "     55  0.6326  \u001b[32m257035538.0500\u001b[0m  1427129699.7937  0.0022  3.7197\n",
      "     56  0.6359  262889002.8000  \u001b[35m1414327034.5680\u001b[0m  0.0022  3.5844\n",
      "     57  0.6372  270749925.3500  \u001b[35m1409362749.0971\u001b[0m  0.0022  3.8311\n",
      "     58  0.6365  \u001b[32m251428962.6500\u001b[0m  1412085243.3156  0.0022  3.9162\n",
      "     59  0.6359  253680980.1000  1414424440.1760  0.0022  4.1563\n",
      "     60  0.6348  \u001b[32m246896692.4500\u001b[0m  1418512084.4945  0.0022  3.7214\n",
      "     61  0.6335  248491681.7000  1423705840.8005  0.0022  3.8557\n",
      "     62  0.6291  \u001b[32m242487780.8500\u001b[0m  1440835055.7539  0.0022  3.3607\n",
      "     63  0.6311  247476561.6500  1432770114.7284  0.0022  5.5281\n",
      "     64  0.6346  250913862.3500  1419347360.7164  0.0011  3.5502\n",
      "     65  0.6299  \u001b[32m238426133.2000\u001b[0m  1437549120.4859  0.0011  3.4753\n",
      "     66  0.6315  242984614.9000  1431288221.0785  0.0011  3.3004\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 57.\n",
      "[CV 3/5] END batch_size=128, lambda1=0.0001, lr=0.00447, module__dropout_rate=0.5, module__n_units_last=256, module__num_hidden_layers=3;, score=0.686 total time= 4.1min\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-2.2893\u001b[0m  \u001b[32m12680100313.6000\u001b[0m  \u001b[35m13019065563.4713\u001b[0m  0.0045  3.1798\n",
      "      2  -2.2153  \u001b[32m12500300627.2000\u001b[0m  \u001b[35m12726267575.2914\u001b[0m  0.0045  3.6042\n",
      "      3  -2.1053  \u001b[32m12130374380.8000\u001b[0m  \u001b[35m12290748454.3726\u001b[0m  0.0045  2.9941\n",
      "      4  -1.9681  \u001b[32m11603066995.2000\u001b[0m  \u001b[35m11747712666.6864\u001b[0m  0.0045  3.8315\n",
      "      5  -1.7860  \u001b[32m10943125942.4000\u001b[0m  \u001b[35m11027177126.6467\u001b[0m  0.0045  4.1300\n",
      "      6  -1.6263  \u001b[32m10162841536.0000\u001b[0m  \u001b[35m10394994632.5840\u001b[0m  0.0045  3.2778\n",
      "      7  -1.4074  \u001b[32m9334898003.2000\u001b[0m  \u001b[35m9528755852.8324\u001b[0m  0.0045  4.5725\n",
      "      8  -1.2156  \u001b[32m8470037456.0000\u001b[0m  \u001b[35m8769525150.9223\u001b[0m  0.0045  3.8391\n",
      "      9  -0.9787  \u001b[32m7589622940.8000\u001b[0m  \u001b[35m7831695985.8719\u001b[0m  0.0045  3.5449\n",
      "     10  -0.7791  \u001b[32m6708646521.6000\u001b[0m  \u001b[35m7041620762.4123\u001b[0m  0.0045  3.2174\n",
      "     11  -0.6426  \u001b[32m5882211657.6000\u001b[0m  \u001b[35m6501447041.6695\u001b[0m  0.0045  4.0168\n",
      "     12  -0.4659  \u001b[32m5120709193.6000\u001b[0m  \u001b[35m5802290838.6996\u001b[0m  0.0045  3.8228\n",
      "     13  -0.2541  \u001b[32m4408628369.6000\u001b[0m  \u001b[35m4963887096.9733\u001b[0m  0.0045  4.2690\n",
      "     14  -0.1400  \u001b[32m3760424387.2000\u001b[0m  \u001b[35m4512152930.5727\u001b[0m  0.0045  4.1855\n",
      "     15  0.0446  \u001b[32m3160604113.6000\u001b[0m  \u001b[35m3781503436.7452\u001b[0m  0.0045  4.5197\n",
      "     16  0.1136  \u001b[32m2700744980.0000\u001b[0m  \u001b[35m3508239752.0483\u001b[0m  0.0045  3.6574\n",
      "     17  0.2114  \u001b[32m2272523037.6000\u001b[0m  \u001b[35m3121350256.7008\u001b[0m  0.0045  3.4124\n",
      "     18  0.2680  \u001b[32m1897212283.2000\u001b[0m  \u001b[35m2897396845.0380\u001b[0m  0.0045  3.4552\n",
      "     19  0.3552  \u001b[32m1582463883.6000\u001b[0m  \u001b[35m2552100472.2383\u001b[0m  0.0045  3.9166\n",
      "     20  0.4163  \u001b[32m1362969101.4000\u001b[0m  \u001b[35m2310235038.7604\u001b[0m  0.0045  3.5546\n",
      "     21  0.4451  \u001b[32m1140731333.0000\u001b[0m  \u001b[35m2196360212.7685\u001b[0m  0.0045  3.2681\n",
      "     22  0.5167  \u001b[32m998525163.6000\u001b[0m  \u001b[35m1912886316.4773\u001b[0m  0.0045  3.3188\n",
      "     23  0.5238  \u001b[32m895566091.6000\u001b[0m  \u001b[35m1884848034.3734\u001b[0m  0.0045  3.5096\n",
      "     24  0.5604  \u001b[32m770897040.4000\u001b[0m  \u001b[35m1740084211.8777\u001b[0m  0.0045  3.7606\n",
      "     25  0.5623  \u001b[32m692860896.2000\u001b[0m  \u001b[35m1732620978.1585\u001b[0m  0.0045  3.1001\n",
      "     26  0.5541  \u001b[32m625637036.0000\u001b[0m  1765064631.4409  0.0045  3.4046\n",
      "     27  0.5829  \u001b[32m571690614.6000\u001b[0m  \u001b[35m1650842971.2034\u001b[0m  0.0045  3.2452\n",
      "     28  0.5791  \u001b[32m553414976.2000\u001b[0m  1665771029.2482  0.0045  3.1661\n",
      "     29  0.5894  \u001b[32m530984843.6000\u001b[0m  \u001b[35m1625292829.7886\u001b[0m  0.0045  3.5867\n",
      "     30  0.6022  \u001b[32m511198981.0000\u001b[0m  \u001b[35m1574494766.3212\u001b[0m  0.0045  3.2393\n",
      "     31  0.5834  \u001b[32m479369341.5000\u001b[0m  1648872987.0851  0.0045  3.8171\n",
      "     32  0.6237  \u001b[32m465381800.9000\u001b[0m  \u001b[35m1489374131.9961\u001b[0m  0.0045  3.0651\n",
      "     33  0.6069  \u001b[32m441775438.1000\u001b[0m  1556080150.5065  0.0045  4.2985\n",
      "     34  0.6143  \u001b[32m414811153.8000\u001b[0m  1526596160.0000  0.0045  4.3571\n",
      "     35  0.6119  \u001b[32m392927664.8000\u001b[0m  1536153181.6142  0.0045  3.4123\n",
      "     36  0.6188  394411058.6000  1508766982.0860  0.0045  3.1676\n",
      "     37  0.6256  \u001b[32m370960310.3000\u001b[0m  \u001b[35m1481771130.2441\u001b[0m  0.0045  3.2728\n",
      "     38  0.6342  \u001b[32m343702450.9000\u001b[0m  \u001b[35m1447823855.7539\u001b[0m  0.0045  3.3162\n",
      "     39  0.6329  \u001b[32m328952522.2000\u001b[0m  1452805159.7555  0.0045  3.5312\n",
      "     40  0.6280  330637897.3000  1472397417.2568  0.0045  3.6461\n",
      "     41  0.6301  333027848.4000  1464253989.2264  0.0045  4.5820\n",
      "     42  0.6245  \u001b[32m322536322.2000\u001b[0m  1486315680.1370  0.0045  3.6986\n",
      "     43  0.6383  324025856.7000  \u001b[35m1431457590.8554\u001b[0m  0.0045  3.8857\n",
      "     44  0.6386  \u001b[32m309598086.7000\u001b[0m  \u001b[35m1430524169.4063\u001b[0m  0.0045  3.4478\n",
      "     45  0.6193  313766418.3000  1506925721.7022  0.0045  3.0623\n",
      "     46  0.6381  \u001b[32m301018784.3000\u001b[0m  1432540830.5984  0.0045  3.2974\n",
      "     47  0.6362  305575312.6000  1439745343.8816  0.0045  3.3447\n",
      "     48  0.6347  \u001b[32m292879354.2000\u001b[0m  1445872653.4242  0.0045  3.4442\n",
      "     49  0.6338  311092939.6000  1449323363.9245  0.0045  4.0177\n",
      "     50  0.6361  \u001b[32m289878632.7500\u001b[0m  1440256010.7393  0.0045  3.4165\n",
      "     51  0.6387  \u001b[32m280446559.9500\u001b[0m  \u001b[35m1430071415.8707\u001b[0m  0.0022  3.3167\n",
      "     52  0.6341  \u001b[32m239994973.9500\u001b[0m  1448174737.9217  0.0022  3.0987\n",
      "     53  0.6393  247231881.5500  \u001b[35m1427542980.7717\u001b[0m  0.0022  3.3489\n",
      "     54  0.6394  240786841.0500  \u001b[35m1427245401.9202\u001b[0m  0.0022  3.5371\n",
      "     55  0.6401  249060110.6000  \u001b[35m1424449713.6663\u001b[0m  0.0022  3.9403\n",
      "     56  0.6428  258551837.7000  \u001b[35m1413905569.7691\u001b[0m  0.0022  3.1427\n",
      "     57  0.6367  242399855.4000  1437882580.9430  0.0022  3.4889\n",
      "     58  0.6381  \u001b[32m239561853.3000\u001b[0m  1432267873.0528  0.0022  2.9968\n",
      "     59  0.6365  252524346.5000  1438771155.0056  0.0022  3.9899\n",
      "     60  0.6404  \u001b[32m234648290.3000\u001b[0m  1423354797.5425  0.0022  3.4747\n",
      "     61  0.6345  \u001b[32m226611766.0000\u001b[0m  1446482979.2517  0.0022  4.1025\n",
      "     62  0.6411  245631517.5000  1420598274.8530  0.0022  4.5670\n",
      "     63  0.6415  \u001b[32m220261547.0000\u001b[0m  1418982248.3224  0.0011  3.4600\n",
      "     64  0.6444  \u001b[32m220097613.8500\u001b[0m  \u001b[35m1407563953.5231\u001b[0m  0.0011  3.0789\n",
      "     65  0.6444  227256584.9000  \u001b[35m1407461733.2949\u001b[0m  0.0011  3.0095\n",
      "     66  0.6346  221273129.8500  1446397877.1485  0.0011  3.6668\n",
      "     67  0.6397  224971068.9000  1426246896.0093  0.0011  2.9640\n",
      "     68  0.6456  227613081.1500  \u001b[35m1402834677.6157\u001b[0m  0.0011  2.8942\n",
      "     69  0.6406  244485115.6000  1422421694.4925  0.0011  3.2784\n",
      "     70  0.6408  \u001b[32m216182605.9500\u001b[0m  1421915646.8725  0.0011  3.1902\n",
      "     71  0.6369  237945757.1000  1437210838.7183  0.0011  3.5788\n",
      "     72  0.6426  \u001b[32m213923822.1000\u001b[0m  1414791354.9356  0.0011  3.3408\n",
      "     73  0.6341  226286898.3500  1448206428.7919  0.0011  3.6259\n",
      "     74  0.6415  214930941.3500  1418808750.9877  0.0011  3.5695\n",
      "     75  0.6372  226317391.4500  1436166951.4316  0.0006  4.4889\n",
      "     76  0.6341  \u001b[32m205139812.2000\u001b[0m  1448146926.7136  0.0006  4.6831\n",
      "     77  0.6451  \u001b[32m201246152.6500\u001b[0m  1404825845.3790  0.0006  3.6079\n",
      "     78  0.6457  226658865.1000  \u001b[35m1402138564.6035\u001b[0m  0.0006  3.7756\n",
      "     79  0.6401  \u001b[32m199682920.9500\u001b[0m  1424336005.0644  0.0006  3.1883\n",
      "     80  0.6431  226586491.9500  1412519274.6521  0.0006  3.4182\n",
      "     81  0.6404  \u001b[32m196258846.5500\u001b[0m  1423179286.9550  0.0006  3.0825\n",
      "     82  0.6365  211042609.0000  1438602970.4933  0.0006  3.3546\n",
      "     83  0.6384  208997206.7000  1431114289.1057  0.0006  3.4097\n",
      "     84  0.6415  208637671.1000  1418797360.9687  0.0006  4.3851\n",
      "     85  0.6395  203116686.1500  1426894071.5219  0.0003  3.0197\n",
      "     86  0.6196  \u001b[32m195310949.7000\u001b[0m  1505466973.5270  0.0003  3.0882\n",
      "     87  0.6437  204483455.8500  1410113007.2307  0.0003  4.8246\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 78.\n",
      "[CV 4/5] END batch_size=128, lambda1=0.0001, lr=0.00447, module__dropout_rate=0.5, module__n_units_last=256, module__num_hidden_layers=3;, score=0.644 total time= 5.3min\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-2.2669\u001b[0m  \u001b[32m12609184550.4000\u001b[0m  \u001b[35m13291240604.1814\u001b[0m  0.0045  4.3061\n",
      "      2  -2.1989  \u001b[32m12428729107.2000\u001b[0m  \u001b[35m13014665451.4183\u001b[0m  0.0045  3.7090\n",
      "      3  -2.0911  \u001b[32m12059943273.6000\u001b[0m  \u001b[35m12576045982.0253\u001b[0m  0.0045  3.5819\n",
      "      4  -1.9487  \u001b[32m11533675824.0000\u001b[0m  \u001b[35m11996477852.1316\u001b[0m  0.0045  3.7977\n",
      "      5  -1.7709  \u001b[32m10869937324.8000\u001b[0m  \u001b[35m11273408699.3778\u001b[0m  0.0045  4.6588\n",
      "      6  -1.6032  \u001b[32m10104676160.0000\u001b[0m  \u001b[35m10590828353.7816\u001b[0m  0.0045  3.5481\n",
      "      7  -1.4120  \u001b[32m9274311110.4000\u001b[0m  \u001b[35m9813259014.0798\u001b[0m  0.0045  3.1928\n",
      "      8  -1.2122  \u001b[32m8406585353.6000\u001b[0m  \u001b[35m9000400060.9726\u001b[0m  0.0045  3.2299\n",
      "      9  -1.0039  \u001b[32m7528705904.0000\u001b[0m  \u001b[35m8152736427.5305\u001b[0m  0.0045  3.5128\n",
      "     10  -0.8386  \u001b[32m6676779350.4000\u001b[0m  \u001b[35m7480320177.6103\u001b[0m  0.0045  5.1979\n",
      "     11  -0.6737  \u001b[32m5849537937.6000\u001b[0m  \u001b[35m6809378373.5192\u001b[0m  0.0045  3.8840\n",
      "     12  -0.4569  \u001b[32m5068543686.4000\u001b[0m  \u001b[35m5927504699.7267\u001b[0m  0.0045  3.6527\n",
      "     13  -0.3422  \u001b[32m4365087492.8000\u001b[0m  \u001b[35m5460583870.1437\u001b[0m  0.0045  3.6590\n",
      "     14  -0.1541  \u001b[32m3732817276.8000\u001b[0m  \u001b[35m4695588319.1590\u001b[0m  0.0045  2.7792\n",
      "     15  -0.0641  \u001b[32m3153551464.8000\u001b[0m  \u001b[35m4329386771.1240\u001b[0m  0.0045  3.4387\n",
      "     16  0.0098  \u001b[32m2655944064.0000\u001b[0m  \u001b[35m4028744323.3015\u001b[0m  0.0045  3.1684\n",
      "     17  0.1619  \u001b[32m2250434439.2000\u001b[0m  \u001b[35m3409865139.9525\u001b[0m  0.0045  4.3989\n",
      "     18  0.2293  \u001b[32m1876449957.2000\u001b[0m  \u001b[35m3135549051.2283\u001b[0m  0.0045  3.0472\n",
      "     19  0.2909  \u001b[32m1570808528.8000\u001b[0m  \u001b[35m2884773156.5474\u001b[0m  0.0045  3.1255\n",
      "     20  0.3336  \u001b[32m1347760772.8000\u001b[0m  \u001b[35m2711310165.1298\u001b[0m  0.0045  3.6725\n",
      "     21  0.3993  \u001b[32m1144710236.0000\u001b[0m  \u001b[35m2443925427.3919\u001b[0m  0.0045  3.2600\n",
      "     22  0.4434  \u001b[32m970677544.4000\u001b[0m  \u001b[35m2264530490.7674\u001b[0m  0.0045  3.3905\n",
      "     23  0.4376  \u001b[32m832893339.8000\u001b[0m  2288151451.7080  0.0045  3.1685\n",
      "     24  0.5054  \u001b[32m767604004.2000\u001b[0m  \u001b[35m2012061929.8454\u001b[0m  0.0045  3.8566\n",
      "     25  0.5299  \u001b[32m711349637.4000\u001b[0m  \u001b[35m1912609362.4076\u001b[0m  0.0045  3.2828\n",
      "     26  0.5285  \u001b[32m629909363.1000\u001b[0m  1918159508.3481  0.0045  4.1685\n",
      "     27  0.5316  \u001b[32m571102247.2000\u001b[0m  \u001b[35m1905697660.5022\u001b[0m  0.0045  3.1552\n",
      "     28  0.5410  \u001b[32m528453298.8000\u001b[0m  \u001b[35m1867555099.1224\u001b[0m  0.0045  3.5033\n",
      "     29  0.5453  \u001b[32m498321236.5000\u001b[0m  \u001b[35m1850108197.2716\u001b[0m  0.0045  3.3511\n",
      "     30  0.5635  \u001b[32m488052162.9000\u001b[0m  \u001b[35m1775803506.3235\u001b[0m  0.0045  3.6073\n",
      "     31  0.5599  \u001b[32m447149464.2000\u001b[0m  1790444168.8269  0.0045  3.1305\n",
      "     32  0.5701  464909249.5000  \u001b[35m1748952277.9459\u001b[0m  0.0045  3.6762\n",
      "     33  0.5669  \u001b[32m428775892.6000\u001b[0m  1762201291.5772  0.0045  2.7469\n",
      "     34  0.5786  432181021.0000  \u001b[35m1714439198.7791\u001b[0m  0.0045  3.6906\n",
      "     35  0.5558  \u001b[32m401119396.0000\u001b[0m  1807262319.4674  0.0045  3.2057\n",
      "     36  0.5830  \u001b[32m378403807.4000\u001b[0m  \u001b[35m1696577491.6099\u001b[0m  0.0045  3.3833\n",
      "     37  0.5801  \u001b[32m342527873.4000\u001b[0m  1708157362.9714  0.0045  3.1826\n",
      "     38  0.5685  \u001b[32m307461623.7000\u001b[0m  1755445016.6151  0.0045  3.1784\n",
      "     39  0.5880  336693882.6000  \u001b[35m1676375425.4296\u001b[0m  0.0045  3.3684\n",
      "     40  0.5938  327579979.2000  \u001b[35m1652739439.7103\u001b[0m  0.0045  2.7192\n",
      "     41  0.5912  314958670.3000  1663341653.9490  0.0045  3.5076\n",
      "     42  0.5831  313802963.4000  1696196289.4172  0.0045  2.7640\n",
      "     43  0.5996  \u001b[32m283408741.0000\u001b[0m  \u001b[35m1629008152.9142\u001b[0m  0.0045  3.6410\n",
      "     44  0.5931  318416914.5000  1655374081.5106  0.0045  3.5863\n",
      "     45  0.5921  288822453.8000  1659624069.3136  0.0045  3.4830\n",
      "     46  0.5955  286194128.5000  1645594774.1982  0.0045  2.7422\n",
      "     47  0.5966  290528897.1500  1641237172.4882  0.0045  3.5983\n",
      "     48  0.5976  \u001b[32m278033259.2500\u001b[0m  1637281406.6918  0.0045  2.7219\n",
      "     49  0.5963  285039173.6000  1642453782.9114  0.0045  2.9686\n",
      "     50  0.5977  \u001b[32m272236613.5000\u001b[0m  1636906346.1818  0.0022  2.9171\n",
      "     51  0.5922  277737825.6500  1659243343.7446  0.0022  3.2634\n",
      "     52  0.5905  \u001b[32m257718979.8500\u001b[0m  1666041060.5505  0.0022  3.9438\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 43.\n",
      "[CV 5/5] END batch_size=128, lambda1=0.0001, lr=0.00447, module__dropout_rate=0.5, module__n_units_last=256, module__num_hidden_layers=3;, score=0.677 total time= 3.1min\n",
      "  epoch       r2        train_loss        valid_loss      lr     dur\n",
      "-------  -------  ----------------  ----------------  ------  ------\n",
      "      1  \u001b[36m-2.3312\u001b[0m  \u001b[32m12693187353.6000\u001b[0m  \u001b[35m13000536869.1979\u001b[0m  0.0045  3.6187\n",
      "      2  -2.2279  \u001b[32m12414995225.6000\u001b[0m  \u001b[35m12597301260.9176\u001b[0m  0.0045  3.4800\n",
      "      3  -2.0546  \u001b[32m11835506984.9600\u001b[0m  \u001b[35m11921057981.2989\u001b[0m  0.0045  3.6193\n",
      "      4  -1.8242  \u001b[32m11062783029.7600\u001b[0m  \u001b[35m11021941218.4171\u001b[0m  0.0045  4.3358\n",
      "      5  -1.5912  \u001b[32m10118210362.8800\u001b[0m  \u001b[35m10112748596.6273\u001b[0m  0.0045  4.1302\n",
      "      6  -1.3095  \u001b[32m9066732817.9200\u001b[0m  \u001b[35m9013285627.4150\u001b[0m  0.0045  4.1776\n",
      "      7  -1.0678  \u001b[32m7981161551.3600\u001b[0m  \u001b[35m8070070482.5093\u001b[0m  0.0045  3.4460\n",
      "      8  -0.7766  \u001b[32m6892472829.4400\u001b[0m  \u001b[35m6933389619.4711\u001b[0m  0.0045  4.7616\n",
      "      9  -0.5937  \u001b[32m5863744128.0000\u001b[0m  \u001b[35m6219793721.1325\u001b[0m  0.0045  4.3861\n",
      "     10  -0.3685  \u001b[32m4919410880.0000\u001b[0m  \u001b[35m5340808515.8972\u001b[0m  0.0045  4.0506\n",
      "     11  -0.2070  \u001b[32m4081710551.0400\u001b[0m  \u001b[35m4710530261.1406\u001b[0m  0.0045  4.9220\n",
      "     12  -0.0463  \u001b[32m3327688480.6400\u001b[0m  \u001b[35m4083526243.4337\u001b[0m  0.0045  4.3409\n",
      "     13  0.0995  \u001b[32m2697860834.5600\u001b[0m  \u001b[35m3514381632.9469\u001b[0m  0.0045  3.9288\n",
      "     14  0.2415  \u001b[32m2169619004.4800\u001b[0m  \u001b[35m2960154223.6736\u001b[0m  0.0045  4.4318\n",
      "     15  0.3608  \u001b[32m1758869820.4800\u001b[0m  \u001b[35m2494488875.6169\u001b[0m  0.0045  3.7839\n",
      "     16  0.4164  \u001b[32m1426666780.4800\u001b[0m  \u001b[35m2277717995.1484\u001b[0m  0.0045  4.0715\n",
      "     17  0.4511  \u001b[32m1188873808.9600\u001b[0m  \u001b[35m2142004787.4711\u001b[0m  0.0045  3.5095\n",
      "     18  0.4961  \u001b[32m980525723.3600\u001b[0m  \u001b[35m1966653571.0500\u001b[0m  0.0045  5.0170\n",
      "     19  0.4988  \u001b[32m842519084.6400\u001b[0m  \u001b[35m1955893489.5474\u001b[0m  0.0045  4.8466\n",
      "     20  0.5304  \u001b[32m734797861.7600\u001b[0m  \u001b[35m1832876379.6194\u001b[0m  0.0045  4.2652\n",
      "     21  0.5717  \u001b[32m673781912.3200\u001b[0m  \u001b[35m1671358674.3697\u001b[0m  0.0045  4.0470\n",
      "     22  0.5813  \u001b[32m638606979.2000\u001b[0m  \u001b[35m1634047188.0841\u001b[0m  0.0045  4.0395\n",
      "     23  0.5796  \u001b[32m587574860.0000\u001b[0m  1640604711.7296  0.0045  4.2301\n",
      "     24  0.5880  \u001b[32m529469886.4800\u001b[0m  \u001b[35m1607797052.5613\u001b[0m  0.0045  3.4456\n",
      "     25  0.6204  \u001b[32m512462794.0000\u001b[0m  \u001b[35m1481344239.7134\u001b[0m  0.0045  4.5539\n",
      "     26  0.6148  \u001b[32m485669875.6800\u001b[0m  1503377364.5825  0.0045  3.7726\n",
      "     27  0.6014  \u001b[32m454130934.8000\u001b[0m  1555508507.1509  0.0045  3.7929\n",
      "     28  0.6165  \u001b[32m452603057.8400\u001b[0m  1496706625.9835  0.0045  4.1310\n",
      "     29  0.6088  \u001b[32m405334397.7600\u001b[0m  1526689824.4734  0.0045  3.6559\n",
      "     30  0.6231  \u001b[32m403186745.2800\u001b[0m  \u001b[35m1470732405.8334\u001b[0m  0.0045  4.0323\n",
      "     31  0.6219  \u001b[32m373512057.8400\u001b[0m  1475708641.4801  0.0045  3.5944\n",
      "     32  0.6295  \u001b[32m368457975.5200\u001b[0m  \u001b[35m1446043746.8357\u001b[0m  0.0045  4.1892\n",
      "     33  0.6344  \u001b[32m360255498.4000\u001b[0m  \u001b[35m1426664412.9949\u001b[0m  0.0045  4.3085\n",
      "     34  0.6205  \u001b[32m342944350.7200\u001b[0m  1481246880.9718  0.0045  3.6734\n",
      "     35  0.6226  \u001b[32m329547796.8000\u001b[0m  1472740995.6879  0.0045  4.1360\n",
      "     36  0.6282  344478128.2400  1450876178.4395  0.0045  3.5782\n",
      "     37  0.6249  346766899.6000  1463915478.0975  0.0045  4.1324\n",
      "     38  0.6310  331815822.4800  1440082726.2545  0.0045  3.1943\n",
      "     39  0.6438  354945878.3200  \u001b[35m1390121991.7944\u001b[0m  0.0045  3.5902\n",
      "     40  0.6367  339715608.3200  1417832500.5874  0.0045  3.4177\n",
      "     41  0.6260  343090360.8000  1459422361.4365  0.0045  4.0658\n",
      "     42  0.6326  \u001b[32m316661446.4800\u001b[0m  1433913922.5815  0.0045  3.9683\n",
      "     43  0.6301  322333250.2400  1443733625.4814  0.0045  4.5559\n",
      "     44  0.6320  \u001b[32m298031814.4400\u001b[0m  1436319013.7362  0.0045  4.6285\n",
      "     45  0.6276  316083301.6800  1453173294.0888  0.0045  4.0611\n",
      "     46  0.6422  298090669.2800  1396203818.1816  0.0022  6.1122\n",
      "     47  0.6248  \u001b[32m253239390.1200\u001b[0m  1464093979.0313  0.0022  5.1074\n",
      "     48  0.6303  264858452.7600  1442903363.8374  0.0022  4.2512\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 39.\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "y_train_tensor = torch.tensor(np.array(y_train).reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "model = make_pipeline(\n",
    "    clone(preprocessor), \n",
    "    TensorTransformer(),\n",
    "    BayesSearchCV(\n",
    "        CustomNeuralNetRegressor(\n",
    "            Model,\n",
    "            max_epochs=100,\n",
    "            torch_load_kwargs={'weights_only': True},\n",
    "            criterion=nn.MSELoss,\n",
    "            optimizer=optim.AdamW,\n",
    "            iterator_train__shuffle=True,\n",
    "            iterator_train__drop_last=True,\n",
    "            train_split=dataset.ValidSplit(cv=5),\n",
    "            callbacks=[\n",
    "                EarlyStopping(patience=10, monitor='valid_loss', load_best=True),\n",
    "                LRScheduler(policy=optim.lr_scheduler.ReduceLROnPlateau, patience=5, factor=0.5, monitor='valid_loss'),  # type: ignore\n",
    "                EpochScoring(scoring='r2', on_train=False),\n",
    "            ],\n",
    "            device=DEVICE,\n",
    "        ),\n",
    "        # Comment to use tuned hyperparameters\n",
    "        {\n",
    "            'lambda1': [0.0001],\n",
    "            'lr': [0.00447],\n",
    "            'batch_size': [128],\n",
    "            'module__num_hidden_layers': [3],\n",
    "            'module__n_units_last': [256],\n",
    "            'module__dropout_rate': [0.5],\n",
    "        },\n",
    "        # Uncomment to tune hyperparameters\n",
    "        # { \n",
    "        #     'lambda1': (1e-4, 1e-1, 'log-uniform'),\n",
    "        #     'lr': (1e-4, 1e-1, 'log-uniform'),\n",
    "        #     'batch_size': [32, 64, 128, 256],\n",
    "        #     'module__num_hidden_layers': [1, 2, 3, 4],\n",
    "        #     'module__n_units_last': [16, 32, 64, 128, 256],\n",
    "        #     'module__dropout_rate': (0.1, 0.5, 'uniform'),\n",
    "        # },\n",
    "        verbose=3,\n",
    "        scoring='r2',\n",
    "        n_iter=1,\n",
    "        # n_iter=50,\n",
    "        random_state=42,\n",
    "        cv=KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    )\n",
    ").fit(X_train, y_train_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ce3f0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([247.37242007]),\n",
       " 'std_fit_time': array([43.10327035]),\n",
       " 'mean_score_time': array([0.22405906]),\n",
       " 'std_score_time': array([0.04551781]),\n",
       " 'param_batch_size': masked_array(data=[128],\n",
       "              mask=[False],\n",
       "        fill_value=999999),\n",
       " 'param_lambda1': masked_array(data=[0.0001],\n",
       "              mask=[False],\n",
       "        fill_value=1e+20),\n",
       " 'param_lr': masked_array(data=[0.00447],\n",
       "              mask=[False],\n",
       "        fill_value=1e+20),\n",
       " 'param_module__dropout_rate': masked_array(data=[0.5],\n",
       "              mask=[False],\n",
       "        fill_value=1e+20),\n",
       " 'param_module__n_units_last': masked_array(data=[256],\n",
       "              mask=[False],\n",
       "        fill_value=999999),\n",
       " 'param_module__num_hidden_layers': masked_array(data=[3],\n",
       "              mask=[False],\n",
       "        fill_value=999999),\n",
       " 'params': [OrderedDict([('batch_size', 128),\n",
       "               ('lambda1', 0.0001),\n",
       "               ('lr', 0.00447),\n",
       "               ('module__dropout_rate', 0.5),\n",
       "               ('module__n_units_last', 256),\n",
       "               ('module__num_hidden_layers', 3)])],\n",
       " 'split0_test_score': array([0.68820858]),\n",
       " 'split1_test_score': array([0.6457184]),\n",
       " 'split2_test_score': array([0.68603957]),\n",
       " 'split3_test_score': array([0.6436944]),\n",
       " 'split4_test_score': array([0.67742282]),\n",
       " 'mean_test_score': array([0.66821675]),\n",
       " 'std_test_score': array([0.01954277]),\n",
       " 'rank_test_score': array([1], dtype=int32)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = model[-1]\n",
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffb657f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('batch_size', 128),\n",
       "             ('lambda1', 0.0001),\n",
       "             ('lr', 0.00447),\n",
       "             ('module__dropout_rate', 0.5),\n",
       "             ('module__n_units_last', 256),\n",
       "             ('module__num_hidden_layers', 3)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5e350ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 32103\n",
      "Train R2: 0.8977\n",
      "Train RMSE: 19280.7058\n",
      "Train MAE: 9552.3990\n"
     ]
    }
   ],
   "source": [
    "result_train = salary.evaluate_train_predictions(model.predict(X_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29620562",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f1bb3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_test, y_test) = salary.get_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7d3c255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 10000\n",
      "Test R2: 0.6726\n",
      "Test RMSE: 34290.0580\n",
      "Test MAE: 20141.6577\n"
     ]
    }
   ],
   "source": [
    "result_test = salary.evaluate_test_predictions(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9115cfa1",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4703ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_preprocessor = model[0]\n",
    "with open('models/preprocessor.cloudpickle', 'wb') as f:\n",
    "    cloudpickle.dump(trained_preprocessor, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2915a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = model[-1].best_estimator_\n",
    "net.save_params(f_params='models/mlp_params.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
