{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc2fc48f-dbcc-4e6c-bc0e-8d1e72e670a4",
   "metadata": {},
   "source": [
    "# Salary Prediction from LinkedIn Job Postings - Train Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3375fb27-0be7-4da8-9378-9137587226e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nginyc/repos/job_posting_salary_prediction/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import salary\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, TargetEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import random\n",
    "from skorch import NeuralNetRegressor, dataset\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, EpochScoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eab292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc83ff-8948-46b2-94bd-096093c31122",
   "metadata": {},
   "source": [
    "## Train & Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfcb495",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train) = salary.get_train_dataset()\n",
    "(X_test, y_test) = salary.get_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06ea4d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('title_sbert_pca_encoder', make_pipeline(\n",
    "                salary.SentenceBertEncoder(),\n",
    "                StandardScaler(),\n",
    "                PCA(n_components=0.9, random_state=42) \n",
    "            ), ['title']),\n",
    "            ('location_sbert_pca_encoder', make_pipeline(\n",
    "                salary.SentenceBertEncoder(),\n",
    "                StandardScaler(),\n",
    "                PCA(n_components=0.9, random_state=42) \n",
    "            ), ['location']),\n",
    "            ('company_industries_sbert_pca_encoder', make_pipeline(\n",
    "                SimpleImputer(strategy='constant', fill_value='Unknown'),\n",
    "                salary.SentenceBertEncoder(),\n",
    "                StandardScaler(),\n",
    "                PCA(n_components=0.9, random_state=42) \n",
    "            ), ['company_industries']),\n",
    "            ('requirements_sbert_pca_encoder', make_pipeline(\n",
    "                SimpleImputer(strategy='constant', fill_value='Unknown'),\n",
    "                salary.SentenceBertEncoder(),\n",
    "                StandardScaler(),\n",
    "                PCA(n_components=0.9, random_state=42) \n",
    "            ), ['Educational_Requirements', 'Preferred_Qualifications', 'Required_Skills']),\n",
    "            ('one_hot_encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), ['formatted_experience_level', 'formatted_work_type']),\n",
    "            ('target_encoder', make_pipeline(\n",
    "                TargetEncoder(random_state=42),\n",
    "                StandardScaler(),\n",
    "            ), ['norm_title', 'clustered_edu_req', 'clustered_pref_qual', 'clustered_req_skill', 'location_state', 'company_industries', 'formatted_experience_level', 'formatted_work_type']),\n",
    "            ('experience_level', salary.experience_level_encoder, ['formatted_experience_level']),\n",
    "            ('work_type', salary.work_type_encoder, ['formatted_work_type']),\n",
    "            ('remote_allowed', 'passthrough', ['remote_allowed']),\n",
    "            ('company_employee_count', make_pipeline(\n",
    "                SimpleImputer(strategy='median'),\n",
    "                StandardScaler(),\n",
    "            ), ['company_employee_count']),\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bee0b32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27885, 693)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_size, num_features) = clone(preprocessor).fit_transform(X_train, y_train).shape\n",
    "(train_size, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ec93e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_units_1=256, n_units_2=128, n_units_3=64,\n",
    "                dropout_rate=0.3, leaky_relu_slope=0.2):\n",
    "        super().__init__()\n",
    "        # Layer 1\n",
    "        self.linear1 = nn.Linear(num_features, n_units_1).double()\n",
    "        self.bn1 = nn.BatchNorm1d(n_units_1).double()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate).double()\n",
    "\n",
    "        # Layer 2\n",
    "        self.linear2 = nn.Linear(n_units_1, n_units_2).double()\n",
    "        self.bn2 = nn.BatchNorm1d(n_units_2).double()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate).double()\n",
    "\n",
    "        # Layer 3\n",
    "        self.linear3 = nn.Linear(n_units_2, n_units_3).double()\n",
    "        self.bn3 = nn.BatchNorm1d(n_units_3).double()\n",
    "        self.dropout3 = nn.Dropout(dropout_rate).double()\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(n_units_3, 1).double()\n",
    "\n",
    "        # Activation function\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=leaky_relu_slope).double()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Layer 1\n",
    "        X = self.leaky_relu(self.linear1(X))\n",
    "        X = self.bn1(X)\n",
    "        X = self.dropout1(X)\n",
    "\n",
    "        # Layer 2\n",
    "        X = self.leaky_relu(self.linear2(X))\n",
    "        X = self.bn2(X)\n",
    "        X = self.dropout2(X)\n",
    "\n",
    "        # Layer 3\n",
    "        X = self.leaky_relu(self.linear3(X))\n",
    "        X = self.bn3(X)\n",
    "        X = self.dropout3(X)\n",
    "\n",
    "        # Output layer\n",
    "        X = self.output(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9844899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nginyc/repos/job_posting_salary_prediction/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch       r2        train_loss       valid_loss      lr     dur\n",
      "-------  -------  ----------------  ---------------  ------  ------\n",
      "      1  \u001b[36m-1.0049\u001b[0m  \u001b[32m11310803984.4794\u001b[0m  \u001b[35m8017317688.9673\u001b[0m  0.0500  1.2315\n",
      "      2  0.2749  \u001b[32m4943660606.8604\u001b[0m  \u001b[35m2899475631.9895\u001b[0m  0.0500  1.4282\n",
      "      3  0.4804  \u001b[32m2350779603.1019\u001b[0m  \u001b[35m2077917644.0610\u001b[0m  0.0500  1.2294\n",
      "      4  0.5320  \u001b[32m1868645828.5832\u001b[0m  \u001b[35m1871566533.9728\u001b[0m  0.0500  1.1663\n",
      "      5  0.5504  \u001b[32m1746741505.7012\u001b[0m  \u001b[35m1798026135.4735\u001b[0m  0.0500  1.2045\n",
      "      6  0.5765  \u001b[32m1657345899.1574\u001b[0m  \u001b[35m1693412445.5932\u001b[0m  0.0500  1.2065\n",
      "      7  0.5705  \u001b[32m1613961236.1124\u001b[0m  1717595886.5485  0.0500  1.1979\n",
      "      8  0.5500  \u001b[32m1546857175.2528\u001b[0m  1799543725.6605  0.0500  1.1423\n",
      "      9  0.5395  \u001b[32m1539727724.6805\u001b[0m  1841543189.0935  0.0500  1.2030\n",
      "     10  0.5548  \u001b[32m1492516503.0028\u001b[0m  1780267156.8163  0.0500  1.1696\n",
      "     11  0.5452  \u001b[32m1489065880.3144\u001b[0m  1818878513.1821  0.0500  1.2095\n",
      "     12  0.5551  \u001b[32m1460891035.0887\u001b[0m  1779021070.2233  0.0500  1.3167\n",
      "     13  0.5828  \u001b[32m1301330688.9929\u001b[0m  \u001b[35m1668336096.7991\u001b[0m  0.0250  1.2120\n",
      "     14  0.5865  \u001b[32m1244017700.7786\u001b[0m  \u001b[35m1653562048.1708\u001b[0m  0.0250  1.3141\n",
      "     15  0.5937  \u001b[32m1201700219.3882\u001b[0m  \u001b[35m1624696124.7089\u001b[0m  0.0250  1.1476\n",
      "     16  0.5893  \u001b[32m1158448504.2725\u001b[0m  1642347368.1760  0.0250  1.2054\n",
      "     17  0.5939  \u001b[32m1148158872.9671\u001b[0m  \u001b[35m1624059205.4712\u001b[0m  0.0250  1.2243\n",
      "     18  0.5683  \u001b[32m1122271345.6162\u001b[0m  1726474764.6145  0.0250  1.5138\n",
      "     19  0.5833  \u001b[32m1095032314.6865\u001b[0m  1666498012.8360  0.0250  1.1606\n",
      "     20  0.5868  1137292554.0604  1652439696.1681  0.0250  1.3545\n",
      "     21  0.5690  1155131164.3074  1723333735.3073  0.0250  1.2580\n",
      "     22  0.5940  1099545837.4721  \u001b[35m1623392171.0686\u001b[0m  0.0250  1.2048\n",
      "     23  0.5943  1131566778.3918  \u001b[35m1622201105.5824\u001b[0m  0.0250  1.2332\n",
      "     24  0.5936  1109321197.8211  1625345421.5331  0.0250  1.2835\n",
      "     25  0.5781  1131134341.1443  1687234651.5226  0.0250  1.2408\n",
      "     26  0.5483  \u001b[32m1059312008.2935\u001b[0m  1806407730.1275  0.0250  1.2776\n",
      "     27  0.6000  1092970699.1998  \u001b[35m1599707135.1291\u001b[0m  0.0250  1.2977\n",
      "     28  0.6016  1115297899.3249  \u001b[35m1593055912.8826\u001b[0m  0.0250  1.2772\n",
      "     29  0.6078  1086296063.9484  \u001b[35m1568495958.8852\u001b[0m  0.0250  1.2145\n",
      "     30  0.5791  1096418444.7136  1682986619.0426  0.0250  1.3766\n",
      "     31  0.5854  1084559956.9130  1658032682.0722  0.0250  1.2420\n",
      "     32  0.5614  1101516476.2418  1753800134.4202  0.0250  1.2595\n",
      "     33  0.5586  1108222245.5928  1764992908.4622  0.0250  1.2518\n",
      "     34  0.5933  1116471957.7715  1626539712.1110  0.0250  1.3059\n",
      "     35  0.5924  1098581043.6623  1630092027.7887  0.0250  1.2717\n",
      "     36  0.5981  \u001b[32m974635904.5663\u001b[0m  1607110714.1978  0.0125  1.2947\n",
      "     37  0.5992  \u001b[32m949166471.3207\u001b[0m  1602797661.0923  0.0125  1.2316\n",
      "     38  0.6249  \u001b[32m923135067.7376\u001b[0m  \u001b[35m1499955296.4114\u001b[0m  0.0125  1.2735\n",
      "     39  0.5998  \u001b[32m913010045.5377\u001b[0m  1600376539.5571  0.0125  1.3973\n",
      "     40  0.5945  921880523.9258  1621620090.6790  0.0125  1.3330\n",
      "     41  0.5997  \u001b[32m875861893.1645\u001b[0m  1600843811.2688  0.0125  1.2620\n",
      "     42  0.5854  \u001b[32m869738909.5208\u001b[0m  1657984588.2913  0.0125  1.3331\n",
      "     43  0.5942  \u001b[32m862466155.5595\u001b[0m  1622863899.5559  0.0125  1.4356\n",
      "     44  0.6104  894780061.7827  1558108964.7609  0.0125  1.4340\n",
      "     45  0.5971  \u001b[32m823770451.7830\u001b[0m  1611015725.7824  0.0063  1.4985\n",
      "     46  0.6187  \u001b[32m797424974.1172\u001b[0m  1524672957.4824  0.0063  1.4044\n",
      "     47  0.5811  800217725.0989  1675086370.7962  0.0063  1.3711\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 38.\n",
      "  epoch       r2        train_loss       valid_loss      lr     dur\n",
      "-------  -------  ----------------  ---------------  ------  ------\n",
      "      1  \u001b[36m-1.1623\u001b[0m  \u001b[32m11099430665.7610\u001b[0m  \u001b[35m7675163988.4027\u001b[0m  0.0500  1.3780\n",
      "      2  0.2622  \u001b[32m4839653015.5294\u001b[0m  \u001b[35m2618956722.9735\u001b[0m  0.0500  1.2845\n",
      "      3  0.4694  \u001b[32m2277902806.1382\u001b[0m  \u001b[35m1883360661.1401\u001b[0m  0.0500  1.4824\n",
      "      4  0.5064  \u001b[32m1787348152.5192\u001b[0m  \u001b[35m1752155142.7205\u001b[0m  0.0500  1.3659\n",
      "      5  0.5475  \u001b[32m1668622574.0416\u001b[0m  \u001b[35m1606030256.8184\u001b[0m  0.0500  1.1494\n",
      "      6  0.5520  \u001b[32m1629255376.4644\u001b[0m  \u001b[35m1590293546.9081\u001b[0m  0.0500  1.3264\n",
      "      7  0.5633  \u001b[32m1562004887.9955\u001b[0m  \u001b[35m1550065219.1196\u001b[0m  0.0500  1.4242\n",
      "      8  0.5945  \u001b[32m1507527681.5094\u001b[0m  \u001b[35m1439481046.5857\u001b[0m  0.0500  1.3735\n",
      "      9  0.5647  \u001b[32m1475129742.5334\u001b[0m  1545270778.7454  0.0500  1.4985\n",
      "     10  0.5625  \u001b[32m1418299624.1009\u001b[0m  1552934426.0964  0.0500  1.3783\n",
      "     11  0.5775  1426795899.9335  1499759123.5205  0.0500  1.6386\n",
      "     12  0.5660  \u001b[32m1378718964.4019\u001b[0m  1540451009.2716  0.0500  1.4059\n",
      "     13  0.5731  \u001b[32m1375056783.1116\u001b[0m  1515400852.4369  0.0500  1.4472\n",
      "     14  0.5497  1398682933.3800  1598259645.8628  0.0500  1.5381\n",
      "     15  0.5871  \u001b[32m1225964992.8282\u001b[0m  1465628081.4769  0.0250  1.4349\n",
      "     16  0.5836  \u001b[32m1145843865.6266\u001b[0m  1477985861.1864  0.0250  1.2820\n",
      "     17  0.5744  \u001b[32m1099746162.2894\u001b[0m  1510567987.7204  0.0250  1.3249\n",
      "     18  0.5965  1113349209.8425  \u001b[35m1432246410.8674\u001b[0m  0.0250  1.5494\n",
      "     19  0.5763  \u001b[32m1098667880.9045\u001b[0m  1503929477.4073  0.0250  1.3549\n",
      "     20  0.5753  \u001b[32m1078649993.2449\u001b[0m  1507493347.3471  0.0250  1.2865\n",
      "     21  0.5938  \u001b[32m1056498016.9218\u001b[0m  1441837543.7762  0.0250  1.2594\n",
      "     22  0.5785  1060367703.6892  1496226215.9241  0.0250  1.2743\n",
      "     23  0.5991  1060252272.1793  \u001b[35m1423122391.6828\u001b[0m  0.0250  1.2500\n",
      "     24  0.5741  1065165014.5819  1511660353.3542  0.0250  1.2547\n",
      "     25  0.5709  1071780654.8005  1522973199.5305  0.0250  1.3207\n",
      "     26  0.5846  \u001b[32m1018719227.1047\u001b[0m  1474586566.1606  0.0250  1.4387\n",
      "     27  0.5787  1078966774.0657  1495534766.5923  0.0250  1.3159\n",
      "     28  0.5959  1033513732.0573  1434404365.8102  0.0250  1.2855\n",
      "     29  0.5859  1050283246.4086  1469937143.3689  0.0250  1.5134\n",
      "     30  0.5946  \u001b[32m956068480.1577\u001b[0m  1439082556.8669  0.0125  1.3247\n",
      "     31  0.5980  \u001b[32m883679547.2345\u001b[0m  1426941229.9339  0.0125  1.2627\n",
      "     32  0.6090  \u001b[32m862875021.1175\u001b[0m  \u001b[35m1387789880.7679\u001b[0m  0.0125  1.3498\n",
      "     33  0.6080  911343454.3934  1391533562.7459  0.0125  1.2852\n",
      "     34  0.5876  \u001b[32m847073891.3056\u001b[0m  1463976241.1312  0.0125  1.3831\n",
      "     35  0.5978  858776343.9472  1427544451.7522  0.0125  1.3388\n",
      "     36  0.6009  \u001b[32m839695294.5439\u001b[0m  1416742531.6766  0.0125  1.3541\n",
      "     37  0.5951  857853715.1900  1437216998.9082  0.0125  1.3859\n",
      "     38  0.5863  841421174.6680  1468419377.7411  0.0125  1.3869\n",
      "     39  0.5977  \u001b[32m780634195.1507\u001b[0m  1427939011.9042  0.0063  1.3103\n",
      "     40  0.5615  784852705.3442  1556580400.4786  0.0063  1.3439\n",
      "     41  0.5767  794087356.8056  1502365305.9057  0.0063  1.2913\n",
      "     42  0.6106  \u001b[32m766956030.8553\u001b[0m  \u001b[35m1382091946.2404\u001b[0m  0.0063  1.3835\n",
      "     43  0.5754  \u001b[32m733179742.5498\u001b[0m  1507107999.3231  0.0063  1.3043\n",
      "     44  0.6015  745753107.2276  1414654555.1473  0.0063  1.3412\n",
      "     45  0.6010  \u001b[32m728426349.6051\u001b[0m  1416344402.0997  0.0063  1.4798\n",
      "     46  0.5729  730113161.0937  1515976198.6120  0.0063  1.4069\n",
      "     47  0.6010  733516611.3300  1416392314.4869  0.0063  1.3394\n",
      "     48  0.5874  748963378.1073  1464592883.6732  0.0063  1.3086\n",
      "     49  0.6071  \u001b[32m724555564.6492\u001b[0m  1394681946.3224  0.0031  1.2957\n",
      "     50  0.6181  \u001b[32m711201024.4699\u001b[0m  \u001b[35m1355532806.5734\u001b[0m  0.0031  1.5682\n",
      "     51  0.6139  712356222.4664  1370304683.6253  0.0031  1.4830\n",
      "     52  0.6218  \u001b[32m680279353.8625\u001b[0m  \u001b[35m1342491425.1070\u001b[0m  0.0031  1.4099\n",
      "     53  0.5911  \u001b[32m653886251.6280\u001b[0m  1451255451.9065  0.0031  1.4286\n",
      "     54  0.6052  665140405.6206  1401486115.9826  0.0031  1.7546\n",
      "     55  0.6125  675643803.0737  1375366054.1183  0.0031  1.4243\n",
      "     56  0.6130  688017145.9849  1373560289.3961  0.0031  1.4806\n",
      "     57  0.6129  \u001b[32m653691064.2339\u001b[0m  1374152516.5784  0.0031  1.4853\n",
      "     58  0.6081  661956961.3562  1391037135.2264  0.0031  1.3088\n",
      "     59  0.6024  \u001b[32m644847609.9757\u001b[0m  1411284080.1392  0.0016  1.4685\n",
      "     60  0.5858  667084149.2040  1470072958.0497  0.0016  1.5487\n",
      "     61  0.5863  \u001b[32m642425472.1394\u001b[0m  1468457221.0591  0.0016  1.6524\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 52.\n",
      "  epoch       r2        train_loss       valid_loss      lr     dur\n",
      "-------  -------  ----------------  ---------------  ------  ------\n",
      "      1  \u001b[36m-0.9904\u001b[0m  \u001b[32m11215521036.6037\u001b[0m  \u001b[35m7354473135.8340\u001b[0m  0.0500  1.3259\n",
      "      2  0.1572  \u001b[32m4844665638.3284\u001b[0m  \u001b[35m3114034757.2988\u001b[0m  0.0500  1.3157\n",
      "      3  0.4987  \u001b[32m2259875245.4388\u001b[0m  \u001b[35m1852202092.1299\u001b[0m  0.0500  1.3967\n",
      "      4  0.5382  \u001b[32m1790589701.6333\u001b[0m  \u001b[35m1706375957.2006\u001b[0m  0.0500  1.3514\n",
      "      5  0.5752  \u001b[32m1669616940.1044\u001b[0m  \u001b[35m1569493521.1879\u001b[0m  0.0500  1.3550\n",
      "      6  0.5807  \u001b[32m1575435860.2062\u001b[0m  \u001b[35m1549403357.1554\u001b[0m  0.0500  1.3589\n",
      "      7  0.5704  \u001b[32m1551912726.8047\u001b[0m  1587497921.2153  0.0500  1.4626\n",
      "      8  0.5596  \u001b[32m1476518789.3481\u001b[0m  1627351578.1253  0.0500  1.1436\n",
      "      9  0.5970  1488837039.1010  \u001b[35m1489041018.3500\u001b[0m  0.0500  1.1498\n",
      "     10  0.5952  \u001b[32m1441084390.9650\u001b[0m  1495718161.4211  0.0500  1.2908\n",
      "     11  0.5947  \u001b[32m1407726430.5990\u001b[0m  1497470453.1481  0.0500  1.3870\n",
      "     12  0.5875  1409727993.5021  1524202780.2574  0.0500  1.2717\n",
      "     13  0.6044  \u001b[32m1400166732.5529\u001b[0m  \u001b[35m1461720612.3782\u001b[0m  0.0500  1.3499\n",
      "     14  0.5979  \u001b[32m1381141760.0202\u001b[0m  1485743891.5156  0.0500  1.2548\n",
      "     15  0.6165  \u001b[32m1355925766.8394\u001b[0m  \u001b[35m1416916034.0195\u001b[0m  0.0500  1.3434\n",
      "     16  0.6069  1373581395.7923  1452334779.7922  0.0500  1.2973\n",
      "     17  0.5853  1374506749.3891  1532306698.0763  0.0500  1.2841\n",
      "     18  0.5952  \u001b[32m1341475815.0248\u001b[0m  1495785273.7631  0.0500  1.3699\n",
      "     19  0.5485  1360031137.7351  1668257897.1217  0.0500  1.6388\n",
      "     20  0.6094  1354273507.2763  1443136075.9859  0.0500  1.2550\n",
      "     21  0.5949  \u001b[32m1335610735.1965\u001b[0m  1496704005.3500  0.0500  1.3027\n",
      "     22  0.6299  \u001b[32m1218859696.0475\u001b[0m  \u001b[35m1367294289.6489\u001b[0m  0.0250  1.3999\n",
      "     23  0.6235  \u001b[32m1131623210.0691\u001b[0m  1391257684.1560  0.0250  1.3029\n",
      "     24  0.6026  \u001b[32m1103968032.3556\u001b[0m  1468420206.1297  0.0250  1.3344\n",
      "     25  0.6095  \u001b[32m1072242837.3224\u001b[0m  1442895750.1785  0.0250  1.2792\n",
      "     26  0.6112  1080457744.4162  1436493420.5226  0.0250  1.2735\n",
      "     27  0.6212  \u001b[32m1042301007.6442\u001b[0m  1399793977.4145  0.0250  1.4184\n",
      "     28  0.6137  1047661030.9799  1427198727.5277  0.0250  1.3416\n",
      "     29  0.6215  \u001b[32m975712864.4654\u001b[0m  1398653048.5928  0.0125  1.2744\n",
      "     30  0.6216  \u001b[32m935159053.9081\u001b[0m  1398234498.0461  0.0125  1.1886\n",
      "     31  0.6310  \u001b[32m907765073.6077\u001b[0m  \u001b[35m1363313445.4047\u001b[0m  0.0125  1.3119\n",
      "     32  0.6214  \u001b[32m896388636.7508\u001b[0m  1399017244.1263  0.0125  1.3092\n",
      "     33  0.6386  898848547.1846  \u001b[35m1335380891.2328\u001b[0m  0.0125  1.3675\n",
      "     34  0.6343  \u001b[32m883312541.3287\u001b[0m  1351074395.2021  0.0125  1.3126\n",
      "     35  0.6039  904896388.9310  1463489324.8571  0.0125  1.2734\n",
      "     36  0.6217  \u001b[32m842555059.4792\u001b[0m  1397696172.6362  0.0125  1.2350\n",
      "     37  0.6234  852616933.8860  1391538239.0421  0.0125  1.2839\n",
      "     38  0.6118  867686777.6653  1434352956.4144  0.0125  1.3136\n",
      "     39  0.6330  854377837.8756  1355943001.7297  0.0125  1.5306\n",
      "     40  0.6446  \u001b[32m799707618.2694\u001b[0m  \u001b[35m1313161701.3665\u001b[0m  0.0063  1.1716\n",
      "     41  0.6213  812752556.1003  1399282345.3772  0.0063  1.4197\n",
      "     42  0.6413  \u001b[32m759706411.2307\u001b[0m  1325209555.4379  0.0063  1.2760\n",
      "     43  0.6217  778687727.0994  1397740401.3505  0.0063  1.4493\n",
      "     44  0.6320  783804650.7735  1359795146.7370  0.0063  1.3528\n",
      "     45  0.6291  781805395.7325  1370376704.8705  0.0063  1.3658\n",
      "     46  0.6274  \u001b[32m735801849.7849\u001b[0m  1376861172.3605  0.0063  1.3688\n",
      "     47  0.6398  753317026.2242  1330870298.7207  0.0031  1.4054\n",
      "     48  0.6406  742141327.8380  1327788593.9471  0.0031  1.2875\n",
      "     49  0.6415  \u001b[32m692029356.5357\u001b[0m  1324623894.0527  0.0031  1.4409\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 40.\n",
      "  epoch       r2        train_loss       valid_loss      lr     dur\n",
      "-------  -------  ----------------  ---------------  ------  ------\n",
      "      1  \u001b[36m-1.1241\u001b[0m  \u001b[32m11233653653.5406\u001b[0m  \u001b[35m8408148976.2479\u001b[0m  0.0500  1.5036\n",
      "      2  0.1907  \u001b[32m4867705040.1798\u001b[0m  \u001b[35m3203614077.8172\u001b[0m  0.0500  1.2525\n",
      "      3  0.4974  \u001b[32m2275281968.7811\u001b[0m  \u001b[35m1989486054.0475\u001b[0m  0.0500  1.2249\n",
      "      4  0.5274  \u001b[32m1828645234.1047\u001b[0m  \u001b[35m1870788417.1419\u001b[0m  0.0500  1.2721\n",
      "      5  0.5545  \u001b[32m1699133762.7936\u001b[0m  \u001b[35m1763616088.7524\u001b[0m  0.0500  1.2334\n",
      "      6  0.5777  \u001b[32m1625530198.2339\u001b[0m  \u001b[35m1671569364.4951\u001b[0m  0.0500  1.1297\n",
      "      7  0.5892  \u001b[32m1590198138.6035\u001b[0m  \u001b[35m1626244270.2202\u001b[0m  0.0500  1.1805\n",
      "      8  0.5605  \u001b[32m1528367443.0067\u001b[0m  1739776148.3569  0.0500  1.1479\n",
      "      9  0.5742  \u001b[32m1490768757.8220\u001b[0m  1685574505.6946  0.0500  1.1893\n",
      "     10  0.5728  \u001b[32m1480635840.6777\u001b[0m  1691076552.4541  0.0500  1.2918\n",
      "     11  0.5482  \u001b[32m1468871819.9584\u001b[0m  1788240143.6871  0.0500  1.2611\n",
      "     12  0.5491  \u001b[32m1403855588.1995\u001b[0m  1785014662.8210  0.0500  1.2475\n",
      "     13  0.5899  1427286789.9432  \u001b[35m1623552681.7232\u001b[0m  0.0500  1.2304\n",
      "     14  0.5720  \u001b[32m1401684289.6713\u001b[0m  1694167225.5238  0.0500  1.2630\n",
      "     15  0.5925  \u001b[32m1378650002.8742\u001b[0m  \u001b[35m1613081988.2503\u001b[0m  0.0500  1.2677\n",
      "     16  0.5605  1380720044.4885  1739555551.1520  0.0500  1.1917\n",
      "     17  0.5722  \u001b[32m1368766687.1691\u001b[0m  1693418433.8986  0.0500  1.2103\n",
      "     18  0.5739  \u001b[32m1363480896.2925\u001b[0m  1686687700.5409  0.0500  1.2579\n",
      "     19  0.5814  \u001b[32m1353317002.6593\u001b[0m  1656916589.7532  0.0500  1.1816\n",
      "     20  0.5787  \u001b[32m1350172856.5588\u001b[0m  1667519271.4110  0.0500  1.2337\n",
      "     21  0.5874  1353610978.6363  1633244564.3690  0.0500  1.3049\n",
      "     22  0.5998  \u001b[32m1211831736.3105\u001b[0m  \u001b[35m1584150418.7429\u001b[0m  0.0250  1.2301\n",
      "     23  0.5936  \u001b[32m1117306487.2658\u001b[0m  1608734543.0958  0.0250  1.1390\n",
      "     24  0.5644  1124226491.1992  1724154851.3585  0.0250  1.1301\n",
      "     25  0.5875  \u001b[32m1097801927.4446\u001b[0m  1632913821.3828  0.0250  1.1586\n",
      "     26  0.5720  \u001b[32m1080238133.6614\u001b[0m  1694345801.8541  0.0250  1.3628\n",
      "     27  0.5767  \u001b[32m1076609739.1342\u001b[0m  1675709996.8065  0.0250  1.3193\n",
      "     28  0.5870  1098533521.9316  1634781363.7670  0.0250  1.0367\n",
      "     29  0.6024  \u001b[32m999057987.0856\u001b[0m  \u001b[35m1574005299.0903\u001b[0m  0.0125  1.1342\n",
      "     30  0.5953  \u001b[32m971790125.9487\u001b[0m  1601962810.0239  0.0125  1.2248\n",
      "     31  0.5778  \u001b[32m922254386.3898\u001b[0m  1671150471.4969  0.0125  1.2320\n",
      "     32  0.6103  \u001b[32m911698241.0779\u001b[0m  \u001b[35m1542710644.8576\u001b[0m  0.0125  1.4451\n",
      "     33  0.6000  928694786.9837  1583347694.7142  0.0125  1.2343\n",
      "     34  0.5713  \u001b[32m889011646.5008\u001b[0m  1697140932.1295  0.0125  1.4215\n",
      "     35  0.6045  902667585.0063  1565703650.5277  0.0125  1.2185\n",
      "     36  0.6172  902750130.9218  \u001b[35m1515383087.0083\u001b[0m  0.0125  1.4061\n",
      "     37  0.5802  919537355.1152  1661666647.4230  0.0125  1.3439\n",
      "     38  0.5826  \u001b[32m880656616.6599\u001b[0m  1652107230.8865  0.0125  1.2909\n",
      "     39  0.5853  889528224.7406  1641455557.4303  0.0125  1.2091\n",
      "     40  0.5583  899870020.1060  1748423742.5671  0.0125  1.3588\n",
      "     41  0.5849  \u001b[32m867780095.6061\u001b[0m  1643214599.7913  0.0125  1.4801\n",
      "     42  0.5779  887219477.8596  1671006200.6062  0.0125  1.3805\n",
      "     43  0.5920  \u001b[32m837728944.8016\u001b[0m  1614986270.1193  0.0063  1.3105\n",
      "     44  0.5657  \u001b[32m790188606.1973\u001b[0m  1718966801.9209  0.0063  1.4048\n",
      "     45  0.6018  816332385.6510  1576387340.1972  0.0063  1.3496\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 36.\n",
      "  epoch       r2        train_loss       valid_loss      lr     dur\n",
      "-------  -------  ----------------  ---------------  ------  ------\n",
      "      1  \u001b[36m-0.9567\u001b[0m  \u001b[32m11336569670.9267\u001b[0m  \u001b[35m7510461823.7166\u001b[0m  0.0500  1.4016\n",
      "      2  0.2007  \u001b[32m4948475216.9517\u001b[0m  \u001b[35m3067935824.2865\u001b[0m  0.0500  1.4605\n",
      "      3  0.4896  \u001b[32m2381666321.8104\u001b[0m  \u001b[35m1958878685.4292\u001b[0m  0.0500  1.4116\n",
      "      4  0.5583  \u001b[32m1849300841.8830\u001b[0m  \u001b[35m1695463050.3468\u001b[0m  0.0500  1.3705\n",
      "      5  0.5578  \u001b[32m1718716515.8390\u001b[0m  1697099511.3017  0.0500  1.2068\n",
      "      6  0.5542  \u001b[32m1672444238.8228\u001b[0m  1710959450.0746  0.0500  1.2093\n",
      "      7  0.5767  \u001b[32m1618031798.2562\u001b[0m  \u001b[35m1624606803.2378\u001b[0m  0.0500  1.1948\n",
      "      8  0.5813  \u001b[32m1572977261.8431\u001b[0m  \u001b[35m1607143055.0322\u001b[0m  0.0500  1.1813\n",
      "      9  0.5758  \u001b[32m1520486489.0390\u001b[0m  1628178365.1535  0.0500  1.1820\n",
      "     10  0.5643  \u001b[32m1488002048.9361\u001b[0m  1672277669.5823  0.0500  1.2516\n",
      "     11  0.5837  \u001b[32m1466448395.0758\u001b[0m  \u001b[35m1597752034.5686\u001b[0m  0.0500  1.2338\n",
      "     12  0.5643  1478947558.4612  1672159566.7460  0.0500  1.2568\n",
      "     13  0.5618  \u001b[32m1418888488.0637\u001b[0m  1681834672.6777  0.0500  1.2926\n",
      "     14  0.6019  1437890749.8268  \u001b[35m1528027659.0708\u001b[0m  0.0500  1.1473\n",
      "     15  0.5842  \u001b[32m1387274791.2305\u001b[0m  1595881446.8976  0.0500  1.2627\n",
      "     16  0.5739  \u001b[32m1367266876.1087\u001b[0m  1635323434.7330  0.0500  1.3883\n",
      "     17  0.5883  1416118612.6101  1580337267.4737  0.0500  1.2946\n",
      "     18  0.5828  1423973450.0191  1601305510.6024  0.0500  1.2807\n",
      "     19  0.5848  1400938858.5895  1593813402.1356  0.0500  1.2483\n",
      "     20  0.5961  \u001b[32m1365786121.5713\u001b[0m  1550336424.8894  0.0500  1.2127\n",
      "     21  0.5846  \u001b[32m1198578994.2741\u001b[0m  1594333301.8918  0.0250  1.2564\n",
      "     22  0.5804  \u001b[32m1164715121.8500\u001b[0m  1610689915.1975  0.0250  1.2429\n",
      "     23  0.5626  \u001b[32m1127159861.9401\u001b[0m  1678901681.8336  0.0250  1.2134\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 14.\n",
      "  epoch       r2        train_loss       valid_loss      lr     dur\n",
      "-------  -------  ----------------  ---------------  ------  ------\n",
      "      1  \u001b[36m-0.6507\u001b[0m  \u001b[32m10356941850.5546\u001b[0m  \u001b[35m6283255579.4073\u001b[0m  0.0500  1.5498\n",
      "      2  0.4313  \u001b[32m3588526492.0589\u001b[0m  \u001b[35m2164589176.0114\u001b[0m  0.0500  1.6239\n",
      "      3  0.5586  \u001b[32m1951844299.8058\u001b[0m  \u001b[35m1680124370.2527\u001b[0m  0.0500  1.5298\n",
      "      4  0.4809  \u001b[32m1797274819.9454\u001b[0m  1975798663.6091  0.0500  1.8591\n",
      "      5  0.5300  \u001b[32m1697530591.5789\u001b[0m  1788844358.3005  0.0500  1.7911\n",
      "      6  0.5948  \u001b[32m1668921330.5465\u001b[0m  \u001b[35m1542347139.5832\u001b[0m  0.0500  1.8188\n",
      "      7  0.5858  \u001b[32m1606883472.3952\u001b[0m  1576454029.7551  0.0500  1.6941\n",
      "      8  0.5768  \u001b[32m1579481193.6092\u001b[0m  1610705248.8131  0.0500  1.6145\n",
      "      9  0.5932  \u001b[32m1548340039.5801\u001b[0m  1548478570.0506  0.0500  1.7146\n",
      "     10  0.5848  1561348215.4638  1580555715.5286  0.0500  1.7051\n",
      "     11  0.5948  \u001b[32m1509068380.6907\u001b[0m  1542472746.9901  0.0500  1.5901\n",
      "     12  0.5905  \u001b[32m1486994257.4261\u001b[0m  1558675540.2094  0.0500  1.8483\n",
      "     13  0.5795  \u001b[32m1359265181.1332\u001b[0m  1600477793.4049  0.0250  1.6306\n",
      "     14  0.5925  \u001b[32m1254397045.7525\u001b[0m  1551139124.3651  0.0250  1.6078\n",
      "     15  0.6008  \u001b[32m1236252564.4002\u001b[0m  \u001b[35m1519622849.2560\u001b[0m  0.0250  1.7077\n",
      "     16  0.5931  \u001b[32m1225929846.2228\u001b[0m  1549033032.7427  0.0250  1.6538\n",
      "     17  0.5972  \u001b[32m1213769078.6747\u001b[0m  1533370765.6042  0.0250  1.6351\n",
      "     18  0.6128  \u001b[32m1167554551.5536\u001b[0m  \u001b[35m1473825128.3706\u001b[0m  0.0250  1.7370\n",
      "     19  0.5958  1169058817.3966  1538571242.0459  0.0250  1.5810\n",
      "     20  0.6091  \u001b[32m1157534791.9860\u001b[0m  1488111357.3757  0.0250  1.5566\n",
      "     21  0.5924  1191495631.1172  1551384373.7062  0.0250  1.4452\n",
      "     22  0.6065  1159493931.2470  1497898175.7267  0.0250  1.3139\n",
      "     23  0.6123  1187730964.0570  1475681389.0524  0.0250  1.5767\n",
      "     24  0.5784  1160895731.8751  1604947751.2656  0.0250  1.5202\n",
      "     25  0.6059  \u001b[32m1075807396.6669\u001b[0m  1500029275.2413  0.0125  1.4598\n",
      "     26  0.6203  \u001b[32m1014460347.5034\u001b[0m  \u001b[35m1445282116.7895\u001b[0m  0.0125  1.6737\n",
      "     27  0.6234  \u001b[32m993944754.4950\u001b[0m  \u001b[35m1433572125.0804\u001b[0m  0.0125  1.6477\n",
      "     28  0.5693  1001908211.1005  1639481588.2510  0.0125  1.5755\n",
      "     29  0.5925  \u001b[32m989838928.7706\u001b[0m  1551258241.2838  0.0125  1.6819\n",
      "     30  0.5719  \u001b[32m981037984.1906\u001b[0m  1629536172.6793  0.0125  1.5208\n",
      "     31  0.6230  \u001b[32m968244001.0014\u001b[0m  1435136740.2862  0.0125  1.4567\n",
      "     32  0.6211  \u001b[32m934012958.5284\u001b[0m  1442446651.7507  0.0125  1.5395\n",
      "     33  0.5930  961921988.4059  1549260123.9252  0.0125  1.4346\n",
      "     34  0.6281  \u001b[32m892157829.3770\u001b[0m  \u001b[35m1415481828.9419\u001b[0m  0.0063  1.5467\n",
      "     35  0.6206  \u001b[32m858989430.3495\u001b[0m  1444046374.5837  0.0063  1.5693\n",
      "     36  0.6273  \u001b[32m853219040.4083\u001b[0m  1418536250.2985  0.0063  1.5953\n",
      "     37  0.6357  859247732.8936  \u001b[35m1386503337.3004\u001b[0m  0.0063  1.5554\n",
      "     38  0.6323  \u001b[32m843090327.5139\u001b[0m  1399809002.0645  0.0063  1.5637\n",
      "     39  0.6144  \u001b[32m818014000.1359\u001b[0m  1467633768.5782  0.0063  1.6572\n",
      "     40  0.6257  820377169.7864  1424608463.2811  0.0063  1.7367\n",
      "     41  0.6372  818051778.2929  \u001b[35m1381145670.8469\u001b[0m  0.0063  1.8044\n",
      "     42  0.6030  \u001b[32m812431082.9670\u001b[0m  1511139281.5692  0.0063  1.6269\n",
      "     43  0.6148  \u001b[32m807734353.4964\u001b[0m  1466180298.0535  0.0063  1.6822\n",
      "     44  0.6102  815063215.2951  1483620649.2750  0.0063  1.7034\n",
      "     45  0.5919  809156752.7100  1553383376.5479  0.0063  1.7317\n",
      "     46  0.6121  \u001b[32m800334153.5510\u001b[0m  1476341892.7991  0.0063  1.5840\n",
      "     47  0.6123  \u001b[32m793419645.7138\u001b[0m  1475688163.0884  0.0063  1.6597\n",
      "     48  0.6295  \u001b[32m785418316.3980\u001b[0m  1410184655.1601  0.0031  1.6246\n",
      "     49  0.5823  \u001b[32m744964797.0898\u001b[0m  1590084868.5437  0.0031  1.8337\n",
      "     50  0.5967  \u001b[32m725561252.9064\u001b[0m  1535328352.6708  0.0031  1.5867\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Restoring best model from epoch 41.\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(\n",
    "    clone(preprocessor), \n",
    "    GridSearchCV(\n",
    "        NeuralNetRegressor(\n",
    "            Model,\n",
    "            max_epochs=150,\n",
    "            criterion=nn.MSELoss,\n",
    "            batch_size=64,\n",
    "            optimizer=optim.AdamW,\n",
    "            iterator_train__shuffle=True,\n",
    "            train_split=dataset.ValidSplit(cv=5),\n",
    "            callbacks=[\n",
    "                EarlyStopping(patience=10, monitor='valid_loss', load_best=True),\n",
    "                LRScheduler(policy=optim.lr_scheduler.ReduceLROnPlateau, patience=5, factor=0.5, monitor='valid_loss'),  # type: ignore\n",
    "                EpochScoring(scoring='r2', on_train=False),\n",
    "            ]\n",
    "        ),\n",
    "        { 'lr': [5e-2] },\n",
    "        scoring='r2',\n",
    "        cv=KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    )\n",
    ").fit(X_train, np.array(y_train).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ce3f0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([60.6911325]),\n",
       " 'std_fit_time': array([17.91788435]),\n",
       " 'mean_score_time': array([0.10628586]),\n",
       " 'std_score_time': array([0.00624012]),\n",
       " 'param_lr': masked_array(data=[0.05],\n",
       "              mask=[False],\n",
       "        fill_value=1e+20),\n",
       " 'params': [{'lr': 0.05}],\n",
       " 'split0_test_score': array([0.62353731]),\n",
       " 'split1_test_score': array([0.61052332]),\n",
       " 'split2_test_score': array([0.56944357]),\n",
       " 'split3_test_score': array([0.57867442]),\n",
       " 'split4_test_score': array([0.58790514]),\n",
       " 'mean_test_score': array([0.59401675]),\n",
       " 'std_test_score': array([0.02010229]),\n",
       " 'rank_test_score': array([1], dtype=int32)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = model[-1]\n",
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5e350ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.8703\n",
      "Train RMSE: 22155.0747\n",
      "Train MAE: 13860.8270\n"
     ]
    }
   ],
   "source": [
    "result_train = salary.evaluate_train_predictions(model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7d3c255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R2: 0.6303\n",
      "Test RMSE: 35445.6845\n",
      "Test MAE: 22076.0723\n"
     ]
    }
   ],
   "source": [
    "result_test = salary.evaluate_test_predictions(model.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
