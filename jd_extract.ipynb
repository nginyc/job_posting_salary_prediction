{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Need API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=\"AIzaSyCIPinvQ0DIhZJ87czUyLf6_MFBtrrvpNc\",\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the dataset, jobs_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/jobs_clean.csv\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Loop\n",
    "\n",
    "idea here is to concatenate a few JDs together and send them in one api call, this is to prevent the max request limit reach. \n",
    "\n",
    "Process 20 JDs per 1 api call, cannot put too many JDs in one together because the LLM might hallucinate. \n",
    "\n",
    "Need to use the open ai structured outputs and a custom format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = '''\n",
    "You are to summarize the following features concisely from each job description: \"Core Responsibilities, Required Skills, Educational Requirements, Experience Level, Preferred Qualifications, Compensation and Benefits.\n",
    "\n",
    "Please return the output in the following format:\n",
    "\n",
    "{\n",
    "  \"Core Responsibilities\": \"Summarize the core responsibilites of the job here\",\n",
    "  \"Required Skills\": \"Summarize the required skills here\",\n",
    "  \"Educational Requirements\": \"Summarize the educational requirements\",\n",
    "  \"Experience Level\": \"Summarize the experience level in years, if none put N/A\",\n",
    "  \"Preferred Qualifications\": \"Summarize the qualifications required\",\n",
    "  \"Compensation and Benefits\": \"Summarize the monthly or hourly salary, if none put N/A\"\n",
    "}\n",
    "return the output as a string, not markdown\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "class MessageInfo(BaseModel):\n",
    "    Required_Skills: str\n",
    "    Educational_Requirements: str\n",
    "    Experience_Level: str\n",
    "    Preferred_Qualifications: str\n",
    "    Compensation_and_Benefits:str \n",
    "\n",
    "class Messages(BaseModel):\n",
    "    messages: List[MessageInfo]\n",
    "\n",
    "class BatchProcess():\n",
    "    def __init__(self, df:pd.DataFrame, start_index: int, batch_start_number:int, system_message:str):\n",
    "        \"\"\" \n",
    "        prepares raw jd from df, sends it to llms and writes outputs to json\n",
    "        \n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.start_index = start_index #index of the dataframe to START\n",
    "        self.batch_start_number = batch_start_number #batch number to START\n",
    "        self.system_message = system_message\n",
    "\n",
    "    def process(self, no_of_batches:int, batch_size:int) -> None:\n",
    "        #prepare the messages in the batch\n",
    "        for i in range(no_of_batches):\n",
    "            \n",
    "            messages, job_ids, checkpoint = self.prepare_llm_inputs(self.df, self.start_index, batch_size) #process before llm\n",
    "            #print(checkpoint) #debug\n",
    "            llm_structured_output = self.get_structured_output_from_llm(self.system_message, messages) #pass to llm, get responses\n",
    "\n",
    "            if len(llm_structured_output.messages) != len(job_ids):\n",
    "                print(\"job_ids:\",job_ids)\n",
    "                print(llm_structured_output.messages)\n",
    "                raise ValueError(f\"Error: Expected batch size of {batch_size}, but got jobids: {len(job_ids)} and llmoutputs: {len(llm_structured_output.messages)}.\")\n",
    "\n",
    "            batch_jds = self.append_jb_ids(job_ids, llm_structured_output) #process output, append jd ids to the response\n",
    "\n",
    "            self.write_to_file(batch_jds, checkpoint, self.batch_start_number) #write to json for the batch\n",
    "            \n",
    "            self.start_index = checkpoint\n",
    "\n",
    "            self.batch_start_number += 1\n",
    "            time.sleep(10)\n",
    "        \n",
    "\n",
    "\n",
    "       \n",
    "    # need to iterate through the rows in the df for that batch from the start checkpoint, \n",
    "    def prepare_llm_inputs(self, df: pd.DataFrame, checkpoint:int, batch_size:int) -> tuple[str, List[int], int]:\n",
    "        \"\"\" \n",
    "        This function iterates through the rows in the dataframe in that batch and prepares multiple raw JDs into one message/string for the llm\n",
    "        returns the updated checkpoint, input prompt which is a string and the list of their job ids for addition later\n",
    "        \"\"\"\n",
    "        messages =\"\"\n",
    "        job_ids = []\n",
    "        counter = 1\n",
    "        for i in range(checkpoint, checkpoint+batch_size):\n",
    "            job_id = int(df.iloc[i]['job_id'])\n",
    "            raw_jd = df.iloc[i]['description'].strip()\n",
    "            job_ids.append(job_id)\n",
    "            messages += f\"Job {counter}: \\n{raw_jd}\\n\\n\"\n",
    "            counter+=1\n",
    "        # Check if the number of job ids does not match the batch size\n",
    "        if len(job_ids) != batch_size:\n",
    "            \n",
    "            raise ValueError(f\"Error: Expected batch size of {batch_size}, but got {len(job_ids)}.\")\n",
    "\n",
    "        return (messages, job_ids, checkpoint + batch_size)\n",
    "            \n",
    "    def get_structured_output_from_llm(self, system_message: str, user_message:str) -> Messages:\n",
    "        \"\"\" \n",
    "        sends the messages to gemini, returns a Messages object\n",
    "        \"\"\"\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=\"gemini-2.0-flash-lite\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_message\n",
    "                }\n",
    "            ],\n",
    "            response_format=Messages,\n",
    "        )\n",
    "        return response.choices[0].message.parsed\n",
    "    \n",
    "    # we need to append the job ids to each message, saves output to json file\n",
    "    def append_jb_ids(self, job_ids:List, messages_list: Messages) -> List:\n",
    "        batch_jds = []\n",
    "        for idx, message in enumerate(messages_list.messages):\n",
    "            temp = message.model_dump()\n",
    "            #print(temp, idx)\n",
    "            temp['job_id'] = job_ids[idx]\n",
    "            batch_jds.append(temp)\n",
    "\n",
    "        return batch_jds\n",
    "    \n",
    "    def write_to_file(self, batch_jds:List, row_checkpoint, batch_no) -> None:\n",
    "            row_checkpoint -= 1\n",
    "            dir_name = \"extracted_jds\"\n",
    "            os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "            filename = f\"{dir_name}/batch_{batch_no}_row_{row_checkpoint}_extracted_jd.json\"\n",
    "\n",
    "            with open(filename, \"w\") as f:\n",
    "                json.dump(batch_jds, f, indent=4) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_process = BatchProcess(df=df, start_index=2270, batch_start_number=130, system_message=system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    batch_process.process(10, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = df.query(\"job_id == 3885855930\")['description'].values\n",
    "# print(description[0] if len(description) > 0 else \"Job ID not found.\")\n",
    "\n",
    "print(df.iloc[2272]['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids= [3885855930]\n",
    "a = [MessageInfo(Required_Skills='Conduct primary and secondary research, business acumen, convert information into frameworks, strong writing and communication, experience with marketing campaigns', Educational_Requirements=\"Bachelor's degree required, Masterâ€™s degree preferred\", Experience_Level='5+ years', Preferred_Qualifications=\"Master's degree and/or PhD in a social science\", Compensation_and_Benefits='Competitive salary, generous PTO, medical, dental & vision plans, parental leave, employee assistance program, professional development, growth opportunities'), MessageInfo(Required_Skills='Digital marketing, sales enablement, campaign creation, lead management, pipeline conversion, data analysis, project management, communication, SEO/SEM, web content creation', Educational_Requirements=\"Bachelor's Degree preferred\", Experience_Level='5+ years', Preferred_Qualifications=\"Bachelor's Degree preferred, experience with Google Analytics, marketing automation tools (Eloqua), Adobe Experience Manager\", Compensation_and_Benefits='Base salary range: $84,000 - $115,920, bonus or incentive plan, PTO, 401k match, stock purchase opportunity')]\n",
    "# for i in range(len(a)):\n",
    "#     print(job_ids[i])\n",
    "#     print(a[i])\n",
    "#     print(\"\\n\")\n",
    "\n",
    "print(a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_process.start_index)\n",
    "print(batch_process.batch_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "def validate_files():\n",
    "    json_files = glob.glob(\"extracted_jds/*\")\n",
    "    # Iterate over each file and read the data\n",
    "    for file in json_files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)  # Load the data from each JSON file\n",
    "            rand_idx = random.randint(0,len(data)-1)\n",
    "            rand_job_id = data[rand_idx]['job_id']\n",
    "            match = re.search(r\"batch_\\d+_row_(\\d+)_extracted_jd\\.json\", file)\n",
    "            first_row_idx = int(match.group(1)) - (len(data)-1)\n",
    "            correct_row_no = first_row_idx +  rand_idx \n",
    "            correct_job_id = df.iloc[correct_row_no]['job_id'] \n",
    "            if correct_job_id != rand_job_id:\n",
    "                raise Exception(f\"row {correct_row_no} dont match, correct id: {correct_job_id}, wrong id:{rand_job_id}, filename:{file}\")\n",
    "\n",
    "for i in range(100):\n",
    "    validate_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMBINE JSON FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "def combine(filename):\n",
    "    combined = []\n",
    "    # Find all JSON files in a directory\n",
    "    json_files = glob.glob(\"extracted_jds/*\")\n",
    "    # Iterate over each file and read the data\n",
    "    for file in json_files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)  # Load the data from each JSON file\n",
    "            combined.extend(data)\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(combined, f, indent=4) \n",
    "\n",
    "combine(\"batch_130_row_2270_extracted_jd.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path):\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)  # Load the data from each JSON file\n",
    "    \n",
    "    return data   \n",
    "data = load(\"extracted_jds/batch_77_row_1239_extracted_jd.json\")\n",
    "print(len(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
